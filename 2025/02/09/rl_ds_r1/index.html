<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Reiase's Blog"><meta name=author content="reiase <reiase@gmail.com>"><link href=https://reiase.github.io/2025/02/09/rl_ds_r1/ rel=canonical><link href=../../../../2024/01/25/review-dist-train/ rel=prev><link href=../../12/fp8_training/ rel=next><link rel=icon href=../../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.3"><title>从强化学习到DeepSeek R1 - Overfitting</title><link rel=stylesheet href=../../../../assets/stylesheets/main.d7758b05.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Han+Sans+SC:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Source Han Sans SC";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../assets/_markdown_exec_pyodide.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LMDTYMWWTF"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LMDTYMWWTF",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LMDTYMWWTF",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script> <link href="../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#1-rl-reinforcement-learning class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../../.. title=Overfitting class="md-header__button md-logo" aria-label=Overfitting data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Overfitting </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 从强化学习到DeepSeek R1 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../.. class=md-tabs__link> 博客 </a> </li> <li class=md-tabs__item> <a href=../../../../tags/ class=md-tabs__link> 标签 </a> </li> <li class=md-tabs__item> <a href=../../../../nodes/AdamW/ class=md-tabs__link> 代码片段 </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title=Overfitting class="md-nav__button md-logo" aria-label=Overfitting data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> Overfitting </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../../.. class="md-nav__link "> <span class=md-ellipsis> 博客 </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=true> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> 博客 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_2> <label class=md-nav__link for=__nav_1_2 id=__nav_1_2_label tabindex> <span class=md-ellipsis> 归档 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_1_2> <span class="md-nav__icon md-icon"></span> 归档 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex> <span class=md-ellipsis> 分类 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> 分类 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/fp8/ class=md-nav__link> <span class=md-ellipsis> FP8 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/rl/ class=md-nav__link> <span class=md-ellipsis> RL </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/training/ class=md-nav__link> <span class=md-ellipsis> Training </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../tags/ class=md-nav__link> <span class=md-ellipsis> 标签 </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../nodes/AdamW/ class=md-nav__link> <span class=md-ellipsis> 代码片段 </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../.. class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> 回到主页 </span> </a> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> 元数据 </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2025-02-09 00:00:00+00:00" class=md-ellipsis>2025年2月9日</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> 分类于 <a href=../../../../category/llm/ >LLM</a>, <a href=../../../../category/training/ >Training</a>, <a href=../../../../category/rl/ >RL</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 需要 6 分钟阅读时间 </span> </div> </li> </ul> </nav> </li> </ul> </nav> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-rl-reinforcement-learning class=md-nav__link> <span class=md-ellipsis> 1. 什么是强化学习(RL, Reinforcement Learning) </span> </a> </li> <li class=md-nav__item> <a href=#2 class=md-nav__link> <span class=md-ellipsis> 2. 如何进行强化学习 </span> </a> <nav class=md-nav aria-label="2. 如何进行强化学习"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-q-learning class=md-nav__link> <span class=md-ellipsis> 2.1. Q-Learning </span> </a> </li> <li class=md-nav__item> <a href=#22-q-learning class=md-nav__link> <span class=md-ellipsis> 2.2. Q-Learning 的学习过程 </span> </a> <nav class=md-nav aria-label="2.2. Q-Learning 的学习过程"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#221-exploitation-exploration class=md-nav__link> <span class=md-ellipsis> 2.2.1. 利用和探索（exploitation &amp; exploration） </span> </a> </li> <li class=md-nav__item> <a href=#222 class=md-nav__link> <span class=md-ellipsis> 2.2.2. 强化学习的目标 </span> </a> </li> <li class=md-nav__item> <a href=#223 class=md-nav__link> <span class=md-ellipsis> 2.2.3. 强化学习的挑战 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3 class=md-nav__link> <span class=md-ellipsis> 3. 策略梯度方法 </span> </a> <nav class=md-nav aria-label="3. 策略梯度方法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31 class=md-nav__link> <span class=md-ellipsis> 3.1. 策略梯度定理 </span> </a> </li> <li class=md-nav__item> <a href=#32-trpotrust-region-policy-optimization class=md-nav__link> <span class=md-ellipsis> 3.2. 策略梯度方法的改进&ndash;TRPO（Trust Region Policy Optimization） </span> </a> <nav class=md-nav aria-label="3.2. 策略梯度方法的改进–TRPO（Trust Region Policy Optimization）"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 优势函数 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#33-ppoproximal-policy-optimization class=md-nav__link> <span class=md-ellipsis> 3.3. 策略梯度方法的改进&ndash;PPO(Proximal Policy Optimization) </span> </a> <nav class=md-nav aria-label="3.3. 策略梯度方法的改进–PPO(Proximal Policy Optimization)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 优势函数的估计 </span> </a> </li> <li class=md-nav__item> <a href=#rlhf-with-ppo class=md-nav__link> <span class=md-ellipsis> RLHF with PPO </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#34-grpogroup-relative-policy-optimization class=md-nav__link> <span class=md-ellipsis> 3.4. 策略梯度方法的改进&ndash;GRPO（Group Relative Policy Optimization） </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#deepseek-r1 class=md-nav__link> <span class=md-ellipsis> DeepSeek R1 </span> </a> <nav class=md-nav aria-label="DeepSeek R1"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#deepseek-r1-zero class=md-nav__link> <span class=md-ellipsis> DeepSeek R1-Zero </span> </a> </li> <li class=md-nav__item> <a href=#deepseek-r1_1 class=md-nav__link> <span class=md-ellipsis> DeepSeek R1 </span> </a> <nav class=md-nav aria-label="DeepSeek R1"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cold-start class=md-nav__link> <span class=md-ellipsis> Cold Start </span> </a> </li> <li class=md-nav__item> <a href=#reasoning-rl class=md-nav__link> <span class=md-ellipsis> Reasoning RL </span> </a> </li> <li class=md-nav__item> <a href=#sft class=md-nav__link> <span class=md-ellipsis> 拒绝采样与SFT </span> </a> </li> <li class=md-nav__item> <a href=#rl-for-all-scenarios class=md-nav__link> <span class=md-ellipsis> RL for all scenarios </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <nav class=md-tags> <a href=../../../../tags/#tag:llm class=md-tag>LLM</a> <a href=../../../../tags/#tag:rl class=md-tag>RL</a> <a href=../../../../tags/#tag:training class=md-tag>Training</a> </nav> <h1>从强化学习到DeepSeek R1</h1> <h2 id=1-rl-reinforcement-learning>1. 什么是强化学习(RL, Reinforcement Learning)<a class=headerlink href=#1-rl-reinforcement-learning title="Permanent link">&para;</a></h2> <p>传统的机器学习，包括深度学习，其本质是数学性的，严格遵守函数的数学定义：对于给定输入，产生确定的输出</p> <div class=arithmatex>\[F(x) = y\]</div> <p>随着输入<span class=arithmatex>\(x\)</span>和输出<span class=arithmatex>\(y\)</span>的不同，这一范式可以适配各种不同的任务，比如：</p> <ul> <li><span class=arithmatex>\(x\)</span> 是图像，<span class=arithmatex>\(y\)</span>是类别，那么<span class=arithmatex>\(F\)</span>就是Resnet这种图像模型；</li> <li><span class=arithmatex>\(x\)</span> 是语音信号，<span class=arithmatex>\(y\)</span>是文字，那么<span class=arithmatex>\(F\)</span>就是一个语音识别模型；</li> <li><span class=arithmatex>\(x\)</span> 是文本输入，<span class=arithmatex>\(y\)</span>是文本输出，那么<span class=arithmatex>\(F\)</span>就是时下火热的大语言模型；<br> &hellip;</li> </ul> <p>强化学习（Reinforcement Learning）的本质上则是哲学性的，它探讨三个核心问题：</p> <ul> <li>我是谁？一个Agent</li> <li>我在哪？处于某个State</li> <li>到哪里去？采取一个Action</li> </ul> <p>如果站在上帝视角去观测这个Agent，我们还会发现：</p> <ul> <li>Agent处在一个环境中（Environment）</li> <li>Agent有一个用来策略（Policy）告诉我该采取什么动作（Action）</li> <li>每执行一个动作（Action），环境都会给我反馈 (Reward)</li> </ul> <p>以上就是强化学习中的主要概念。</p> <p><a class=glightbox href=../../../../imgs/rl.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/rl.png></a></p> <h2 id=2>2. 如何进行强化学习<a class=headerlink href=#2 title="Permanent link">&para;</a></h2> <p>这里以一个迷宫问题为例，介绍如何进行强化学习：</p> <p>迷宫：(S: Start, E: End, W: Wall)</p> <pre class=mermaid><code>block-beta
  columns 3
  S1["S1(S)"] S2 S3["S3(W)"]
  S4 S5 S6
  S7["S7(W)"] S8 S9["S9(E)"]
</code></pre> <p>这个迷宫就是一个Environment。我们放置一个机器人在开始处（Start），让机器人自动学习如何走迷宫的策略（Policy）。这个策略可以记成<span class=arithmatex>\(\pi(s)\rightarrow a, s \in [1-9], a \in [上, 下, 左, 右]\)</span>。开始时机器人对于迷宫一无所知，所以<span class=arithmatex>\(\pi(s)会随机输出一个方向\)</span>。</p> <h3 id=21-q-learning>2.1. Q-Learning<a class=headerlink href=#21-q-learning title="Permanent link">&para;</a></h3> <p>Q-Learning是最早，也是最简单的强化学习算法。前文提到的策略<span class=arithmatex>\(\pi(s)\)</span>过于抽象了，所以我们也定义以下表格</p> <table> <thead> <tr> <th></th> <th>上</th> <th>下</th> <th>左</th> <th>右</th> </tr> </thead> <tbody> <tr> <td>s1</td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>s2</td> <td></td> <td></td> <td></td> <td></td> </tr> <tr> <td>&hellip;</td> <td></td> <td></td> <td></td> <td></td> </tr> </tbody> </table> <p>我们称这张表为Q值表，表里边的元素 <span class=arithmatex>\(Q(s, a)\)</span>表示在状态<span class=arithmatex>\(s\)</span>下，采取动作<span class=arithmatex>\(a\)</span>的奖励<span class=arithmatex>\(reward\)</span>。基于这张Q值表，我们可以定义策略：<br> $$<br> \pi(s_i) \rightarrow \arg\max_a Q(s, a)|_{s=s_i}<br> $$<br> 完成迷宫所需要的最小的Q值表如下：</p> <table> <thead> <tr> <th></th> <th>上</th> <th>下</th> <th>左</th> <th>右</th> </tr> </thead> <tbody> <tr> <td>s1</td> <td>-1</td> <td>1</td> <td>-1</td> <td>0</td> </tr> <tr> <td>s4</td> <td>0</td> <td>-1</td> <td>-1</td> <td>1</td> </tr> <tr> <td>s5</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> </tr> <tr> <td>s8</td> <td>0</td> <td>-1</td> <td>-1</td> <td>1</td> </tr> </tbody> </table> <p>可以指挥Agent沿如下路径行进</p> <pre class=mermaid><code>block-beta
  columns 3
  S1["S1(S)↓"] S2 S3["S3(W)"]
  S4["S4→"] S5["S5↓"] S6
  S7["S7(W)"] S8["S8→"] S9["S9(E)"]</code></pre> <p>但是我们很快发现Agent行进的路径并不唯一，比如以下路径也能让Agent走到终点</p> <pre class=mermaid><code>block-beta
  columns 3
  S1["S1(S)→"] S2["S2↓"] S3["S3(W)"]
  S4["S4"] S5["S5↓"] S6
  S7["S7(W)"] S8["S8→"] S9["S9(E)"]
</code></pre> <p>这是强化学习的第一个问题，<strong>模型的解不唯一</strong>。稍后我们讨论这种解不唯一带来的问题。</p> <h3 id=22-q-learning>2.2. Q-Learning 的学习过程<a class=headerlink href=#22-q-learning title="Permanent link">&para;</a></h3> <ol> <li>Q值表初始化，可以全部初始化为0；</li> <li> <p>迭代学习：我们假设Agent被初始化在S0位置</p> <ol> <li> <p>选择action </p> <div class=arithmatex>\[a = \pi(s_i) = \arg\max_a Q(s, a)|_{s=s_i}\]</div> </li> <li> <p>执行action, 环境Environment给出下一个状态<span class=arithmatex>\(s'与奖励\)</span>r$</p> <div class=arithmatex>\[s',r = Environment(s, a)\]</div> </li> <li> <p>更新Q值，这里一般使用Bellman方程：</p> <div class=arithmatex>\[Q(s,a)\leftarrow Q(s,a) + \alpha [r+\gamma \max_{a'}Q(s',a')-Q(s,a)]\]</div> </li> </ol> <p>其中：</p> <ul> <li><span class=arithmatex>\(\alpha\)</span> 为学习率，控制Q值表的更新幅度；</li> <li><span class=arithmatex>\(\gamma\)</span> 为折扣因子，控制长期奖励与即时奖励的平衡；</li> </ul> </li> </ol> <p>Q-Learning本质上就是记住当前格子的奖励，同时不断根据未来价值重新评估当前格子的价值。最优情况下，总是选择未来价值最高的动作。在学习过程中，每个格子的Q值总是依赖于其他格子的Q值，格子之间相互依赖，非常容易导致Q值长期震荡无法收敛。此时需要对学习过程进行精细挑参才能保证学习过程的收敛性。这是强化学习的第二个问题。</p> <h4 id=221-exploitation-exploration>2.2.1. 利用和探索（exploitation &amp; exploration）<a class=headerlink href=#221-exploitation-exploration title="Permanent link">&para;</a></h4> <p>如果单纯使用上述学习方法，很容易出现如下问题：</p> <pre class=mermaid><code>block-beta
  columns 3
  S1["S1(S)→"] S2["S2↓"] S3["S3(W)"]
  S4["S4"] S5["S5↑"] S6
  S7["S7(W)"] S8["S8"] S9["S9(E)"]</code></pre> <p>即Agent在S2和S5之间反复震荡，无法真的走到终点S9。其原因在于Agent只能获取其历史路径上的Q值，缺乏对整个世界（Environment）的认知，无法发现S8和S6这种更加靠近终点的路径。我们称这种利用历史知识的过程为“利用”（exploitation）。</p> <p>除了“利用”以外，我们还需要让Agent有一定的“探索”（exploration）能力，保持对世界的好奇心。最常见的探索方法是使用<span class=arithmatex>\(\epsilon-greedy\)</span>改造策略 <span class=arithmatex>\(\phi\)</span>:</p> <div class=arithmatex>\[ \phi(s_i) = \begin{cases} \pi(s_i) &amp; \text{with probability } 1 - \epsilon \\ \text{random action} &amp; \text{with probability } \epsilon \end{cases} \]</div> <p>即以概率<span class=arithmatex>\(1-\epsilon\)</span>选择最优动作，以概率 <span class=arithmatex>\(\epsilon\)</span> 选择随机动作。为了让Q-Learning更好的收敛，可以在训练过程中逐步降低探索概率<span class=arithmatex>\(\epsilon\)</span>。</p> <h4 id=222>2.2.2. 强化学习的目标<a class=headerlink href=#222 title="Permanent link">&para;</a></h4> <p>强化学习的核心目标是找到一个最优策略<span class=arithmatex>\(\pi^*\)</span>，使Agent在其生命周期内获得的期望奖励最大化：</p> <div class=arithmatex>\[J(\pi) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid \pi \right]\]</div> <p>直接在策略空间优化<span class=arithmatex>\(\pi\)</span> 通常是不可行的：</p> <ul> <li>策略空间过大：<span class=arithmatex>\(\pi(a|s)\)</span>是一个概率分布，极大的增加了搜索难度；</li> <li>无梯度信息：不经过特殊设计，难以直接对策略求导；</li> </ul> <p>为了解决上述问题，引入了 <strong>值函数</strong> 来简化优化过程，值函数一般有两类：</p> <ul> <li><strong>状态价值函数<span class=arithmatex>\(V^\pi(s)\)</span></strong>: 表示从某个状态<span class=arithmatex>\(s\)</span>开始，遵循策略<span class=arithmatex>\(\pi\)</span>后，所能获得的长期回报</li> </ul> <div class=arithmatex>\[ V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s \right] \]</div> <ul> <li><span class=arithmatex>\(\gamma\)</span>为折扣因子，控制未来奖励的重要性；</li> <li><span class=arithmatex>\(r_t\)</span>为时刻<span class=arithmatex>\(t\)</span>的即时奖励；</li> <li><span class=arithmatex>\(\mathbb{{E}}_\pi\)</span>表示对策略<span class=arithmatex>\(\pi\)</span>的所有可能多做求期望。</li> </ul> <p>Bellman方程是上述值函数定义的递归形似，将上述定义拆解成了当前奖励和未来状态价值：</p> <div class=arithmatex>\[ V^\pi(s) = \sum_{a} \pi(a|s) [r(s, a)+\gamma V^\pi(s')] \]</div> <ul> <li><strong>动作价值函数</strong><span class=arithmatex>\(Q^\pi(s, a)\)</span>: 与状态价值函数类似，只是表示状态<span class=arithmatex>\(s\)</span>下执行动作<span class=arithmatex>\(a\)</span>后，能够得到的长期回报</li> <li> <div class=arithmatex>\[ Q^\pi(s,a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0=a \right] \]</div> </li> </ul> <h4 id=223>2.2.3. 强化学习的挑战<a class=headerlink href=#223 title="Permanent link">&para;</a></h4> <ol> <li> <p><strong>训练样本的概率分布问题</strong> 传统的监督学习中，一般要求训练样本和测试样本独立同分布，即样本独立抽样，但样本来源的概率分布保持不变。这样才能让监督学习算法从数据里边总结和学习规律。而强化学习通过与环境交互来获取样本，Agent的初始状态、运行轨迹都会影响采样过程。即不能保证两次训练之间的样本是独立同分布，也无法保证测试与训练的样本是独立同分布。</p> </li> <li> <p><strong>部分观测与环境随机性</strong> 强化学习训练过程只能对整个世界（Environment）进行部分观测，不能感知整个世界（没见过市面）。模型经常出现对局部过度学习、过拟合，对全局欠拟合。由于无法界定过拟合和欠拟合部分的边界，难以保证Agent在未见过的新环境中行为的合理性和稳定性。</p> </li> <li> <p><strong>延迟反馈与稀疏奖励</strong> 在许多强化学习问题中，奖励信号往往是延迟且稀疏的，导致Agent难以及时判定行为的好坏。这种激励信号的不连续性使得价值函数估计方差变大，会增加优化难度，必须采用奖励整形、经验回放等策略来缓解这一问题。</p> </li> </ol> <h2 id=3>3. 策略梯度方法<a class=headerlink href=#3 title="Permanent link">&para;</a></h2> <p>在介绍策略梯度方法之前，我们先来看值函数方法的几个问题：</p> <ol> <li>只适合离散状态与动作空间</li> <li>需要额外组合探索策略</li> <li>随机探索对最坏情况缺乏控制；</li> </ol> <p>而策略梯度方法是直接对策略进行求导的方法，首先需要将策略函数参数化<span class=arithmatex>\(\pi(a|s) \rightarrow \pi_\theta(a|s)\)</span>，此时可以将强化学习的目标函数写为</p> <div class=arithmatex>\[ J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[ R(\tau) \right] \]</div> <p>这里：</p> <ul> <li><span class=arithmatex>\(\tau = (s_0, a_0, s_1, a_1, ..., s_T, a_T)\)</span>是一个轨迹；</li> <li><span class=arithmatex>\(R(\tau) = \sum_{t=0}^{T} r(s_t, a_t)\)</span> 是轨迹的概率分布；</li> <li><span class=arithmatex>\(p_\theta(\tau)\)</span> 是轨迹的概率，由策略概率和环境的转移概率组成；</li> </ul> <h3 id=31>3.1. 策略梯度定理<a class=headerlink href=#31 title="Permanent link">&para;</a></h3> <p>策略梯度可以根据策略梯度定理计算</p> <div class=arithmatex>\[ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)R(\tau)\right] \]</div> <p>其中，<span class=arithmatex>\(\nabla_\theta \log \pi_\theta(a_t|s_t)\)</span>是策略的梯度，通过这个梯度可以更新策略在状态<span class=arithmatex>\(s_t\)</span>下采用 <span class=arithmatex>\(a_t\)</span>的概率。而<span class=arithmatex>\(R(\tau)\)</span>表示策略带来的未来回报，回报越高动作被采纳的概率越高。估计未来回报是计算策略梯度最重要的步骤，常见实现有两种：</p> <ul> <li>使用蒙特卡洛方法估计<span class=arithmatex>\(Q_\pi(s,a)\)</span>，由于不需要使用模型，也称为无模型策略梯度算法；</li> <li>通过引入价值函数（<span class=arithmatex>\(V_\pi\)</span>或者<span class=arithmatex>\(Q_\pi\)</span>）来估计未来回报；</li> </ul> <p>策略梯度方法对比值函数方法，具备如下特点：</p> <ul> <li>可以基于轨迹学习，而不需要实时交互和反馈</li> <li>适合高维连续动作空间（包括embedding空间）</li> <li>可以把序列生成模型可以作为策略：<ul> <li>用户输入作为状态空间　Ｓ；</li> <li>模型输出作为动作空间　Ａ；</li> <li>模型参数作为策略参数　<span class=arithmatex>\(\theta\)</span>; </li> </ul> </li> </ul> <p>给出人类偏好反馈 <span class=arithmatex>\(R(\tau)\)</span>即可直接使用PG方法训练LLM对齐人类偏好。但PG算法的梯度方差较大，稳定性欠佳。同时<span class=arithmatex>\(R(\tau)\)</span> 也难以直接定义、打分，因此PG方法并未实际应用于LLM对齐任务。</p> <h3 id=32-trpotrust-region-policy-optimization>3.2. 策略梯度方法的改进&ndash;TRPO（Trust Region Policy Optimization）<a class=headerlink href=#32-trpotrust-region-policy-optimization title="Permanent link">&para;</a></h3> <p>当把策略梯度方法应用于训练时，可以通过下式计算minibatch梯度：</p> <div class=arithmatex>\[ \nabla_\theta J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)R(\tau) \]</div> <p>最大化累计奖励，需要使用梯度上升方法对参数进行更新：</p> <div class=arithmatex>\[ \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta) \]</div> <p>策略更新如何选择步长<span class=arithmatex>\(\alpha\)</span>是一个非常关键的问题：</p> <ul> <li>过大的步长会导致策略剧烈变化，破坏已学到的好的行为</li> <li>过小的步长会导致训练效率低下</li> </ul> <p>常见的步长选择方法有三种：固定步长，线性搜索和信赖域，以下是三种方法的图示：</p> <div class=arithmatex>\[ \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta) \]</div> <p><a class=glightbox href=../../../../imgs/line_search_trust_region.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/line_search_trust_region.png></a></p> <p>固定步长方法是最为朴素的方法，但是步长会影响收敛性和稳定性，对实际问题搜索合理步长会比较麻烦；线性搜索会沿着梯度方向尝试多个步长，并选择最优步长，搜索开销较大，但效果最好；信赖域方法会根据梯度计算自适应信赖域，并保证在信赖域内更新。信赖域方法一方面能够保证每一步更新不会距离原策略太远，另一方面能够在梯度噪声过高时自动缩小信赖域，能够比较好的解决PG方法梯度噪声高问题。</p> <h4 id=_1>优势函数<a class=headerlink href=#_1 title="Permanent link">&para;</a></h4> <p>上边讨论的步长搜索算法都在某种程度上要求目标函数具备凸性，而强化学习的目标显然不是凸函数。为了解决目标函数凸性的问题，TRPO又引入了 <a href="/nodes/Minorize Maximization/">Minorize-Maximization思想</a> ：对于一个难以优化，难以求导的目标函数，可以选择一个替代函数来描述目标函数的上届，通过迭代最大化这个上届替代函数可以完成对原目标函数的优化。</p> <div class="admonition note"> <p class=admonition-title>什么是凸性</p> <p><strong>凸性</strong> 是优化理论中最为重要的一个概念。若一个函数满足任意亮点<span class=arithmatex>\(x_1\)</span>与<span class=arithmatex>\(x_2\)</span>，满足：<br> $$<br> f(\lambda x_1 +(1-\lambda)x_2) \le \lambda f(x_1) + (1-\lambda)f(x_2)<br> $$<br> 则函数具有凸性。具备凸性的函数局部最优等价于全局最优，因此基于凸性有一系列高效的优化算法。</p> </div> <p>为了构造这个替代函数，我们引入一个优势函数的定义：</p> <div class=arithmatex>\[ A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s) \]</div> <p>这个函数表示策略<span class=arithmatex>\(\pi\)</span>采取动作<span class=arithmatex>\(a\)</span>时的奖励比平均奖励高多少。TRPO最终求解的问题就变成<br> $$<br> \begin{aligned}<br> \max_{\theta} &amp; E[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A_\theta(s,a)] \<br> \textrm{s.t.} &amp;　E[D_{KL}(\pi_{\theta_{old}}(a|s) \pi_\theta(a|s))] \le \delta<br> \end{aligned}<br> $$</p> <p>其中，<span class=arithmatex>\(\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}\)</span>是 <a href="/nodes/Importance Sampling/">重要性采样</a> 。由于训练时只能使用旧策略产出的轨迹，我们需要通过重要性采样对目标函数中的期望进行修正，使其能够反映新策略的期望回报。</p> <h3 id=33-ppoproximal-policy-optimization>3.3. 策略梯度方法的改进&ndash;PPO(Proximal Policy Optimization)<a class=headerlink href=#33-ppoproximal-policy-optimization title="Permanent link">&para;</a></h3> <p>近端优化（Proximal Optimization）是这样一种优化方法：通过引入一个近端约束项，让优化迭代过程中步长尽可能小，避免因权重剧烈变化而带来的训练不稳定。Proximal OPtimization在传统的凸优化领域有着广泛的应用。<br> $$<br> \theta^* = \arg \max_\theta \left( <br> f(\theta) + \lambda |\theta - \theta_t|_p<br> \right)<br> $$<br> 其中<span class=arithmatex>\(p \in \{1, 2\}\)</span>，常见使用<span class=arithmatex>\(L_1\)</span>或者<span class=arithmatex>\(L_2\)</span>作为约束项。</p> <p>PPO通过clip操作限制策略的更新幅度</p> <p>$$<br> J_{PPO}(\theta) = \mathbb{E} \left[ <br> \min(<br> \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}A(s,a),<br> clip(\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}, 1-\epsilon,1+\epsilon) A(s,a)<br> )<br> \right]<br> $$<br> 其中<span class=arithmatex>\(clip()\)</span>函数会将<span class=arithmatex>\(\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}\)</span>这个比值限制在<span class=arithmatex>\([1-\epsilon,1+\epsilon]\)</span>。</p> <p>TRPO算法在计算信赖域时需要计算Fisher信息矩阵，并使用共轭梯度方法来更新，计算量较大。而PPO仅需要对重要性采样进行截断，更加的高效和稳定。</p> <h4 id=_2>优势函数的估计<a class=headerlink href=#_2 title="Permanent link">&para;</a></h4> <p>在进行优势函数（Advantage Function）估计的时候，通常有两种方法：蒙特卡扩估计与时间差分估计：</p> <ol> <li>蒙特卡洛估计：通过完整回报近似<span class=arithmatex>\(Q(s,a)\)</span>，需要完整的轨迹才能计算，一般用于离线数据</li> </ol> <div class=arithmatex>\[ A(s,a) = Q(s,a)-V(s) \approx \sum \gamma^tr_{t} - V(s) \]</div> <ol start=2> <li>时间差分估计：通过折扣未来奖励来近似<span class=arithmatex>\(Q(s,a)\)</span>，可实时更新</li> </ol> <div class=arithmatex>\[ A(s,a) = Q(s,a) - V(s) \approx r_t+\gamma V(s_{t+1}) -V(s) \]</div> <p>蒙特卡洛估计可以给出无偏估计，但是优势函数的方差较大，而时间差分方差较小，但误差偏差较大。为了平衡两者PPO引入了GAE（Generalized Advantage Estimation）：</p> <div class=arithmatex>\[ A_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{T-t}(\gamma \lambda)^l\delta_t +l \]</div> <p>其中<span class=arithmatex>\(\delta_t\)</span>是<span class=arithmatex>\(t\)</span>时刻的TD残差</p> <div class=arithmatex>\[ \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \]</div> <p>参数<span class=arithmatex>\(\lambda\)</span>用来控制蒙特卡洛估计与时间差分估计之前的权衡：</p> <ol> <li><span class=arithmatex>\(\lambda\)</span> 越大越趋向于未来奖励，更倾向于蒙特卡罗方法，提供高方差低偏差的估计；</li> <li><span class=arithmatex>\(\lambda\)</span> 越小越趋向于即时奖励，更加倾向于时间差分方法，提供低方差但高偏差的估计；</li> </ol> <h4 id=rlhf-with-ppo>RLHF with PPO<a class=headerlink href=#rlhf-with-ppo title="Permanent link">&para;</a></h4> <p>RLHF巧妙的在PPO框架下引入了人类反馈（Human Feedback），整个框架共涉及4个模型：</p> <ul> <li><strong>Actor Model：</strong> 演员模型，就是需要对齐的LLM模型，用于提供策略<span class=arithmatex>\(\pi(a|s)\)</span>；</li> <li><strong>Critic Model：</strong> 评论家模型，用于估计总收益 <span class=arithmatex>\(V_t\)</span>；</li> <li><strong>Reward Model：</strong> 奖励模型，用于估计计算即时收益 <span class=arithmatex>\(R_t\)</span>；</li> <li><strong>Reference Model：</strong> 参考模型，用于提供训练阶段的约束，防止模型训歪（用于计算KL散度）；</li> </ul> <p>训练时，RM模型需要独立训练，而Actor Model与Critic Model则是在PPO的过程中一起训练。如果两者共享参数，但是使用不同的head，我们也称Actor Model为策略网络，Critic Model为值网络。</p> <p>RLHF的过程相当繁琐，并且训练过程也并不非常稳定，因此LLaMa系列模型在开始的时候仅使用了SFT，但是在LLaMa2时又重新引入了RL进行偏好对齐，提升用户体验。</p> <h3 id=34-grpogroup-relative-policy-optimization>3.4. 策略梯度方法的改进&ndash;GRPO（Group Relative Policy Optimization）<a class=headerlink href=#34-grpogroup-relative-policy-optimization title="Permanent link">&para;</a></h3> <p>PPO算法中的值函数只对最后一个token评估，复杂并且引入额外的不确定性。GRPO能够避免值函数估计，使得整个训练过程更加简单，具体来说：</p> <div class=arithmatex>\[ J_{GRPO}(\theta) = \frac{1}{G} \sum_{i=1}^G\frac{1}{|o_i|} \sum_{t=1}^{o_i} \{ \min \left [ \frac{\pi_\theta}{\pi_{\theta_{old}}} \hat{A}_{i,t} clip\left( \frac{\pi_\theta}{\pi_{\theta_{old}}}, 1-\epsilon, 1+\epsilon \right) \hat{A}_{i,t} \right ] -\beta \mathbb{D}_{KL}[\pi_\theta \| \pi_{\theta_{old}}] \} \]</div> <p>对于每个问题<span class=arithmatex>\(q\)</span>，生成<span class=arithmatex>\(G\)</span>个回答<span class=arithmatex>\(o_i, i\in[1..G]\)</span>，每个答案的长度为<span class=arithmatex>\(|o_i|\)</span>。</p> <p>GRPO引入了一种全新的优势函数定义：</p> <div class=arithmatex>\[ \hat{A}_{i,t} = \frac{r_i - mean(r)}{std(r)} \]</div> <p>即第<span class=arithmatex>\(i\)</span>个输出的奖励<span class=arithmatex>\(r_i\)</span>，相对整个Group的优势，该式与Batch Normal极为类似。计算这个优势函数只需要计算每个输出的奖励即可，无需再估计值函数。</p> <h2 id=deepseek-r1>DeepSeek R1<a class=headerlink href=#deepseek-r1 title="Permanent link">&para;</a></h2> <p>奖励是GRPO整个训练的信号来源，DeepSeek R1在训练中主要使用基于规则的奖励。一方面基于规则的奖励能够节约训练资源，另一方面也能避免Reward Hacking问题。</p> <div class="admonition note"> <p class=admonition-title>什么是Reward Hacking</p> <p>Reward Hacking是指强化学习训练时，Agent通过不符合预期的方式，利用奖励函数的漏洞来最大化其奖励，从而破解训练过程。产生Reward Hacking的主要原因在于奖励函数本身不完美，特别是一些基于规则设计的复杂奖励函数，在不限定可行域的时候，经常会导致Agent尝试逃逸到非可行域，从而获得超额奖励。</p> </div> <h4 id=deepseek-r1-zero>DeepSeek R1-Zero<a class=headerlink href=#deepseek-r1-zero title="Permanent link">&para;</a></h4> <p>DeepSeek R1-Zero是纯粹基于强化学习训练的模型，主要采用了两种规则奖励：</p> <ul> <li> <p>Accuracy Rewards：主要同归规则，比如文本匹配和正则表达式来给出奖励，针对LeetCode甚至会使用compiler作为奖励反馈的来源；</p> </li> <li> <p>Format Rewards：奖励模型将思考过程放入<code>&lt;think&gt;...&lt;/think&gt;</code>标签中；</p> </li> </ul> <p>在训练的过程中，使用了如下的模板：</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>The assistant first thinks about the reasoning process in the mind and then provides the user
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>&lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt;
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>&lt;answer&gt; answer here &lt;/answer&gt;. User: prompt. Assistant:
</span></code></pre></div> <p>在R1-Zero的训练过程中，可以看到模型在Reasoning Task上持续稳定则性能改善。并且观测到了Aha-Moment。</p> <div class="admonition note"> <p class=admonition-title>Aha Moment</p> <p>Aha Moment是指某个人突然理解或者领悟某个概念、问题或者想法的瞬间，中文可以称作“顿悟”时刻，使得对某个问题的解决有一个飞跃。</p> </div> <p><a class=glightbox href=../../../../imgs/aha_moment.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/aha_moment.png></a></p> <p>虽然R1-Zero表现出了很轻的Reasoning能力，但其输出的可读性较差，并且会中英文。为了让模型的体现更好，又开发了R1，加入human-friendly的冷启动数据。</p> <h4 id=deepseek-r1_1>DeepSeek R1<a class=headerlink href=#deepseek-r1_1 title="Permanent link">&para;</a></h4> <h5 id=cold-start>Cold Start<a class=headerlink href=#cold-start title="Permanent link">&para;</a></h5> <p>使用数千条长CoT数据对模型进行微调：</p> <ul> <li>数据可读性：主要解决多语言混合问题与Markdown格式缺失问题；</li> <li>潜力：基于人工先验设计数据；</li> </ul> <h5 id=reasoning-rl>Reasoning RL<a class=headerlink href=#reasoning-rl title="Permanent link">&para;</a></h5> <p>使用RL提升模型的Reasoning能力，为了避免多语言混合，引入了惩罚项。</p> <h5 id=sft>拒绝采样与SFT<a class=headerlink href=#sft title="Permanent link">&para;</a></h5> <p>主要为了提升Reasoning以外的能力，比如写作和对话，通过SFT来实现。数据主要包含：</p> <ul> <li>Reasoning数据：使用生成式Reward函数，将ground truth和模型输出同时输入给base模型进行判断；以此种方式收集60W训练数据；</li> <li>Non-Reasong数据：使用prompt让模型为writing、QA等任务添加CoT，并收集20W输出作为训练数据；</li> </ul> <h5 id=rl-for-all-scenarios>RL for all scenarios<a class=headerlink href=#rl-for-all-scenarios title="Permanent link">&para;</a></h5> <p>再次通过RL提升模型的helpness和harmlessness。</p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最后更新> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年2月11日</span> </span> <span class=md-source-file__fact> <span class=md-icon title=创建日期> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年2月10日</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../../../../2024/01/25/review-dist-train/ class="md-footer__link md-footer__link--prev" aria-label="上一页: 关于分布式模型并行的分正式评论"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> 关于分布式模型并行的分正式评论 </div> </div> </a> <a href=../../12/fp8_training/ class="md-footer__link md-footer__link--next" aria-label="下一页: 从DeepSeek V3看FP8训练的挑战"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> 从DeepSeek V3看FP8训练的挑战 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2023 - 2025 Reiase </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/reiase target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../../assets/javascripts/bundle.f1b6f286.min.js></script> <script src=../../../../assets/_markdown_exec_pyodide.js></script> <script src=../../../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>