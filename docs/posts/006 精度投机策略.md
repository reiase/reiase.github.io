---
title: 精度投机策略——一种动态精度优化方法
date: 2025-03-06
tags:
  - LLM
  - Training
  - Training Dynamics
categories:
  - LLM
  - Training
  - Training Dynamics
slug: training_dynamics
draft: true
description: >-
  精度投机策略
---

## 1. 引言：从Training Dynamics到精度投机

在前三篇文章中，我们分别探讨了FP8训练的挑战与DeepSeek V3的成功、INT8训练的可能性与LLM训练过程中的权重与激活的动态性。低精度方法有能力带来训练性能质的提升，但受限于精度而无法充分发挥。而主要的制约因素就在于outlier导致低精度量化误差难以控制。想要处理好outlier就必然需要从动力学层面研究其在训练过程中产生与演化。

### 1.1. 从投机解码到精度投机

**什么是投机策略(Speculative Strategy)**: 投机策略是一种基于对未来的预测而采取执行动作的策略，旨在通过承担一定风险来获取潜在收益或优势。其核心在于“预判可能性，提前布局”，并在不确定的环境中平衡风险与回报。

**投机解码** 也被称为投机采样(Speculative Sampling，或推测解码Speculative Decoding)，尝试先用草案模型（Draft Model）进行"投机性"生成，然后再用主模型（Target Model）来判断是否接收草案模型的结果。若草案模型的结果被接收，则计算速度将显著提升；若投机失败，不仅浪费了草案模型的计算资源，还需要重新使用主模型进行生成。

**精度投机** 根据上述分析，投机策略的关键在于对未来的准确预测，在之前的文章中我们深入分析了激活值动态性。借助这些分析，我们可以预测计算过程中哪些阶段可能面临高动态性，从而有针对性地对特定计算环节应用低精度"投机"。而投机成功的关键在于准确评估低精度运算对最终结果精度的影响。我们首先讨论如何预测低精度量化对矩阵乘法的影响，再讨论如何借助这种预测来引入"投机"策略。

## 2. 激活分布估计与量化误差分析

在前文的分析中我们发现，神经网络激活值中的极端值(outlier)虽然数量极少，却造成了数值表示空间的严重浪费，导致低精度浮点数有限的动态范围大多分配给了几乎无值的数值区间。针对这一问题，我们将专注设计一种高效的分布估计方法，用以精确把握激活值的主体分布特征及outlier的绝对最大值(absmax)，为精度投机策略提供决策依据。

### 2.1. 激活分布估计

刻画激活分布需要刻画主要数值分布以及outlier，这里主要数值分布可以通过高斯分布$\mathbf{N}(\mu, \sigma)$来表示，outlier可以通过absmax表示。我们通过以下方法对激活的分布进行估计：

1. 指定采样率$\gamma$，比如取0.01
2. 初始化参数 $X=0$, $X^2=0$分别表示激活值的和与平方和，$N=0$表示激活值个数，$M=0$表示outlier的最大值；
3. 遍历所有激活值$x$:
    1. 若$|x| > M$，则令$M=|x|$；
    2. 取随机数$r$，若$r \le \gamma$则进行采样（执行后续步骤），否则跳过当前激活值回到步骤3处理下一个激活值；
    3. 更新$X$与$X^2$: $X \leftarrow X + x$，$X^2 \leftarrow X^2 + x^2$
    4. 更新样本计数$N \leftarrow N + 1$
4. 输出对均值和方差的估计$\mu = \frac{X}{N}, \sigma = \sqrt{\frac{X^2}{N}-(\frac{X}{N})^2}$，以及outlier的最大值$M$。

估计主要数值分布时，主要需要避免outlier的影响。上述算法直接通过负采样来避免outlier的命中，简单高效。若需要确保完全排除outlier对高斯分布估计的影响，可以进一步引入多重负采样。

**多重负采样** 是指进行多次独立的随机采样（如进行5次），并从中选择方差最小的那一次结果作为最终估计。其核心思想基于概率保证：如果outlier在总体中占比极小，那么进行多次独立采样，至少有一次采样会完全避开outlier。具体步骤如下：

1. 设定采样次数$K$（如$K=5$）和采样率$\gamma$
2. 重复执行$K$次上述采样过程，每次得到估计值$\mu_i$和$\sigma_i$
3. 选择$K$次采样中方差$\sigma_i^2$最小的结果作为最终估计

由于outlier会显著增大样本方差，选择方差最小的采样结果实际上就是选择最可能不含outlier的样本。这种方法无需复杂的outlier判定标准，实现简单且效果可靠，特别适合处理具有长尾分布特性的神经网络激活值。虽然进行多次采样会增加计算开销，但由于每次采样仅处理少量数据（采样率$\gamma$很小），总体计算量仍然可控。

设$\epsilon$ 为 单个元素为outlier的概率（通常$\epsilon \ll 1$），$\gamma$为采样率，$N$为总元素数量，$K$为独立采样次数。则：

- 单次采样包含$m=\gamma N$个元素；
- 采样不包含任何outlier的概率为$(1-\epsilon)^m$；
- K次多重采样中至少有一次安全的概率为$1-(1-(1-\epsilon)^m)^K$；

当$\epsilon$很小时，可利用泰勒展开近似：$(1-\epsilon)^m \approx e^{-m\epsilon}$，因此安全采样的概率可表示为$1-(1-e^{-m\epsilon})^K$。这意味着通过增加K值，我们可以显著提高获得无outlier样本的概率。

### 2.2. 量化误差分析

此处以定点量化为例，分析量化误差对主要数值分布的影响。对数值进行定点量化时，主要考虑outlier的分布来选取量化间隔：

$$
\Delta = \frac{2M}{2^{bits}-1}
$$

我们计算四舍五入量化下，主要数值分布落入区间$\left [-\frac{\Delta}{2},+\frac{\Delta}{2}\right]$的概率，落入该区间的数值会被量化为0：

$$
\begin{align*}
P_{\text{zero}} &= P\left(-\frac{\Delta}{2} < x < \frac{\Delta}{2}\right)
= \Phi\left(\frac{\Delta}{2\sigma}\right) - \Phi\left(\frac{-\Delta}{2\sigma}\right)\\
&=2\Phi\left(\frac{\Delta}{2\sigma}\right) - 1 \\
&= \text{erf}\left(\frac{\Delta}{2\sqrt{2}\sigma}\right)
\end{align*}
$$

基于$P_{\text{zero}}$（即数值被量化为零的概率）可以进一步评估矩阵乘的量化误差。根据之前文章的分析，我们可以把矩阵乘法的误差分析简化成向量内积的分析。由于数值被零化之后，该数值对最终内积结果的贡献为0，因此考虑一个向量的零化概率时相对误差可以简单的按$P_{\text{zero}}$来估计。若参与内积计算的两个向量的元素零化概率分别为$P_{Z1}$和$P_{Z2}$，则相对误差为$P_{Z1}+P_{Z2}-P_{Z1}P_{Z2}$，当零化概率足够小时可以忽略交叉项按$P_{Z1}+P_{Z2}$计算。假设输入向量的信号能量为1，则可推导内积的信噪比为：

$$
SNR = 10 \log_{10} \frac{1^2}{1\times(P_{Z1}+P_{Z2})^2} = -20 \log_{10} (P_{Z1}+P_{Z2})
$$

或者使用更准确的表达$-20 \log_{10} (P_{Z1}+P_{Z2}-P_{Z1}P_{Z2})$。接下来我们将基于对量化误差的估计来设计"投机"策略。

## 3. 精度投机策略

### 3.1. 投机算法流程

基于前一节量化误差信噪比的推导，我们可以构建精度投机的决策框架如下:

1. **分析阶段**: 使用多重负采样估计激活分布参数$\mu$、$\sigma$和最大outlier$M$
2. **评估阶段**: 
   - 计算不同精度下的量化间隔$\Delta = \frac{2M}{2^{bits}-1}$
   - 计算零化概率$P_{\text{zero}} = \text{erf}\left(\frac{\Delta}{2\sqrt{2}\sigma}\right)$
   - 估算向量内积的信噪比$SNR = -20 \times \log_{10}(P_{Z1} + P_{Z2})$
3. **决策阶段**:
    给定信噪比阈值$T$（T需要通过实验来标定）：
   - 若$SNR > T$ dB，可安全采用INT8精度；
   - 若$SNR \leq T$ dB，采用BF16精度；

决策阶段可以根据需要混入所需要的精度格式（比如INT4等更低精度）。在基于零化概率进行信噪比分析时，我们仅考虑了定点量化方法。浮点量化方法的量化误差分析暂不在此处讨论。

### 3.2. 投机策略的粒度

根据前文分析，在训练过程中随着权重的更新，权重矩阵与相关性矩阵都会发生显著变化，具体如下：
1. 权重矩阵会逐渐从完全随机矩阵退化为部分特征值较大的矩阵；
2. 激活值的数值分布会从完全高斯分布逐步退化为高斯分布带有较大outlier；

因此，精度投机的命中率可能会随着训练的进行而逐步下降。若使用layer层面的投机策略，命中失败带来的成本为整个layer退化到高精度计算。一方面会影响精度投机带来的收益，另一方面大范围的退化也在分布式训练中引入快慢节点问题，并最终导致集合通信等待时间变长。

为了解决粗粒度精度投机的退化问题，可以结合分块量化，在矩阵的block层面引入精度投机：针对每个分块估算其分布，更具分布选择精度策略（INT8或FP8 E5M2）。即对于存在较高动态outlier的block，使用FP8 E5M2存储，计算时使用FP16精度进行计算。为了区分INT8的block与FP8的block，可以将scaling系数的符号位作为标志位：scaling系数为正的block为INT8数值，scaling系数为负的block为FP8数值。
  
## 当我们谈论"投机"时到底在谈论什么？

"投机"（Speculative）一词更为准确的含义是推测、推断，意指在未来尚未可知时基于合理的预测提前行动。这一概念在芯片领域早有应用，如CPU的分支预测和推测执行，它们通过"赌"下一步可能的执行路径来提高性能。无论投机成功与否，这种策略的核心都是：接受小概率失败的风险，以换取大概率下的性能收益。

精度投机策略正是这一思想在深度学习训练中的延伸。我们并不盲目地为所有计算统一选择保守的高精度，而是基于数据特征做出"投机性"的精度决策。这种"大胆尝试，谨慎撤退"的策略，本质上是一种资源分配智慧。毕竟，为了捕捉极少数的异常值而对所有数据使用高精度计算，就像为了极少数的跳转指令就停下整个CPU流水线一样浪费资源。

另一方面，精度投机策略也意味着进一步打破异步计算软件栈的僵化分层，让底层计算逻辑主动感知上层workload的特性。个人认为这种方式是进一步突破字节层面精度限制的有效途径。事实上深度学习领域一直存在着过渡计算的问题：训练好的模型通过量化压缩2倍以上都几乎精度无损，那是否意味着训练阶段我们浪费了超过一半的算力呢。DeepSeek V3的成功已经证明真正的技术突破不是来自算力的简单堆砌，而是来自对问题本质的深刻理解和巧妙应对。精度投机策略正是这样一种尝试——与其被硬件精度限制所束缚，不如聪明地玩转这些精度。
