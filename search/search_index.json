{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stemmer"]},"docs":[{"location":"","title":"Blog","text":""},{"location":"tags/","title":"\u6807\u7b7e","text":""},{"location":"tags/#tag:llm","title":"LLM","text":"<ul> <li>            Kaplan Scaling Law vs Chinchilla Optimal          </li> <li>            LLM\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u975e\u6b63\u5f0f\u8bc4\u8bba          </li> <li>            \u4ece\u5f3a\u5316\u5b66\u4e60\u5230DeepSeek R1          </li> <li>            \u5173\u4e8e\u5206\u5e03\u5f0f\u6a21\u578b\u5e76\u884c\u7684\u5206\u6b63\u5f0f\u8bc4\u8bba          </li> </ul>"},{"location":"tags/#tag:pretrain","title":"Pretrain","text":"<ul> <li>            Kaplan Scaling Law vs Chinchilla Optimal          </li> <li>            LLM\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u975e\u6b63\u5f0f\u8bc4\u8bba          </li> </ul>"},{"location":"tags/#tag:rl","title":"RL","text":"<ul> <li>            \u4ece\u5f3a\u5316\u5b66\u4e60\u5230DeepSeek R1          </li> </ul>"},{"location":"tags/#tag:training","title":"Training","text":"<ul> <li>            AdamW          </li> <li>            \u4ece\u5f3a\u5316\u5b66\u4e60\u5230DeepSeek R1          </li> <li>            \u5173\u4e8e\u5206\u5e03\u5f0f\u6a21\u578b\u5e76\u884c\u7684\u5206\u6b63\u5f0f\u8bc4\u8bba          </li> </ul>"},{"location":"nodes/AdamW/","title":"AdamW","text":"<p>LLM\u8bad\u7ec3\u4e2d\u901a\u5e38\u4f7f\u7528AdamW\u4f18\u5316\u5668\uff0c\u914d\u5408grad clip\u53c2\u65701.0\u4e0eweight decay\u53c2\u65700.1\u4f7f\u7528\u3002AdamW\u4f18\u5316\u5668\u7684\u4f2a\u4ee3\u7801\u5165\u4e0b\u56fe\u6240\u793a\uff1a </p>","tags":["Training"]},{"location":"nodes/AdamW/#weight-decay","title":"weight decay","text":"<p>\u4e0b\u56fe\u7b80\u5355\u7ed8\u5236\u4e86\u6743\u91cd\u56e0weight decay\u968f\u8bad\u7ec3\u8870\u51cf\u7684\u60c5\u51b5\uff0c\\(\\theta_{t+n} = \\gamma^n\\theta_t\\)\uff0c\u5f53\\(\\gamma\\)\u5206\u522b\u53d60.01\u4e0e0.1\u65f6\uff0c\u8bad\u7ec3100\u4e2astep\u540e\u6743\u91cd\u8870\u51cf\u5982\u4e0b\uff1a  \u5f53\\(\\gamma=0.1\\)\u65f6\uff0c40\u4e2astep\u540e\u6743\u91cd\u8870\u51cf\u4e3a\u539f\u6765\u76841%\uff0c\u57fa\u672c\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002\u56e0\u6b64\u5728\u5b9e\u9645LLM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6a21\u578b\u7684\u9057\u5fd8\u4f5c\u7528\u975e\u5e38\u5f3a\u3002</p>","tags":["Training"]},{"location":"nodes/AdamW/#_1","title":"\u7a33\u6001\u5206\u6790","text":"<p>\u82e5\u5c06AdamW\u4f18\u5316\u5668\u770b\u6210\u4e00\u4e2a\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5e76\u5047\u8bbe\u6a21\u578b\u6536\u655b\u65f6\u6743\u91cd\u66f4\u65b0\u8fd1\u4f3c\u4e3a0\uff0c\u5373 $$     \\theta_{t} = \\theta_{t-1} $$ \u5e26\u5165AdamW\u7684\u6700\u540e\u66f4\u65b0\u516c\u5f0f $$     \\theta_t = \\theta_{t-1} - \\lambda \\theta_{t-1} - \\gamma \\frac{m_t}{\\sqrt{v_t}+\\epsilon} $$ \u6709\uff1a $$     \\lambda \\theta_{t-1} = - \\gamma\\frac{m_t}{\\sqrt{v_t}+\\epsilon} $$ \u5373\u7cfb\u7edf\u8fbe\u5230\u7a33\u6001\u65f6\uff0c\u6743\u91cd\u6b63\u6bd4\u4e8e\u77ed\u65f6\u95f4\u5185\u68af\u5ea6\u7684\u6ed1\u52a8\u5e73\u5747\u3002</p>","tags":["Training"]},{"location":"nodes/AdamW/#_2","title":"\u68af\u5ea6\u5206\u6790","text":"<p>\u82e5\u7cfb\u7edf\u8fbe\u5230\u7a33\u6001\uff0c\\(\\theta_t\\) \u8fd1\u4f3c\u4e0d\u53d8\uff0c\u5219\u53ef\u4ee5\u8ba4\u4e3a\u6a21\u578b\u5728\u8be5\u6743\u91cd\u5904\u7684\u68af\u5ea6\u8fd1\u4f3c\u4e0d\u53d8\uff0c\u6b64\u65f6\\(|m_t| \\simeq \\sqrt{v_t}\\)\uff0c\u5219\u6709 $$         \\lambda \\theta_{t-1} = - \\gamma\\frac{m_t / |m_t| }{\\sqrt{v_t}/|m_t|+\\epsilon/|m_t|} = - \\gamma\\frac{sign(m_t)}{1+\\epsilon/|m_t|} $$ $$     \\theta_t = - \\frac{\\gamma}{\\lambda} \\times \\frac{sign(m_t)}{1+\\epsilon/|m_t|} $$ \u8003\u8651\u5230\u7cfb\u7edf\u8fbe\u5230\u7a33\u6001\u65f6\\(m_t = g_t\\)\uff0c\u5219\uff1a $$     \\theta_t = - \\frac{\\gamma}{\\lambda} \\times \\frac{sign(g_t)}{1+\\epsilon/|g_t|} = - \\frac{\\gamma}{\\lambda} \\times \\frac{sign(\\nabla L(\\theta_t))}{1+\\epsilon /  |\\nabla L(\\theta_t)|}  $$ \u4e0a\u8ff0\u516c\u5f0f\u4e3a\u4e00\u4e2a\u4e00\u9636\u5fae\u5206\u65b9\u7a0b\uff0c\u7cfb\u7edf\u7a33\u6001\u5bf9\u5e94\u8be5\u4e00\u9636\u5fae\u5206\u65b9\u7a0b\u7684\u89e3\u3002</p>","tags":["Training"]},{"location":"nodes/Random%20Round/","title":"Random Round","text":"<p>Random Round\u662f\u6307\u6d6e\u70b9\u6570\u91cf\u5316\u65f6\uff0c\u968f\u673around\u5230\u76f8\u90bb\u7684\u6570\u503c\uff0c\u800c\u4e0d\u662fround\u5230\u6700\u8fd1\u7684\u6570\u503c\u3002\u968f\u673around\u867d\u7136\u589e\u52a0\u4e86\u5355\u4e2a\u6570\u503c\u7684\u91cf\u5316\u8bef\u5dee\uff0c\u4f46\u662f\u80fd\u591f\u907f\u514d\u6574\u4f53\u7684\u6709\u504f\u91cf\u5316\u8bef\u5dee\u3002\u56e0\u6b64\uff0c\u5728\u67d0\u4e9b\u8bad\u7ec3\u573a\u666f\u4e0b\u80fd\u591f\u907f\u514d\u6709\u504f\u91cf\u5316\u8bef\u5dee\u5e26\u6765\u7684\u6536\u655b\u901f\u5ea6\u6162\u95ee\u9898\uff0c\u5177\u4f53\u53ef\u89c1Gopher\u8bba\u6587<sup>1</sup> \u7684Figure A7\u3002 </p> <ol> <li> <p>2021, DeepMind, Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher\u00a0\u21a9</p> </li> </ol>"},{"location":"nodes/Scaling%20Law%20%E8%AE%A1%E7%AE%97%E5%99%A8/","title":"Scaling Law \u8ba1\u7b97\u5668","text":""},{"location":"nodes/Scaling%20Law%20%E8%AE%A1%E7%AE%97%E5%99%A8/#kaplan-scaling-law","title":"Kaplan Scaling Law","text":"\u8ba1\u7b97\u5668\u9996\u6b21\u52a0\u8f7d\u9700\u8981\u65f6\u95f4 <p> Editor (session: default) Run <pre>import micropip\n\nawait micropip.install(\"https://cdn.holoviz.org/panel/1.3.8/dist/wheels/bokeh-3.3.3-py3-none-any.whl\")\nawait micropip.install(\"https://cdn.holoviz.org/panel/1.3.8/dist/wheels/panel-1.3.8-py3-none-any.whl\")\n\nimport panel as pn\nfrom bokeh.models.formatters import PrintfTickFormatter\n\npn.extension(sizing_mode=\"stretch_width\")\n\nModelSelector = pn.widgets.Select(name=\"model\", value=\"LLaMa2 7B\", options=[\n    \"Custom Model\",\n    \"LLaMa 7B\",\n    \"LLaMa2 7B\",\n])\n\nNSlider = pn.widgets.FloatSlider(start=1, end=200, step=0.1, \n    format=PrintfTickFormatter(format='%.1f B'), name='\u6a21\u578b\u89c4\u6a21\uff08B\uff09')\nDSlider = pn.widgets.FloatSlider(start=1, end=5000, step=100, \n    format=PrintfTickFormatter(format='%.1f B'), name='\u6570\u636e\u89c4\u6a21\uff08B\uff09')\nCText = pn.widgets.StaticText(name=\"\u7b97\u529b\u9700\u6c42C\uff08pf-day\uff09\")\npfday = pn.widgets.StaticText(value=\"pf-day\u4e3a1P FLOPs x 24\u5c0f\u65f6\uff0c\u7ea6\u4e3a8.64E19\")\n\ndef callback(new):\n    ModelSelector.value = \"Custom Model\"\n    CText.value = \"%.1f pf-day\"%(6*NSlider.value*1E9*DSlider.value*1E9/8.64E19)\n    return\n\ndef select_model(new):\n    if new == \"Custom Model\":\n        return\n    if new == \"LLaMa2 7B\":\n        NSlider.value=7.0\n        DSlider.value=2000\n    elif new == \"LLaMa 7B\":\n        NSlider.value=7.0\n        DSlider.value=1500\n\n\npn.Column(\n    ModelSelector,\n    NSlider, DSlider, \n    CText, pfday,\n    pn.bind(callback, NSlider),\n    pn.bind(callback, DSlider),\n    pn.bind(select_model, ModelSelector),\n).servable(target='kaplan_scaling');</pre> Output Clear <pre></pre> </p>"},{"location":"nodes/kaplan%20vs%20chinchilla/","title":"Kaplan Scaling Law vs Chinchilla Optimal","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n</code></pre> <pre><code>def LN(n):\n    return (8.8*(10**13)/n)**0.076\n\ndef LD(d):\n    return (5.4*(10**13)/d)**0.095\n\ndef ld(l):\n    return 5.4*(10**13)/(l**(1/0.095))\n</code></pre> <p>GPT3\u7684\u53c2\u6570\u91cf\u4e3a175B\uff0c\u9884\u6d4b\u7684loss\u503c\u4e3a<code>LN(175*10**9)</code>\uff0c\u6240\u9700token\u6570\u91cf\u4e3a<code>ld(LN(175*10**9))</code>\u3002</p>","tags":["LLM","Pretrain"]},{"location":"2025/02/09/rl_ds_r1/","title":"\u4ece\u5f3a\u5316\u5b66\u4e60\u5230DeepSeek R1","text":"","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#1-rl-reinforcement-learning","title":"1. \u4ec0\u4e48\u662f\u5f3a\u5316\u5b66\u4e60(RL, Reinforcement Learning)","text":"<p>\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\uff0c\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\uff0c\u5176\u672c\u8d28\u662f\u6570\u5b66\u6027\u7684\uff0c\u4e25\u683c\u9075\u5b88\u51fd\u6570\u7684\u6570\u5b66\u5b9a\u4e49\uff1a\u5bf9\u4e8e\u7ed9\u5b9a\u8f93\u5165\uff0c\u4ea7\u751f\u786e\u5b9a\u7684\u8f93\u51fa</p> \\[F(x) = y\\] <p>\u968f\u7740\u8f93\u5165\\(x\\)\u548c\u8f93\u51fa\\(y\\)\u7684\u4e0d\u540c\uff0c\u8fd9\u4e00\u8303\u5f0f\u53ef\u4ee5\u9002\u914d\u5404\u79cd\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u6bd4\u5982\uff1a</p> <ul> <li>\\(x\\) \u662f\u56fe\u50cf\uff0c\\(y\\)\u662f\u7c7b\u522b\uff0c\u90a3\u4e48\\(F\\)\u5c31\u662fResnet\u8fd9\u79cd\u56fe\u50cf\u6a21\u578b\uff1b</li> <li>\\(x\\) \u662f\u8bed\u97f3\u4fe1\u53f7\uff0c\\(y\\)\u662f\u6587\u5b57\uff0c\u90a3\u4e48\\(F\\)\u5c31\u662f\u4e00\u4e2a\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff1b</li> <li>\\(x\\) \u662f\u6587\u672c\u8f93\u5165\uff0c\\(y\\)\u662f\u6587\u672c\u8f93\u51fa\uff0c\u90a3\u4e48\\(F\\)\u5c31\u662f\u65f6\u4e0b\u706b\u70ed\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff1b \u2026</li> </ul> <p>\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff09\u7684\u672c\u8d28\u4e0a\u5219\u662f\u54f2\u5b66\u6027\u7684\uff0c\u5b83\u63a2\u8ba8\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a</p> <ul> <li>\u6211\u662f\u8c01\uff1f\u4e00\u4e2aAgent</li> <li>\u6211\u5728\u54ea\uff1f\u5904\u4e8e\u67d0\u4e2aState</li> <li>\u5230\u54ea\u91cc\u53bb\uff1f\u91c7\u53d6\u4e00\u4e2aAction</li> </ul> <p>\u5982\u679c\u7ad9\u5728\u4e0a\u5e1d\u89c6\u89d2\u53bb\u89c2\u6d4b\u8fd9\u4e2aAgent\uff0c\u6211\u4eec\u8fd8\u4f1a\u53d1\u73b0\uff1a</p> <ul> <li>Agent\u5904\u5728\u4e00\u4e2a\u73af\u5883\u4e2d\uff08Environment\uff09</li> <li>Agent\u6709\u4e00\u4e2a\u7528\u6765\u7b56\u7565\uff08Policy\uff09\u544a\u8bc9\u6211\u8be5\u91c7\u53d6\u4ec0\u4e48\u52a8\u4f5c\uff08Action\uff09</li> <li>\u6bcf\u6267\u884c\u4e00\u4e2a\u52a8\u4f5c\uff08Action\uff09\uff0c\u73af\u5883\u90fd\u4f1a\u7ed9\u6211\u53cd\u9988 (Reward)</li> </ul> <p>\u4ee5\u4e0a\u5c31\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e3b\u8981\u6982\u5ff5\u3002</p> <p></p>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#2","title":"2. \u5982\u4f55\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60","text":"<p>\u8fd9\u91cc\u4ee5\u4e00\u4e2a\u8ff7\u5bab\u95ee\u9898\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff1a</p> <p>\u8ff7\u5bab\uff1a(S: Start, E: End, W: Wall)</p> S1(S) S2 S3(W) S4 S5 S6 S7(W) S8 S9(E) <p>\u8fd9\u4e2a\u8ff7\u5bab\u5c31\u662f\u4e00\u4e2aEnvironment\u3002\u6211\u4eec\u653e\u7f6e\u4e00\u4e2a\u673a\u5668\u4eba\u5728\u5f00\u59cb\u5904\uff08Start\uff09\uff0c\u8ba9\u673a\u5668\u4eba\u81ea\u52a8\u5b66\u4e60\u5982\u4f55\u8d70\u8ff7\u5bab\u7684\u7b56\u7565\uff08Policy\uff09\u3002\u8fd9\u4e2a\u7b56\u7565\u53ef\u4ee5\u8bb0\u6210\\(\\pi(s)\\rightarrow a, s \\in [1-9], a \\in [\u4e0a, \u4e0b, \u5de6, \u53f3]\\)\u3002\u5f00\u59cb\u65f6\u673a\u5668\u4eba\u5bf9\u4e8e\u8ff7\u5bab\u4e00\u65e0\u6240\u77e5\uff0c\u6240\u4ee5\\(\\pi(s)\u4f1a\u968f\u673a\u8f93\u51fa\u4e00\u4e2a\u65b9\u5411\\)\u3002</p>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#21-q-learning","title":"2.1. Q-Learning","text":"<p>Q-Learning\u662f\u6700\u65e9\uff0c\u4e5f\u662f\u6700\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u524d\u6587\u63d0\u5230\u7684\u7b56\u7565\\(\\pi(s)\\)\u8fc7\u4e8e\u62bd\u8c61\u4e86\uff0c\u6240\u4ee5\u6211\u4eec\u4e5f\u5b9a\u4e49\u4ee5\u4e0b\u8868\u683c</p> \u4e0a \u4e0b \u5de6 \u53f3 s1 s2 \u2026 <p>\u6211\u4eec\u79f0\u8fd9\u5f20\u8868\u4e3aQ\u503c\u8868\uff0c\u8868\u91cc\u8fb9\u7684\u5143\u7d20 \\(Q(s, a)\\)\u8868\u793a\u5728\u72b6\u6001\\(s\\)\u4e0b\uff0c\u91c7\u53d6\u52a8\u4f5c\\(a\\)\u7684\u5956\u52b1\\(reward\\)\u3002\u57fa\u4e8e\u8fd9\u5f20Q\u503c\u8868\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49\u7b56\u7565\uff1a $$ \\pi(s_i) \\rightarrow \\arg\\max_a Q(s, a)|_{s=s_i} $$ \u5b8c\u6210\u8ff7\u5bab\u6240\u9700\u8981\u7684\u6700\u5c0f\u7684Q\u503c\u8868\u5982\u4e0b\uff1a</p> \u4e0a \u4e0b \u5de6 \u53f3 s1 -1 1 -1 0 s4 0 -1 -1 1 s5 0 1 0 0 s8 0 -1 -1 1 <p>\u53ef\u4ee5\u6307\u6325Agent\u6cbf\u5982\u4e0b\u8def\u5f84\u884c\u8fdb</p> S1(S)\\(\\downarrow\\) S2 S3(W) S4\\(\\rightarrow\\) S5\\(\\downarrow\\) S6 S7(W) S8\\(\\rightarrow\\) S9(E) <p>\u4f46\u662f\u6211\u4eec\u5f88\u5feb\u53d1\u73b0Agent\u884c\u8fdb\u7684\u8def\u5f84\u5e76\u4e0d\u552f\u4e00\uff0c\u6bd4\u5982\u4ee5\u4e0b\u8def\u5f84\u4e5f\u80fd\u8ba9Agent\u8d70\u5230\u7ec8\u70b9</p> S1(S)\\(\\rightarrow\\) S2   \\(\\downarrow\\) S3(W) S4 S5\\(\\downarrow\\) S6 S7(W) S8\\(\\rightarrow\\) S9(E) <p>\u8fd9\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u6a21\u578b\u7684\u89e3\u4e0d\u552f\u4e00\u3002\u7a0d\u540e\u6211\u4eec\u8ba8\u8bba\u8fd9\u79cd\u89e3\u4e0d\u552f\u4e00\u5e26\u6765\u7684\u95ee\u9898\u3002</p>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#22-q-learning","title":"2.2. Q-Learning \u7684\u5b66\u4e60\u8fc7\u7a0b","text":"<ol> <li>Q\u503c\u8868\u521d\u59cb\u5316\uff0c\u53ef\u4ee5\u5168\u90e8\u521d\u59cb\u5316\u4e3a0\uff1b</li> <li> <p>\u8fed\u4ee3\u5b66\u4e60\uff1a\u6211\u4eec\u5047\u8bbeAgent\u88ab\u521d\u59cb\u5316\u5728S0\u4f4d\u7f6e</p> <ol> <li> <p>\u9009\u62e9action </p> \\[a = \\pi(s_i) = \\arg\\max_a Q(s, a)|_{s=s_i}\\] </li> <li> <p>\u6267\u884caction, \u73af\u5883Environment\u7ed9\u51fa\u4e0b\u4e00\u4e2a\u72b6\u6001\\(s'\u4e0e\u5956\u52b1\\)r$ \\(\\(s',r = Environment(s, a)\\)\\)</p> </li> <li> <p>\u66f4\u65b0Q\u503c\uff0c\u8fd9\u91cc\u4e00\u822c\u4f7f\u7528Bellman\u65b9\u7a0b\uff1a \\(\\(Q(s,a)\\leftarrow Q(s,a) + \\alpha [r+\\gamma \\max_{a'}Q(s',a')-Q(s,a)]\\)\\)</p> </li> </ol> <p>\u5176\u4e2d\uff1a</p> <ul> <li>\\(\\alpha\\) \u4e3a\u5b66\u4e60\u7387\uff0c\u63a7\u5236Q\u503c\u8868\u7684\u66f4\u65b0\u5e45\u5ea6\uff1b</li> <li>\\(\\gamma\\) \u4e3a\u6298\u6263\u5f15\u5b50\uff0c\u63a7\u5236\u957f\u671f\u5956\u52b1\u4e0e\u5373\u65f6\u5956\u52b1\u7684\u5e73\u8861\uff1b</li> </ul> </li> </ol> <p>Q-Learning\u672c\u8d28\u4e0a\u5c31\u662f\u8bb0\u4f4f\u5f53\u524d\u683c\u5b50\u7684\u5956\u52b1\uff0c\u540c\u65f6\u4e0d\u65ad\u6839\u636e\u672a\u6765\u4ef7\u503c\u91cd\u65b0\u8bc4\u4f30\u5f53\u524d\u683c\u5b50\u7684\u4ef7\u503c\u3002\u6700\u4f18\u60c5\u51b5\u4e0b\uff0c\u603b\u662f\u9009\u62e9\u672a\u6765\u4ef7\u503c\u6700\u9ad8\u7684\u52a8\u4f5c\u3002\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u4e2a\u683c\u5b50\u7684Q\u503c\u603b\u662f\u4f9d\u8d56\u4e8e\u5176\u4ed6\u683c\u5b50\u7684Q\u503c\uff0c\u683c\u5b50\u4e4b\u95f4\u76f8\u4e92\u4f9d\u8d56\uff0c\u975e\u5e38\u5bb9\u6613\u5bfc\u81f4Q\u503c\u957f\u671f\u9707\u8361\u65e0\u6cd5\u6536\u655b\u3002\u6b64\u65f6\u9700\u8981\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u8fdb\u884c\u7cbe\u7ec6\u6311\u53c2\u624d\u80fd\u4fdd\u8bc1\u5b66\u4e60\u8fc7\u7a0b\u7684\u6536\u655b\u6027\u3002\u8fd9\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u7b2c\u4e8c\u4e2a\u95ee\u9898\u3002</p>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#221-exploitation-exploration","title":"2.2.1. \u5229\u7528\u548c\u63a2\u7d22\uff08exploitation &amp; exploration\uff09","text":"<p>\u5982\u679c\u5355\u7eaf\u4f7f\u7528\u4e0a\u8ff0\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f88\u5bb9\u6613\u51fa\u73b0\u5982\u4e0b\u95ee\u9898\uff1a</p> S1(S)\\(\\rightarrow\\) S2   \\(\\downarrow\\) S3(W) S4 S5\\(\\uparrow\\) S6 S7(W) S8 S9(E) <p>\u5373Agent\u5728S2\u548cS5\u4e4b\u95f4\u53cd\u590d\u9707\u8361\uff0c\u65e0\u6cd5\u771f\u7684\u8d70\u5230\u7ec8\u70b9S9\u3002\u5176\u539f\u56e0\u5728\u4e8eAgent\u53ea\u80fd\u83b7\u53d6\u5176\u5386\u53f2\u8def\u5f84\u4e0a\u7684Q\u503c\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2a\u4e16\u754c\uff08Environment\uff09\u7684\u8ba4\u77e5\uff0c\u65e0\u6cd5\u53d1\u73b0S8\u548cS6\u8fd9\u79cd\u66f4\u52a0\u9760\u8fd1\u7ec8\u70b9\u7684\u8def\u5f84\u3002\u6211\u4eec\u79f0\u8fd9\u79cd\u5229\u7528\u5386\u53f2\u77e5\u8bc6\u7684\u8fc7\u7a0b\u4e3a\u201c\u5229\u7528\u201d\uff08exploitation\uff09\u3002</p> <p>\u9664\u4e86\u201c\u5229\u7528\u201d\u4ee5\u5916\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u8ba9Agent\u6709\u4e00\u5b9a\u7684\u201c\u63a2\u7d22\u201d\uff08exploration\uff09\u80fd\u529b\uff0c\u4fdd\u6301\u5bf9\u4e16\u754c\u7684\u597d\u5947\u5fc3\u3002\u6700\u5e38\u89c1\u7684\u63a2\u7d22\u65b9\u6cd5\u662f\u4f7f\u7528\\(\\epsilon-greedy\\)\u6539\u9020\u7b56\u7565 \\(\\phi\\): $$ \\phi(s_i) =  \\begin{cases}  \\pi(s_i) &amp; \\text{with probability } 1 - \\epsilon \\ \\text{random action} &amp; \\text{with probability } \\epsilon  \\end{cases} $$ \u5373\u4ee5\u6982\u7387\\(1-\\epsilon\\)\u9009\u62e9\u6700\u4f18\u52a8\u4f5c\uff0c\u4ee5\u6982\u7387 \\(\\epsilon\\) \u9009\u62e9\u968f\u673a\u52a8\u4f5c\u3002\u4e3a\u4e86\u8ba9Q-Learning\u66f4\u597d\u7684\u6536\u655b\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u964d\u4f4e\u63a2\u7d22\u6982\u7387\\(\\epsilon\\)\u3002</p>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#222","title":"2.2.2. \u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807","text":"<p>\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u76ee\u6807\u662f\u627e\u5230\u4e00\u4e2a\u6700\u4f18\u7b56\u7565\\(\\pi^*\\)\uff0c\u4f7fAgent\u5728\u5176\u751f\u547d\u5468\u671f\u5185\u83b7\u5f97\u7684\u671f\u671b\u5956\u52b1\u6700\u5927\u5316\uff1a</p> \\[J(\\pi) = \\mathbb{E}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid \\pi \\right]\\] <p>\u76f4\u63a5\u5728\u7b56\u7565\u7a7a\u95f4\u4f18\u5316\\(\\pi\\) \u901a\u5e38\u662f\u4e0d\u53ef\u884c\u7684\uff1a</p> <ul> <li>\u7b56\u7565\u7a7a\u95f4\u8fc7\u5927\uff1a\\(\\pi(a|s)\\)\u662f\u4e00\u4e2a\u6982\u7387\u5206\u5e03\uff0c\u6781\u5927\u7684\u589e\u52a0\u4e86\u641c\u7d22\u96be\u5ea6\uff1b</li> <li>\u65e0\u68af\u5ea6\u4fe1\u606f\uff1a\u4e0d\u7ecf\u8fc7\u7279\u6b8a\u8bbe\u8ba1\uff0c\u96be\u4ee5\u76f4\u63a5\u5bf9\u7b56\u7565\u6c42\u5bfc\uff1b</li> </ul> <p>\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u5f15\u5165\u4e86**\u503c\u51fd\u6570**\u6765\u7b80\u5316\u4f18\u5316\u8fc7\u7a0b\uff0c\u503c\u51fd\u6570\u4e00\u822c\u6709\u4e24\u7c7b\uff1a</p> <ul> <li>\u72b6\u6001\u4ef7\u503c\u51fd\u6570\\(V^\\pi(s)\\): \u8868\u793a\u4ece\u67d0\u4e2a\u72b6\u6001\\(s\\)\u5f00\u59cb\uff0c\u9075\u5faa\u7b56\u7565\\(\\pi\\)\u540e\uff0c\u6240\u80fd\u83b7\u5f97\u7684\u957f\u671f\u56de\u62a5 $$ V^\\pi(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s \\right] $$</li> <li>\\(\\gamma\\)\u4e3a\u6298\u6263\u56e0\u5b50\uff0c\u63a7\u5236\u672a\u6765\u5956\u52b1\u7684\u91cd\u8981\u6027\uff1b</li> <li>\\(r_t\\)\u4e3a\u65f6\u523b\\(t\\)\u7684\u5373\u65f6\u5956\u52b1\uff1b</li> <li>\\(\\mathbb{{E}}_\\pi\\)\u8868\u793a\u5bf9\u7b56\u7565\\(\\pi\\)\u7684\u6240\u6709\u53ef\u80fd\u591a\u505a\u6c42\u671f\u671b\u3002</li> </ul> <p>Bellman\u65b9\u7a0b\u662f\u4e0a\u8ff0\u503c\u51fd\u6570\u5b9a\u4e49\u7684\u9012\u5f52\u5f62\u4f3c\uff0c\u5c06\u4e0a\u8ff0\u5b9a\u4e49\u62c6\u89e3\u6210\u4e86\u5f53\u524d\u5956\u52b1\u548c\u672a\u6765\u72b6\u6001\u4ef7\u503c\uff1a $$ V^\\pi(s) = \\sum_{a} \\pi(a|s) [r(s, a)+\\gamma V^\\pi(s\u2019)] $$</p> <ul> <li> <p>\u52a8\u4f5c\u4ef7\u503c\u51fd\u6570\\(Q^\\pi(s, a)\\): \u4e0e\u72b6\u6001\u4ef7\u503c\u51fd\u6570\u7c7b\u4f3c\uff0c\u53ea\u662f\u8868\u793a\u72b6\u6001\\(s\\)\u4e0b\u6267\u884c\u52a8\u4f5c\\(a\\)\u540e\uff0c\u80fd\u591f\u5f97\u5230\u7684\u957f\u671f\u56de\u62a5 $$</p> <p>Q^\\pi(s,a) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0=a \\right]</p> </li> </ul> <p>$$</p>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#223","title":"2.2.3. \u5f3a\u5316\u5b66\u4e60\u7684\u6311\u6218","text":"<ol> <li> <p>\u8bad\u7ec3\u6837\u672c\u7684\u6982\u7387\u5206\u5e03\u95ee\u9898 \u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u4e00\u822c\u8981\u6c42\u8bad\u7ec3\u6837\u672c\u548c\u6d4b\u8bd5\u6837\u672c\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u5373\u6837\u672c\u72ec\u7acb\u62bd\u6837\uff0c\u4f46\u6837\u672c\u6765\u6e90\u7684\u6982\u7387\u5206\u5e03\u4fdd\u6301\u4e0d\u53d8\u3002\u8fd9\u6837\u624d\u80fd\u8ba9\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u4ece\u6570\u636e\u91cc\u8fb9\u603b\u7ed3\u548c\u5b66\u4e60\u89c4\u5f8b\u3002\u800c\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u83b7\u53d6\u6837\u672c\uff0cAgent\u7684\u521d\u59cb\u72b6\u6001\u3001\u8fd0\u884c\u8f68\u8ff9\u90fd\u4f1a\u5f71\u54cd\u91c7\u6837\u8fc7\u7a0b\u3002\u5373\u4e0d\u80fd\u4fdd\u8bc1\u4e24\u6b21\u8bad\u7ec3\u4e4b\u95f4\u7684\u6837\u672c\u662f\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u4e5f\u65e0\u6cd5\u4fdd\u8bc1\u6d4b\u8bd5\u4e0e\u8bad\u7ec3\u7684\u6837\u672c\u662f\u72ec\u7acb\u540c\u5206\u5e03\u3002</p> </li> <li> <p>\u90e8\u5206\u89c2\u6d4b\u4e0e\u73af\u5883\u968f\u673a\u6027 \u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u53ea\u80fd\u5bf9\u6574\u4e2a\u4e16\u754c\uff08Environment\uff09\u8fdb\u884c\u90e8\u5206\u89c2\u6d4b\uff0c\u4e0d\u80fd\u611f\u77e5\u6574\u4e2a\u4e16\u754c\uff08\u6ca1\u89c1\u8fc7\u5e02\u9762\uff09\u3002\u6a21\u578b\u7ecf\u5e38\u51fa\u73b0\u5bf9\u5c40\u90e8\u8fc7\u5ea6\u5b66\u4e60\u3001\u8fc7\u62df\u5408\uff0c\u5bf9\u5168\u5c40\u6b20\u62df\u5408\u3002\u7531\u4e8e\u65e0\u6cd5\u754c\u5b9a\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\u90e8\u5206\u7684\u8fb9\u754c\uff0c\u96be\u4ee5\u4fdd\u8bc1Agent\u5728\u672a\u89c1\u8fc7\u7684\u65b0\u73af\u5883\u4e2d\u884c\u4e3a\u7684\u5408\u7406\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>\u5ef6\u8fdf\u53cd\u9988\u4e0e\u7a00\u758f\u5956\u52b1 \u5728\u8bb8\u591a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u4e2d\uff0c\u5956\u52b1\u4fe1\u53f7\u5f80\u5f80\u662f\u5ef6\u8fdf\u4e14\u7a00\u758f\u7684\uff0c\u5bfc\u81f4Agent\u96be\u4ee5\u53ca\u65f6\u5224\u5b9a\u884c\u4e3a\u7684\u597d\u574f\u3002\u8fd9\u79cd\u6fc0\u52b1\u4fe1\u53f7\u7684\u4e0d\u8fde\u7eed\u6027\u4f7f\u5f97\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u65b9\u5dee\u53d8\u5927\uff0c\u4f1a\u589e\u52a0\u4f18\u5316\u96be\u5ea6\uff0c\u5fc5\u987b\u91c7\u7528\u5956\u52b1\u6574\u5f62\u3001\u7ecf\u9a8c\u56de\u653e\u7b49\u7b56\u7565\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002</p> </li> </ol>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#3","title":"3. \u7b56\u7565\u68af\u5ea6\u65b9\u6cd5","text":"<p>\u5728\u4ecb\u7ecd\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u6765\u770b\u503c\u51fd\u6570\u65b9\u6cd5\u7684\u51e0\u4e2a\u95ee\u9898\uff1a</p> <ol> <li>\u53ea\u9002\u5408\u79bb\u6563\u72b6\u6001\u4e0e\u52a8\u4f5c\u7a7a\u95f4</li> <li>\u9700\u8981\u989d\u5916\u7ec4\u5408\u63a2\u7d22\u7b56\u7565</li> <li>\u968f\u673a\u63a2\u7d22\u5bf9\u6700\u574f\u60c5\u51b5\u7f3a\u4e4f\u63a7\u5236\uff1b</li> </ol> <p>\u800c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u662f\u76f4\u63a5\u5bf9\u7b56\u7565\u8fdb\u884c\u6c42\u5bfc\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u9700\u8981\u5c06\u7b56\u7565\u51fd\u6570\u53c2\u6570\u5316:</p> <p>$$</p> <pre><code>\\pi(a|s) \\rightarrow \\pi_\\theta(a|s)\n</code></pre> <p>$$</p> <p>\u6b64\u65f6\u53ef\u4ee5\u5c06\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u51fd\u6570\u5199\u4e3a</p> \\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}\\left[ R(\\tau) \\right] \\] <p>\u8fd9\u91cc\uff1a</p> <ul> <li>\\(\\tau = (s_0, a_0, s_1, a_1, ..., s_T, a_T)\\)\u662f\u4e00\u4e2a\u8f68\u8ff9\uff1b</li> <li>\\(R(\\tau) = \\sum_{t=0}^{T} r(s_t, a_t)\\) \u662f\u8f68\u8ff9\u7684\u6982\u7387\u5206\u5e03\uff1b</li> <li>\\(p_\\theta(\\tau)\\) \u662f\u8f68\u8ff9\u7684\u6982\u7387\uff0c\u7531\u7b56\u7565\u6982\u7387\u548c\u73af\u5883\u7684\u8f6c\u79fb\u6982\u7387\u7ec4\u6210\uff1b</li> </ul>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#31","title":"3.1. \u7b56\u7565\u68af\u5ea6\u5b9a\u7406","text":"<p>\u7b56\u7565\u68af\u5ea6\u53ef\u4ee5\u6839\u636e\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\u8ba1\u7b97</p> \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)}\\left[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)R(\\tau)\\right] \\\\ = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)R(\\tau) \\] <p>\u4e0a\u5f0f\u6700\u540e\u4e3aminibatch\u4e0a\u7684\u68af\u5ea6\uff0c\u5bf9\u53c2\u6570\u8fdb\u884c\u66f4\u65b0\uff1a</p> \\[ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)  \\] <p>\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5bf9\u6bd4\u503c\u51fd\u6570\u65b9\u6cd5\uff0c\u5177\u5907\u5982\u4e0b\u7279\u70b9\uff1a</p> <ul> <li>\u53ef\u4ee5\u57fa\u4e8e\u8f68\u8ff9\u5b66\u4e60\uff0c\u800c\u4e0d\u9700\u8981\u5b9e\u65f6\u4ea4\u4e92\u548c\u53cd\u9988</li> <li>\u9002\u5408\u9ad8\u7ef4\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\uff08\u5305\u62ecembedding\u7a7a\u95f4\uff09</li> <li>\u53ef\u4ee5\u628a\u5e8f\u5217\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u7b56\u7565\uff1a<ul> <li>\u7528\u6237\u8f93\u5165\u4f5c\u4e3a\u72b6\u6001\u7a7a\u95f4\u3000\uff33\uff1b</li> <li>\u6a21\u578b\u8f93\u51fa\u4f5c\u4e3a\u52a8\u4f5c\u7a7a\u95f4\u3000\uff21\uff1b</li> <li>\u6a21\u578b\u53c2\u6570\u4f5c\u4e3a\u7b56\u7565\u53c2\u6570\u3000\\(\\theta\\); </li> </ul> </li> </ul> <p>\u7ed9\u51fa\u4eba\u7c7b\u504f\u597d\u53cd\u9988 \\(R(\\tau)\\)\u5373\u53ef\u76f4\u63a5\u4f7f\u7528PG\u65b9\u6cd5\u8bad\u7ec3LLM\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u3002\u4f46PG\u7b97\u6cd5\u7684\u68af\u5ea6\u65b9\u5dee\u8f83\u5927\uff0c\u7a33\u5b9a\u6027\u6b20\u4f73\u3002\u540c\u65f6\\(R(\\tau)\\) \u4e5f\u96be\u4ee5\u76f4\u63a5\u5b9a\u4e49\u3001\u6253\u5206\uff0c\u56e0\u6b64PG\u65b9\u6cd5\u5e76\u672a\u5b9e\u9645\u5e94\u7528\u4e8eLLM\u5bf9\u9f50\u4efb\u52a1\u3002</p>","tags":["LLM","Training","RL"]},{"location":"2025/02/09/rl_ds_r1/#32-trpotrust-regon-policy-optimization","title":"3.2 \u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u6539\u8fdb\u2013TRPO\uff08Trust Regon Policy Optimization\uff09","text":"<p>\u5bf9\u4e8e\u7b56\u7565\u7684\u66f4\u65b0\uff0c\u5982\u4f55\u9009\u62e9\u6b65\u957f\\(\\alpha\\)\u662f\u4e00\u4e2a\u975e\u5e38\u5173\u952e\u7684\u95ee\u9898\uff1a</p> <ul> <li>\u8fc7\u5927\u7684\u6b65\u957f\u4f1a\u5bfc\u81f4\u7b56\u7565\u5267\u70c8\u53d8\u5316\uff0c\u7834\u574f\u5df2\u5b66\u5230\u7684\u597d\u7684\u884c\u4e3a</li> <li>\u8fc7\u5c0f\u7684\u6b65\u957f\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b</li> </ul> <p>\u5e38\u89c1\u7684\u6b65\u957f\u9009\u62e9\u65b9\u6cd5\u6709\u4e09\u79cd\uff1a\u56fa\u5b9a\u6b65\u957f\uff0c\u7ebf\u6027\u641c\u7d22\u548c\u4fe1\u8d56\u57df\uff0c\u4ee5\u4e0b\u662f\u4e09\u79cd\u65b9\u6cd5\u7684\u56fe\u793a\uff1a</p> \\[ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)  \\] <p></p> <p>\u56fa\u5b9a\u6b65\u957f\u65b9\u6cd5\u662f\u6700\u4e3a\u6734\u7d20\u7684\u65b9\u6cd5\uff0c\u4f46\u662f\u6b65\u957f\u4f1a\u5f71\u54cd\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5bf9\u5b9e\u9645\u95ee\u9898\u641c\u7d22\u5408\u7406\u6b65\u957f\u4f1a\u6bd4\u8f83\u9ebb\u70e6\uff1b\u7ebf\u6027\u641c\u7d22\u4f1a\u6cbf\u7740\u68af\u5ea6\u65b9\u5411\u5c1d\u8bd5\u591a\u4e2a\u6b65\u957f\uff0c\u5e76\u9009\u62e9\u6700\u4f18\u6b65\u957f\uff0c\u641c\u7d22\u5f00\u9500\u8f83\u5927\uff0c\u4f46\u6548\u679c\u6700\u597d\uff1b\u4fe1\u8d56\u57df\u65b9\u6cd5\u4f1a\u6839\u636e\u68af\u5ea6\u8ba1\u7b97\u81ea\u9002\u5e94\u4fe1\u8d56\u57df\uff0c\u5e76\u4fdd\u8bc1\u5728\u4fe1\u8d56\u57df\u5185\u66f4\u65b0\u3002\u4fe1\u8d56\u57df\u65b9\u6cd5\u4e00\u65b9\u9762\u80fd\u591f\u4fdd\u8bc1\u6bcf\u4e00\u6b65\u66f4\u65b0\u4e0d\u4f1a\u8ddd\u79bb\u539f\u7b56\u7565\u592a\u8fdc\uff0c\u53e6\u4e00\u65b9\u9762\u80fd\u591f\u5728\u68af\u5ea6\u566a\u58f0\u8fc7\u9ad8\u65f6\u81ea\u52a8\u7f29\u5c0f\u4fe1\u8d56\u57df\uff0c\u80fd\u591f\u6bd4\u8f83\u597d\u7684\u89e3\u51b3PG\u65b9\u6cd5\u68af\u5ea6\u566a\u58f0\u9ad8\u95ee\u9898\u3002</p> <p>\u4e0a\u8fb9\u8ba8\u8bba\u7684\u6b65\u957f\u641c\u7d22\u7b97\u6cd5\u90fd\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u8981\u6c42\u76ee\u6807\u51fd\u6570\u5177\u5907\u51f8\u6027\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u663e\u7136\u4e0d\u662f\u56fe\u51fd\u6570\u3002\u4e3a\u4e86\u89e3\u51b3\u76ee\u6807\u51fd\u6570\u51f8\u6027\u7684\u95ee\u9898\uff0cTRPO\u53c8\u5f15\u5165\u4e86Minorize-Maximization \u601d\u60f3\uff1a\u5bf9\u4e8e\u4e00\u4e2a\u96be\u4ee5\u4f18\u5316\uff0c\u96be\u4ee5\u6c42\u5bfc\u7684\u76ee\u6807\u51fd\u6570\uff0c\u53ef\u4ee5\u9009\u62e9\u4e00\u4e2a\u66ff\u4ee3\u51fd\u6570\u6765\u63cf\u8ff0\u76ee\u6807\u51fd\u6570\u7684\u4e0b\u5c4a\uff0c\u901a\u8fc7\u8fed\u4ee3\u6700\u5c0f\u5316\u8fd9\u4e2a\u4e0b\u5c4a\u66ff\u4ee3\u51fd\u6570\u53ef\u4ee5\u5b8c\u6210\u5bf9\u539f\u76ee\u6807\u51fd\u6570\u7684\u4f18\u5316\u3002</p> <p>\u4e3a\u4e86\u6784\u9020\u8fd9\u4e2a\u66ff\u4ee3\u51fd\u6570\uff0c\u6211\u4eec\u5f15\u5165\u4e00\u4e2a\u4f18\u52bf\u51fd\u6570\u7684\u5b9a\u4e49\uff1a</p> \\[ A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s) \\]","tags":["LLM","Training","RL"]},{"location":"2024/01/20/review-llm/","title":"LLM\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u975e\u6b63\u5f0f\u8bc4\u8bba","text":"<p>\u6700\u8fd1\u8c03\u7814\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u65f6\uff0c\u53d1\u73b0\u4e86\u9887\u6709\u610f\u601d\u7684\u4e00\u4e9b\u5185\u5bb9\uff0c\u51c6\u5907\u5199\u4e00\u4e9b\u5173\u4e8e\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u975e\u6b63\u5f0f\u8bc4\u8bba\uff1a</p> <ul> <li>\u5173\u4e8eScaling Law\u7684\u975e\u6b63\u5f0f\u8bc4\u8bba</li> <li>\u5173\u4e8e\u6a21\u578b\u7ed3\u6784\u7684\u975e\u6b63\u5f0f\u8bc4\u8bba</li> <li>\u5173\u4e8e\u4f18\u5316\u5668\u7684\u975e\u6b63\u5f0f\u8bc4\u8bba</li> <li>\u5173\u4e8e\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3\u7684\u975e\u6b63\u5f0f\u8bc4\u8bba</li> </ul>","tags":["LLM","Pretrain"]},{"location":"2024/01/21/review-scaling-law/","title":"\u5173\u4e8eScaling Law\u7684\u975e\u6b63\u5f0f\u8bc4\u8bba","text":"<p>\u5927\u6a21\u578b\u88ab\u5e7f\u6cdb\u5173\u6ce8\u7684\u8d77\u70b9\u662fOpenAI\u53d1\u5e03ChatGPT\uff0c\u51ed\u501f\u4f18\u79c0\u7684\u5bf9\u8bdd\u80fd\u529b\u4e0eIn-Context Learning\u7684\u80fd\u529b\uff0c\u5438\u5f15\u4e86\u6574\u4e2aAI\u5708\u7684\u5173\u6ce8\u3002 LLM\u6280\u672f\u7684\u53d1\u5c55\u4e3b\u8981\u5f97\u76ca\u4e8eScaling Law\u7ed9\u51fa\u7684\u4e00\u7cfb\u5217\u9884\u6d4b\uff0c\u8fd9\u4e9b\u9884\u6d4b\u4e3b\u5bfc\u4e86\u6700\u8fd1\u51e0\u5e74LLM\u6a21\u578b\u5728\u53c2\u6570\u3001\u6570\u636e\u548c\u7b97\u529b\u89c4\u6a21\u4e0a\u5feb\u901f\u589e\u957f\u3002 \u751a\u81f3\u6709\u4eba\u63d0\u51fa\u4e86\u201dScale is All You Need!\u201d\u3002\u672c\u6587\u4e3b\u8981\u8ba8\u8bbaLLM\u884c\u4e3a\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u8bb0\u5f55\u5173\u4e8eScaling Law\u3001Grokking\u548cDouble descent\u7b49empirical phenomenon\u7684\u8ba8\u8bba\u3002</p>"},{"location":"2024/01/21/review-scaling-law/#_1","title":"\u5927\u6a21\u578b\u7684\u826f\u597d\u6cdb\u5316\u6027","text":"<p>OpenAI\u5728GPT3\u8bba\u6587\u4e2d\u63d0\u51faGPT-3\u7b49\u8bed\u8a00\u6a21\u578b\uff08language model\uff09\u662ffew shot learner\u3002\u8fd9\u4e00\u6982\u5ff5\u51fa\u81eaIn-Context Learning\uff0c\u5177\u4f53\u662f\u6307\u5728\u6a21\u578b\u9884\u6d4b\u65f6\u901a\u8fc7\u4e0a\u4e0b\u6587\u4e2d\u7ed9\u51fa\u8db3\u591f\u7684\u80cc\u666f\u77e5\u8bc6\u548c\u4efb\u52a1\u63cf\u8ff0\uff0c\u7136\u540e\u76f4\u63a5\u9884\u6d4b<sup>1</sup>\uff0c\u6bd4\u5982\uff1a</p> <ul> <li>Zero-shot\uff08\u6ca1\u6709\u793a\u4f8b\uff09\uff1a{8+9=?}</li> <li>One-shot\uff08\u4e00\u4e2a\u793a\u4f8b\uff09\uff1a5+5=10, {8+9=?}</li> <li>Few-shot\uff08\u591a\u4e2a\u793a\u4f8b\uff09\uff1a6+7=13,6+6=12,5+5=10, {==8+9=? ==}</li> </ul> <p>Open AI\u57282020\u5e74\u7684\u5927\u6a21\u578bScaling Law\u8bba\u6587\u4e2d\u53d1\u73b0\uff0c\u82e5\u5c06\u6a21\u578b\u8fc1\u79fb\u5230\u65b0\u7684\u6570\u636e\u96c6\uff0c\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5loss\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5loss\u5b58\u5728\u4e00\u4e2a\u76f8\u5bf9\u6052\u5b9a\u7684offset\u3002\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\uff0c\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5loss\u8fd1\u4f3c\u540c\u6b65\u4e0b\u964d\u3002\u8fd9\u4e5f\u5c31\u610f\u5473\u7740\u53ef\u4ee5\u901a\u8fc7\u6301\u7eed\u589e\u5927\u6a21\u578b\u5927\u5c0f\u6765\u964d\u4f4e\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u7684loss\u3002</p> <p></p> <p>\u8fd9\u79cd\u4f18\u79c0\u7684\u6cdb\u5316\u6027\u7ed9\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u63a2\u7d22\u65b9\u5411\uff1a\u76f8\u6bd4\u63a2\u7d22\u66f4\u590d\u6742\u7684\u6a21\u578b\u7ed3\u6784\uff0c\u63a2\u7d22\u66f4\u5927\u7684\u6a21\u578b\u662f\u5426\u80fd\u591f\u6210\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u65b0\u8def\u5f84\u3002</p>"},{"location":"2024/01/21/review-scaling-law/#_2","title":"\u5927\u6a21\u578b\u7684\u53ef\u9884\u6d4b\u6027","text":"<p>\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7684\u9ed1\u76d2\u70bc\u4e39\u662f\u88ab\u5e7f\u4e3a\u8bdf\u75c5\u7684\u95ee\u9898\u3002\u867d\u7136\u4e0d\u4e4f\u4e39\u672f\u5df2\u81f3\u81fb\u5883\u7684\u9876\u7ea7\u4e39\u5e08\uff0c\u4f46\u4e1a\u52a1\u65b9\u548c\u8d44\u672c\u65b9\u662f\u4e07\u4e07\u4e0d\u6562\u5728\u5355\u6b21\u8bad\u7ec3\u8d85\u4e00\u4e2a\u6708\uff0c\u8017\u8d44\u8d85\u767e\u4e07\u7684LLM\u8bad\u7ec3\u4ea4\u7ed9\u8fd9\u4e9b\u4e39\u5e08\u6765\u8d1f\u8d23\u7684\u3002\u5982\u4f55\u9884\u6d4b\u4e00\u6b21\u8bad\u7ec3\u80fd\u8fbe\u5230\u7684\u7cbe\u5ea6\uff0c\u4ee5\u53ca\u5982\u4f55\u5206\u914d\u8fd9\u79cd\u8d85\u767e\u4e07\u7ea7\u522b\u7684\u8bad\u7ec3\u6210\u672c\u6295\u5165\uff0c\u5c31\u53d8\u6210\u4e86\u8bad\u7ec3LLM\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u8fd9\u65b9\u9762\u6bd4\u8f83\u91cd\u8981\u7684\u5de5\u4f5c\u6709\u4e24\u7bc7\uff1a\u4e00\u7bc7\u662fOpenAI\u63d0\u51fa\u7684Kaplan Scaling Law<sup>2</sup>\uff0c\u4e3b\u8981\u7814\u7a76\u4e86\u6a21\u578bloss\u548c\u51e0\u4e2a\u4e3b\u8981\u53c2\u6570\uff1a\u6a21\u578b\u53c2\u6570\u89c4\u6a21N\u3001\u8bad\u7ec3\u6570\u636e\u89c4\u6a21D\u4ee5\u53ca\u7b97\u529b\u6295\u5165C\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u53e6\u4e00\u7bc7\u662fDeepMind\u63d0\u51fa\u7684Chinchilla Optimal<sup>3</sup>\uff0c\u4e3b\u8981\u7814\u7a76\u8ba1\u7b97\u8d44\u6e90\u9650\u5b9a\u4e0b\uff0c\u7b97\u529b\u7684\u5206\u914d\u95ee\u9898\uff0c\u6bd4\u5982\u6bcf\u4e2a\u53c2\u6570\u5bf9\u5e94\u591a\u5c11token\u3002</p>"},{"location":"2024/01/21/review-scaling-law/#kaplan-scaling-law","title":"Kaplan Scaling Law","text":"<p>Kaplan Scaling Law\u53d1\u8868\u4e8e2020\u5e74\uff0c\u540c\u5e74OpenAI\u4e5f\u6b63\u5f0f\u53d1\u8868\u4e86GPT3\u3002\u56e0\u6b64\u4ece\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u6765\u8bf4\uff0cKaplan Scaling Law\u5373\u662f\u5bf9GPT3\u6a21\u578b\u8bad\u7ec3\u7684\u4e00\u4e2a\u603b\u7ed3\uff0c\u4e5f\u662fOpenAI\u575a\u5b9a\u6295\u5165\u5927\u6a21\u578b\u548c\u5927\u7b97\u529b\u7684\u4fe1\u5fc3\u6240\u5728\u3002\u5148\u7b80\u5355\u770b\u4e0bKaplan Scaling Law\u7684\u51e0\u4e2a\u4e3b\u8981\u7ed3\u8bba\uff1a</p> <ol> <li>\u9650\u5b9a\u6a21\u578b\u53c2\u6570\u89c4\u6a21N\uff0c\u7ed9\u8db3\u591f\u5927\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u5230\u6536\u655b\uff1a \\(L(N) = \\left (\\frac{N_c}{N}\\right ) ^{\\alpha_N}; \\alpha_N \\simeq 0.076, N_c \\simeq 8.8 \\times 10^{13}\\)</li> <li>\u9650\u5b9a\u6570\u636e\u89c4\u6a21D\uff0c\u901a\u8fc7early stopping \u8bad\u7ec3\u5927\u6a21\u578b\uff1a \\(L(D) = \\left ( \\frac{D_c}{D} \\right)^{\\alpha_D};\\alpha_D \\simeq 0.095, D_c \\simeq 5.4 \\times 10^{13}\\)</li> <li>\u9650\u5b9a\u8ba1\u7b97\u8d44\u6e90\uff0c\u7ed9\u8db3\u591f\u5927\u7684\u6570\u636e\u96c6\u3001\u6700\u4f18\u6a21\u578b\u89c4\u6a21\u4e0e\u8db3\u591f\u5c0f\u7684batch size\uff1a \\(L(C_{min})=\\left ( \\frac{C_c^{min}}{C}\\right )^{\\alpha_C^{min}}; \\alpha_C^{min} \\simeq 0.050, C_C^{min} \\simeq 3.1 \\times 10^8\\  (PFdays)\\) </li> </ol> <p>\u76f4\u89c2\u4e00\u70b9\u5c31\u662f\u8bf4\u5f71\u54cdloss\u7684\u4e3b\u8981\u56e0\u7d20\u662fN\u3001D\u548cC\u3002\u968f\u7740\u4e09\u8005\u7684\u6307\u6570\u589e\u957f\uff0closs\u7ebf\u6027\u4e0b\u964d\uff1a</p> <p></p> <p>\u53e6\u4e00\u65b9\u9762\u6a21\u578b\u7ed3\u6784\u8d85\u53c2\u3001\u5b66\u4e60\u5668\u7684\u8d85\u53c2\u6ca1\u90a3\u4e48\u91cd\u8981\uff0c\u6574\u4f53\u5bf9loss\u5f71\u54cd\u4e0d\u5927\uff1a</p> <p></p> <p>\u6709\u4e9b\u53c2\u6570\u5728\u4e00\u5230\u4e24\u4e2a\u5c3a\u5ea6\u5185\u53d8\u5316\uff0c\u800c\u5bf9\u6700\u7ec8\u7684loss\u5f71\u54cd\u57282%\u4ee5\u5185\u3002\u800c\u5b66\u4e60\u7387\u53ea\u8981\u4e0d\u8fc7\u4e8e\u5c0f\uff0c\u6216\u8005\u8870\u51cf\u8fc7\u4e8e\u8fc5\u901f\uff0c\u5bf9\u6700\u7ec8loss\u7684\u5f71\u54cd\u5e76\u4e0d\u5927\u3002</p> <p></p> <p>Kaplan Scaling Law\u6700\u4e3a\u91cd\u5927\u7684\u610f\u4e49\u5728\u4e8e\u4e24\u70b9\uff1a</p> <ul> <li>\u7ed9\u51fa\u4e86\u5f71\u54cd\u6a21\u578b\u6548\u679c\u7684\u4e3b\u8981\u56e0\u7d20\u6a21\u578b\u53c2\u6570\u89c4\u6a21N\u548c\u8bad\u7ec3\u6570\u636e\u89c4\u6a21D\uff0c\u5e76\u8ba4\u4e3a\u5176\u4ed6\u8d85\u53c2\u662f\u6b21\u8981\u56e0\u7d20\uff0c\u5f71\u54cd\u4e0d\u5927\uff0c\u56e0\u6b64\u4e5f\u5c31\u907f\u514d\u4e86LLM\u4e0a\u7ee7\u7eed\u9ed1\u76d2\u70bc\u4e39\uff1b</li> <li>\u7ed9\u51fa\u4e86\u5bf9Loss\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u53ea\u8981\u6307\u6570\u589e\u957fN\u3001D\u548c\u7b97\u529bC\uff0c\u5373\u53ef\u83b7\u5f97loss\u7684\u7ebf\u6027\u6539\u8fdb\uff1b \u6839\u636eKaplan Scaling Law\uff0cGPT3\u7684175B\u53c2\u6570\u9700\u8981300B token\u8bad\u7ec3\uff0c\u7ea6\u6bcf\u4e2a\u53c2\u65701.7\u4e2atoken\uff08\u5177\u4f53\u53c2\u8003kaplan vs chinchilla\uff09\u3002</li> </ul>"},{"location":"2024/01/21/review-scaling-law/#chinchila-scaling-law","title":"Chinchila Scaling Law","text":"<p>Chinchila Scaling Law\u7531DeepMind\u53d1\u8868\u4e8e2022\u5e74\uff0c\u540c\u5e74Google\u53d1\u8868\u4e86PaLM\uff08Language Modeling with Pathways<sup>4</sup>\uff09\u3002Chinchila Scaling Law \u5173\u6ce8\u6c42\u89e3\u7b97\u529b\u7ea6\u675f\u4e0b\uff0c\u6700\u4f18\u6a21\u578b\u89c4\u6a21N\u4e0e\u6570\u636e\u89c4\u6a21D\uff0c\u5373\uff1a \\(N_{opt}(C), D_{opt}(C) = \\underset{N, D, \\ s.t.\\ FLOPs(N,D)=C}{\\arg\\min} L(N, D)\\) \u6700\u7ec8\u53ef\u4ee5\u62df\u5408\u51fa\u6765N\u3001D\u4e0e\u7b97\u529bC\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a\\(N_{opt} \\propto  C^a, D_{opt} \\propto C^b\\)\u3002DeepMind\u7ed9\u51fa\u4e86\u4e09\u79cd\u5b9e\u73b0\uff1a</p> \u5b9e\u73b0 \u53c2\u6570\\(a\\) \u53c2\u6570\\(b\\) \u56fa\u5b9aN\uff0c\u641c\u7d22\u6700\u4f18D 0.50 0.50 \u56fa\u5b9aC\uff0c\u641c\u7d22\u6700\u4f18N\u548cD 0.49 0.51 \u62df\u5408L(N,D)\uff0c\u641c\u7d22\u6700\u4f18N\u548cD 0.46 0.54 Kaplan Scaling Law 0.73 0.27 <p>\u5176\u4e2d\u7b2c\u4e09\u79cd\u5b9e\u73b0\u62df\u5408\u4e86\u5982\u4e0b\u5173\u7cfb \\(L(N,D)=E+\\frac{A}{N^{\\alpha}}+\\frac{B}{D^{\\beta}}\\) \u5176\u4e2d\\(E\\)\u523b\u753b\u7684\u662f\u7406\u60f3\u6a21\u578b\u7684loss\uff0c\\(\\frac{A}{N^{\\alpha}}\\)\u523b\u753b\u7684\u662f\u6709\u9650\u6a21\u578b\u53c2\u6570\u5bf9loss\u7684\u5f71\u54cd\uff0c\\(\\frac{B}{D^{\\beta}}\\)\u523b\u753b\u7684\u662f\u6709\u9650\u6570\u636e\u91cf\u5bf9loss\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u8fd9\u4e2a\u62df\u5408\u51fa\u6765\u7684loss\uff0c\u53ef\u4ee5\u7ed8\u5236\u5982\u4e0b\u56fe\u50cf\uff1a  \u5728\u5de6\u56fe\u4e2d\uff0c\u6bcf\u6761loss\u7b49\u9ad8\u7ebf\u90fd\u6709\u4e00\u4e2a\u7b97\u529b\u6700\u4f18\u70b9\uff0c\u8fd9\u4e9b\u7b97\u529b\u6700\u4f18\u70b9\u53c8\u5728log-log\u5750\u6807\u4e0a\u8fde\u6210\u4e86\u4e00\u6761\u8fd1\u4f3c\u76f4\u7ebf\uff0c\u76f4\u7ebf\u4e0a\u7684\u70b9\u5373Chinchilla Optimal\u70b9\u3002\u8fbe\u5230Chinchilla Optimal\uff0c\u6bcf\u4e2a\u53c2\u6570\u5927\u7ea6\u9700\u898120\u4e2atoken\u3002</p>"},{"location":"2024/01/21/review-scaling-law/#beyond-power-law-scaling","title":"Beyond Power Law Scaling","text":"<p>Kaplan Scaling Law\u4e0eChinchilla Scaling Law\u6240\u7ed9\u51fa\u7684\u90fd\u662f\u6570\u636e\u4e0eloss\u4e4b\u95f4\u7684Power Law\uff0c\u5373\u6570\u636e\u6307\u6570\u589e\u957f\uff0closs\u7ebf\u6027\u6539\u8fdb\u3002\u6839\u636eChinchilla Scaling Law\uff0c\u5927\u6a21\u578b\u7684\u53c2\u6570\u89c4\u6a21\u4e0e\u6570\u636e\u91cf\u4ecd\u67092-3\u4e2a\u6570\u91cf\u7ea7\u7684\u63d0\u5347\u7a7a\u95f4\uff1a</p> Model size(params) Training tokens (round) Training data used (estimate) How much data is that? If 1 book is about 500KB of text (estimate) 70B 1.4 Trillion 2.3TB More books than in The Kindle store on Amazon US (6.4M). 250B 5 Trillion 8.3TB All 30 libraries at Yale University (16.6M). 500B 10 Trillion 16.6TB The Google Books collection (33.2M). 1T 20 Trillion 33.3TB The US Library of Congress (66.6M). 10T 200 Trillion 333TB All US public libraries combined (666M). 100T 2 Quadrillion 3.3PB All bibles ever sold worldwide (6.6B). 250T 5 Quadrillion 8.3PB A stack all the way to the Moon (16.6B). 500T 10 Quadrillion 16.6PB 4 books about every living human (33.2B). Dataset sizes needed to align with Chinchilla data optimization for models<sup>5</sup>. <ol> <li> <p>2020, OpenAI, Language Models are Few-Shot Learners\u00a0\u21a9</p> </li> <li> <p>2020, OpenAI, Scaling Laws for Neural Language Models\u00a0\u21a9</p> </li> <li> <p>2022, DeepMind, Training Compute-Optimal Large Language Models\u00a0\u21a9</p> </li> <li> <p>2022, DeepMind, PaLM: Scaling Language Modeling with Pathways\u00a0\u21a9</p> </li> <li> <p>2022, https://lifearchitect.ai/the-sky-is-bigger/ \u21a9</p> </li> </ol>"},{"location":"2024/01/25/review-dist-train/","title":"\u5173\u4e8e\u5206\u5e03\u5f0f\u6a21\u578b\u5e76\u884c\u7684\u5206\u6b63\u5f0f\u8bc4\u8bba","text":"<p>\u5173\u4e8eData Parallel\uff08DP\uff09\u3001Tensor Parallel\uff08TP\uff09\u548cPipeline Parallel\uff08PP\uff09\u7b49\u5206\u5e03\u5f0f\u5e76\u884c\u7b56\u7565\uff0c\u4e0eMegatron\u3001DeepSpeed\u548cFSDP\u7b49\u5b9e\u73b0\u7684\u4e00\u4e9b\u6df1\u5165\u7814\u7a76\u4e0e\u8ba8\u8bba\u3002\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3\u4e3b\u8981\u89e3\u51b3\u4e24\u7c7b\u95ee\u9898\uff1a</p> <ol> <li>\u6a21\u578b\u5206\u7247\uff1a\u6a21\u578b\u5927\u5c0f\u8fdc\u8d85\u5355\u8282\u70b9\u5b58\u50a8\u4e0a\u6765\uff0c\u9700\u8981\u591a\u8282\u70b9\u5206\u7247\u5b58\u50a8\u548c\u8ba1\u7b97\u62c5\uff1b</li> <li>\u5e76\u884c\u8bad\u7ec3\uff1a\u63d0\u9ad8\u5355\u4f4d\u65f6\u95f4\u5185\u7684\u7b97\u529b\u5bc6\u5ea6\uff0c\u8fdb\u800c\u964d\u4f4e\u6574\u4f53\u8bad\u7ec3\u65f6\u95f4\uff1b \u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3\u51e0\u4e4e\u603b\u662f\u4f1a\u5f15\u5165\u6602\u8d35\u7684\u6210\u672c\uff0c\u6bd4\u5982\u589e\u52a0\u4e86\u6602\u8d35\u7684\u591a\u8282\u70b9\u901a\u4fe1\u3001\u5f15\u5165\u4e86\u989d\u5916\u7684\u591a\u673a\u7a33\u5b9a\u6027\u95ee\u9898\u3001\u4ee5\u53ca\u989d\u5916\u7684\u5f00\u53d1\u4e0e\u8c03\u8bd5\u6210\u672c\u7b49\uff0c\u56e0\u6b64\u6211\u4eec\u5e94\u8be5\u5c3d\u91cf\u907f\u514d\u5f15\u5165\u5206\u5e03\u5f0f\u5e76\u884c\u8bad\u7ec3\u3002\u800c\u4e0d\u5f97\u4e0d\u5f15\u5165\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u573a\u666f\u4e2d\uff0c\u4e5f\u5e94\u5145\u5206\u8003\u8651\u901a\u4fe1\u5f00\u9500\uff0c\u5c3d\u91cf\u964d\u4f4e\u5e76\u884c\u7684\u89c4\u6a21\u3002</li> </ol>","tags":["LLM","Training"]},{"location":"2024/01/25/review-dist-train/#3d","title":"3D\u6a21\u578b\u5e76\u884c","text":"<p>\u6839\u636e\u5207\u5206\u7ef4\u5ea6\u7684\u4e0d\u540c\uff0c\u5e76\u884c\u7b56\u7565\u4e3b\u8981\u5206\u4e3a\u5982\u4e0b\u51e0\u7c7b\uff1a</p> <ol> <li>Data Parallel\uff08DP\uff09\uff1a\u5c06\u6570\u636e\u5207\u5206\u6210N\u4efd\uff0c\u6bcf\u4e2ainstance\u91c7\u7528\u5b8c\u5168\u76f8\u540c\u7684\u914d\u7f6e\uff0c\u5728\u8ba1\u7b97\u68af\u5ea6\u540e\u901a\u8fc7all reduce\u5168\u5c40\u540c\u6b65\u68af\u5ea6\uff0c\u5e76\u5206\u522b\u66f4\u65b0\uff1b</li> <li>Tensor Parallel\uff08TP\uff09\uff1a\u5c06\u6bcf\u4e2atensor\u5207\u5206\u6210N\u4efd\uff0c\u5728\u77e9\u9635\u4e58\u6cd5\u7b49\u8ba1\u7b97\u65f6\u8fdb\u884c\u540c\u6b65\uff1b\u4e5f\u79f0\u4e3a\u6a2a\u5207</li> <li>Pipeline Parallel \uff08PP\uff09\uff1a\u5c06\u6a21\u578b\u6309\u6267\u884c\u524d\u540e\u987a\u5e8f\u5207\u5206\u591a\u5206\uff08\u901a\u5e38\u6309layer\u5207\u5206\uff09\uff0c\u5e76\u6839\u636e\u987a\u5e8f\u4f9d\u6b21\u6267\u884c\uff1b</li> <li>Zero Redundancy Optimizer\uff08ZeRO\uff09\uff1a\u540c\u6837\u5c06tensor\u5207\u5206\u6210N\u4efd\uff0c\u4f46\u662f\u5728\u524d\u5411\u4e0e\u540e\u5411\u8ba1\u7b97\u65f6\u5728\u6bcf\u4e2a\u5206\u5e03\u5f0f\u8282\u70b9\u91cd\u5efa\u539f\u59cbTensor\uff1b</li> <li>Sequence Parallel\uff08SP\uff09\uff1a\u5728\u8d85\u957f\u5e8f\u5217\u4e0a\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u5c06\u8ba1\u7b97\u5207\u5206\u81f3\u591a\u4e2a\u8282\u70b9\uff1b</li> </ol> <p></p>","tags":["LLM","Training"]},{"location":"2024/01/25/review-dist-train/#data-parallel","title":"Data Parallel","text":"<p>Data \u5e76\u884c\u4e00\u822c\u662f\u6307\u6839\u636e\u8ba1\u7b97\u8d44\u6e90\u5c06\u6a21\u578b\u590d\u5236\u591a\u4e2a\u526f\u672c\uff0c\u6bcf\u4e2a\u6a21\u578b\u526f\u672c\u72ec\u7acb\u8fdb\u884c\u6570\u636e\u52a0\u8f7d\u4e0e\u524d\u540e\u9879\u8ba1\u7b97\uff0c\u4e4b\u540e\u901a\u8fc7\u4e00\u4e2aall reduce\u901a\u4fe1\u6765\u540c\u6b65\u68af\u5ea6\uff0c\u6700\u540e\u518d\u5206\u522b\u66f4\u65b0\u6743\u91cd\u3002 </p> <p>\u4ece\u5355\u8282\u70b9\u8bad\u7ec3\u5230\u591a\u8282\u70b9\u53ea\u9700\u8981\u6539\u52a8\u4e24\u4e2a\u5730\u65b9\uff1a</p> <ol> <li>data loader\u6839\u636e\u5bf9\u8bad\u7ec3\u6570\u636e\u5207\u7247\uff0c\u5e76\u53ea\u8bfb\u53d6\u5bf9\u5e94\u5206\u7247\u7684\u6570\u636e\uff1b</li> <li>backward\u4e4b\u540e\u901a\u8fc7all reduce\u540c\u6b65\u68af\u5ea6\uff1b \u5728PyTorch\u4e2d\u4e24\u8005\u5206\u522b\u901a\u8fc7<code>torch.utils.data.distributed.DistributedSampler</code>\u4e0e<code>torch.nn.parallel.DistributedDataParallel</code>\u5b9e\u73b0\u3002</li> </ol>","tags":["LLM","Training"]},{"location":"2024/01/25/review-dist-train/#zero-zero-redundancy-optimizer","title":"ZeRO: Zero Redundancy Optimizer","text":"<p>\u901a\u8fc7Data Parallel\u6bd4\u8f83\u5bb9\u6613\u5b9e\u73b0\u52a0\u901f\u8bad\u7ec3\uff0c\u4f46\u5bf9\u4e8e\u4e00\u4e9b\u53c2\u6570\u89c4\u6a21\u8f83\u5927\u7684\u6a21\u578b\uff0c\u5355\u8282\u70b9\u7684\u5185\u5b58/\u663e\u5b58\u96be\u4ee5\u653e\u4e0b\uff0c\u6b64\u65f6\u5c31\u65e0\u6cd5\u901a\u8fc7\u7b80\u5355\u7684Data Parallel\u7684\u6570\u636e\u5207\u7247\u6765\u5b9e\u73b0\u52a0\u901f\u4e86\u3002ZeRO\u6280\u672f\u4e3b\u8981\u5c1d\u8bd5\u901a\u8fc7\u5bf9\u6a21\u578b\u72b6\u6001\u5207\u7247\uff0c\u5b9e\u73b0\u5927\u6a21\u578b\u7684Data Parallel\u8bad\u7ec3\u3002\u5177\u4f53\u539f\u7406\u5982\u4e0b\u56fe\u6240\u793a\uff1a  \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6a21\u578b\u72b6\u6001\u53ef\u5206\u4e3a\u4e09\u90e8\u5206\uff1a\u53c2\u6570\uff08Parameters\uff09\u3001 \u68af\u5ea6\uff08Gradients\uff09\u548c\u4f18\u5316\u5668\u72b6\u6001\uff08Optimizer States\uff09\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u4e09\u8005\u5206\u522b\u8fdb\u884c\u5207\u5206\u5e76\u5b58\u50a8\u5230\u591a\u4e2a\u8282\u70b9\u4e0a\u53bb\u3002\u6839\u636e\u5207\u5206\u65b9\u5f0f\u7684\u4e0d\u540c\u53ef\u4ee5\u5206\u4e3a\u4e09\u4e2astage\uff1a - ZeRO Stage 1\uff1a\u5207\u5206Optimizer States - ZeRO Stage 2\uff1a\u5207\u5206Optimizer States\u4e0eGradients - ZeRO Stage 3\uff1a\u5207\u5206Parameters \u5176\u4e2dZeRO1\u548cZeRO2\u5e76\u4e0d\u4f1a\u589e\u52a0\u901a\u4fe1\u91cf\uff0c\u56e0\u6b64\u5b9e\u9645\u5de5\u7a0b\u4e2d\u88ab\u91c7\u7528\u66f4\u591a\u3002</p>","tags":["LLM","Training"]},{"location":"2024/01/25/review-dist-train/#fsdp-fully-sharded-data-parallel","title":"FSDP: Fully Sharded Data Parallel","text":"<p>FSDP\u662fPyTorch\u5728v1.11\u4e4b\u540e\u5f15\u5165\u7684\u5185\u7f6eData Parallel\u5e76\u884c\u6a21\u5f0f\u3002PyTorch\u5b98\u65b9\u5728Data Parallel\u6280\u672f\u7684\u53d1\u5c55\u8def\u5f84\u662fDP -&gt; DDP -&gt; FSDP\uff0c\u4e0d\u65ad\u5438\u7eb3\u6765\u81ea\u5f00\u6e90\u793e\u533a\u7684\u65b0\u6280\u672f\u3002 </p>","tags":["LLM","Training"]},{"location":"2024/01/25/review-dist-train/#tensor-paralle","title":"Tensor Paralle","text":"<p>\u5bf9\u4e8e\u4e00\u4e9b\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\uff0c\u6743\u91cd\u3001\u68af\u5ea6\u4ee5\u53ca\u4f18\u5316\u5668\u72b6\u6001\u53ef\u80fd\u5e76\u4e0d\u662f\u5185\u5b58/\u663e\u5b58\u5360\u7528\u7684\u5927\u5934\uff0c\u6fc0\u6d3b\u503c\u540c\u6837\u4f1a\u5360\u7528\u5927\u91cf\u5185\u5b58\u3002\u56e0\u6b64\u4ec5\u4ec5\u4f7f\u7528ZeRO\u6280\u672f\u5e76\u4e0d\u80fd\u5f88\u597d\u652f\u6301\u8d85\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u65f6\u9700\u8981Tensor Parallel\u8fdb\u4e00\u6b65\u5bf9\u6fc0\u6d3b\u503c\u8fdb\u884c\u5207\u5206\u3002 Tensor Parallel\u6280\u672f\u7684\u57fa\u7840\u662f\u77e9\u9635\u5206\u5757\u4e58\u6cd5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a  \u77e9\u9635\u4e58\u6cd5\u7684\u5e76\u884c\u53ef\u5206\u4e3a\u5217\u5e76\u884c\u4e0e\u884c\u5e76\u884c\uff1a - \u5217\u5e76\u884c\uff1a $$ GeLU(WA) = GeLU(W[A_1, A_2, \u2026, A_n]) = [GeLU(WA_1),GeLU(WA_2),\u2026,GeLU(WA_n)] $$ - \u884c\u5e76\u884c\uff1a$$ GeLU(WA) = GeLU( [W_1, W_2, \u2026, W_n] \\left [ \\begin{smallmatrix} A_1 \\ A_2 \\ \u2026 \\ A_n  \\end{smallmatrix} \\right ] ) = GeLU(W_1A_1+W_2A_2+\u2026+W_nA_n) $$</p>","tags":["LLM","Training"]},{"location":"2024/01/25/review-dist-train/#pipeline","title":"Pipeline\u5e76\u884c","text":"","tags":["LLM","Training"]},{"location":"2024/01/25/review-dist-train/#sequence","title":"Sequence\u5e76\u884c","text":"","tags":["LLM","Training"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/llm/","title":"LLM","text":""},{"location":"category/training/","title":"Training","text":""},{"location":"category/rl/","title":"RL","text":""}]}