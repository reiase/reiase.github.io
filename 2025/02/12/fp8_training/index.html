<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="深入分析DeepSeek V3的FP8训练方案，探讨block-wise scaling和高精度累加如何解决向量相关性问题， 并通过信噪比实验揭示其成功的技术原理。对比Transformer Engine方案，展示FP8训练中精度优化的关键。"><meta name=author content="reiase <reiase@gmail.com>"><link href=https://reiase.github.io/2025/02/12/fp8_training/ rel=canonical><link href=../../09/rl_ds_r1/ rel=prev><link href=../../15/int8_training/ rel=next><link rel=icon href=../../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.3"><title>从DeepSeek V3看FP8训练的挑战 - Overfitting: From Algorithms to Silicon</title><link rel=stylesheet href=../../../../assets/stylesheets/main.d7758b05.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Han+Sans+SC:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Source Han Sans SC";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../assets/_markdown_exec_pyodide.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LMDTYMWWTF"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LMDTYMWWTF",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LMDTYMWWTF",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script> <link href="../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#1-fp8 class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../../.. title="Overfitting: From Algorithms to Silicon" class="md-header__button md-logo" aria-label="Overfitting: From Algorithms to Silicon" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Overfitting: From Algorithms to Silicon </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 从DeepSeek V3看FP8训练的挑战 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../.. class=md-tabs__link> 博客 </a> </li> <li class=md-tabs__item> <a href=../../../../tags/ class=md-tabs__link> 标签 </a> </li> <li class=md-tabs__item> <a href=../../../../nodes/AdamW/ class=md-tabs__link> 代码片段 </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="Overfitting: From Algorithms to Silicon" class="md-nav__button md-logo" aria-label="Overfitting: From Algorithms to Silicon" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> Overfitting: From Algorithms to Silicon </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../../.. class="md-nav__link "> <span class=md-ellipsis> 博客 </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=true> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> 博客 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_2> <label class=md-nav__link for=__nav_1_2 id=__nav_1_2_label tabindex> <span class=md-ellipsis> 归档 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_1_2> <span class="md-nav__icon md-icon"></span> 归档 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_1_3> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex> <span class=md-ellipsis> 分类 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> 分类 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/fp8/ class=md-nav__link> <span class=md-ellipsis> FP8 </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/rl/ class=md-nav__link> <span class=md-ellipsis> RL </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/training/ class=md-nav__link> <span class=md-ellipsis> Training </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/training-dynamics/ class=md-nav__link> <span class=md-ellipsis> Training Dynamics </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../tags/ class=md-nav__link> <span class=md-ellipsis> 标签 </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../nodes/AdamW/ class=md-nav__link> <span class=md-ellipsis> 代码片段 </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../.. class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> 回到主页 </span> </a> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> 元数据 </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2025-02-12 00:00:00+00:00" class=md-ellipsis>2025年2月12日</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> 分类于 <a href=../../../../category/llm/ >LLM</a>, <a href=../../../../category/training/ >Training</a>, <a href=../../../../category/fp8/ >FP8</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 需要 9 分钟阅读时间 </span> </div> </li> </ul> </nav> </li> </ul> </nav> <nav class="md-nav md-nav--secondary" aria-label=目录> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-fp8 class=md-nav__link> <span class=md-ellipsis> 1. FP8 浮点格式 </span> </a> <nav class=md-nav aria-label="1. FP8 浮点格式"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-fp8 class=md-nav__link> <span class=md-ellipsis> 1.1 FP8 格式的历史 </span> </a> </li> <li class=md-nav__item> <a href=#12-ieee-754 class=md-nav__link> <span class=md-ellipsis> 1.2. 常见浮点数与 IEEE 754 </span> </a> </li> <li class=md-nav__item> <a href=#13-fp8 class=md-nav__link> <span class=md-ellipsis> 1.3. FP8 有两种格式 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-transformer-engine-fp8 class=md-nav__link> <span class=md-ellipsis> 2. Transformer Engine 的 FP8 方案 </span> </a> </li> <li class=md-nav__item> <a href=#3-deepseek-v3-fp8 class=md-nav__link> <span class=md-ellipsis> 3. DeepSeek V3 的 FP8 方案 </span> </a> <nav class=md-nav aria-label="3. DeepSeek V3 的 FP8 方案"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31 class=md-nav__link> <span class=md-ellipsis> 3.1. 降低显存开销与通信开销 </span> </a> </li> <li class=md-nav__item> <a href=#32 class=md-nav__link> <span class=md-ellipsis> 3.2. 细粒度量化方法 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-fp8 class=md-nav__link> <span class=md-ellipsis> 4. FP8 精度问题的系统分析 </span> </a> <nav class=md-nav aria-label="4. FP8 精度问题的系统分析"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#41 class=md-nav__link> <span class=md-ellipsis> 4.1. 向量长度的影响 </span> </a> <nav class=md-nav aria-label="4.1. 向量长度的影响"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tensor-wise-scaling class=md-nav__link> <span class=md-ellipsis> Tensor-wise Scaling 的作用 </span> </a> </li> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 高精度累加的作用 </span> </a> </li> <li class=md-nav__item> <a href=#block-wise-scaling class=md-nav__link> <span class=md-ellipsis> Block-wise Scaling 的作用 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#42 class=md-nav__link> <span class=md-ellipsis> 4.2. 向量相关性的影响 </span> </a> <nav class=md-nav aria-label="4.2. 向量相关性的影响"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 高精度累加的作用 </span> </a> </li> <li class=md-nav__item> <a href=#block-wise-scaling_1 class=md-nav__link> <span class=md-ellipsis> Block-wise Scaling </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#43 class=md-nav__link> <span class=md-ellipsis> 4.3. 结论 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <nav class=md-tags> <a href=../../../../tags/#tag:fp8 class=md-tag>FP8</a> <a href=../../../../tags/#tag:llm class=md-tag>LLM</a> <a href=../../../../tags/#tag:training class=md-tag>Training</a> </nav> <h1>从DeepSeek V3看FP8训练的挑战</h1> <p>DeepSeek V3 的发布引起了对 FP8 训练的广泛关注，业界也出现了大量文章解析 How 的问题——DeepSeek 是怎么进行 FP8 训练的，与传统方案有哪些不同。但是目前鲜有文章对 Why 问题进行深入探讨，为何 DeepSeek 的方案能够取得成功。本文尝试对 FP8 训练所面临的挑战进行深入解析，并尝试猜测 DeepSeek 团队设计其 FP 方案的背后原理。（如果你对 INT8 训练感兴趣，可以参考本文的姊妹篇：<a href=../../../../../../../2025/02/15/int8_training>INT8 训练</a>）</p> <h2 id=1-fp8>1. FP8 浮点格式<a class=headerlink href=#1-fp8 title="Permanent link">&para;</a></h2> <h3 id=11-fp8>1.1 FP8 格式的历史<a class=headerlink href=#11-fp8 title="Permanent link">&para;</a></h3> <p>FP8 是一种遵循 IEEE 754 规范<sup id=fnref:ieee754><a class=footnote-ref href=#fn:ieee754>6</a></sup>的 8 位浮点数格式，由 Nvidia 在 2022 年发布的 H100 GPU 中首次引入。在此之前，Nvidia 硬件上浮点数格式的发展历程如下<sup id=fnref:huang_law_1><a class=footnote-ref href=#fn:huang_law_1>3</a></sup>：</p> <ul> <li>2016 年 P100 GPU 首次引入 FP16 数据格式，直接开启了深度学习混合精度训练的技术路线；</li> <li>2017 年 V100 GPU 首次引入 Tensor Core, 用于加速 FP16 矩阵乘法运算；</li> <li>2020 年 A100 GPU 首次引入 TF32 数据格式，可通过 Tensor Core 加速；引入 bfloat16 数据格式，提供比 FP16 更宽的动态范围（当下 BF16 已经成为 LLM 训练的主流方案）；</li> <li>2022 年 H100 GPU 首次引入 FP8 数据格式；</li> </ul> <p>FP8 被 Nvidia 给予厚望，认为其成功的延续了 CEO 提出的 Huang&rsquo;s Law<sup id=fnref:huang_law_2><a class=footnote-ref href=#fn:huang_law_2>4</a></sup>，即 10 年间 GPU 硬件算力提升 1000 倍。在过去的 10 年间，新型数值表达的引入了 16 倍算力提升，是诸多技术中贡献最大者，GPU 架构与复杂指令集紧随其后带来了 12.5 倍提升，而制程进步带来的收益非常有限，仅 2.5 倍<sup id=fnref:nvidia_gpu><a class=footnote-ref href=#fn:nvidia_gpu>5</a></sup>。</p> <h3 id=12-ieee-754>1.2. 常见浮点数与 IEEE 754<a class=headerlink href=#12-ieee-754 title="Permanent link">&para;</a></h3> <p>IEEE 754 是目前广为使用的浮点数规范，定义了浮点数的 bitwise 表达与量化方式。浮点数的二进制表达分为三部分：</p> <ul> <li>符号位（sign）</li> <li>指数位（exponent）</li> <li>尾数位（mantissa）</li> </ul> <p>常见的浮点数格式的二进制表达如下图所示：</p> <pre class=mermaid><code>block-beta
    columns 33
    FP32["fp32"]
    S1["S"]
    E1["E"]
    E2["E"]
    E3["E"]
    E4["E"]
    E5["E"]
    E6["E"]
    E7["E"]
    E8["E"]
    M1["M"]
    M2["M"]
    M3["M"]
    M4["M"]
    M5["M"]
    M6["M"]
    M7["M"]
    M8["M"]
    M9["M"]
    M10["M"]
    M11["M"]
    M12["M"]
    M13["M"]
    M14["M"]
    M15["M"]
    M16["M"]
    M17["M"]
    M18["M"]
    M19["M"]
    M20["M"]
    M21["M"]
    M22["M"]
    M23["M"]

    BF16["bf16"]
    SS1["S"]
    EE1["E"]
    EE2["E"]
    EE3["E"]
    EE4["E"]
    EE5["E"]
    EE6["E"]
    EE7["E"]
    EE8["E"]
    MM1["M"]
    MM2["M"]
    MM3["M"]
    MM4["M"]
    MM5["M"]
    MM6["M"]
    MM7["M"]
    space:16

    FP16["fp16"]
    space:3
    ss1["S"]
    ee1["E"]
    ee2["E"]
    ee3["E"]
    ee4["E"]
    ee5["E"]
    mm1["M"]
    mm2["M"]
    mm3["M"]
    mm4["M"]
    mm5["M"]
    mm6["M"]
    mm7["M"]
    mm8["M"]
    mm9["M"]
    mm10["M"]
    space:13

    E5M2["fp8"]
    space:3
    s1["S"]
    e1["E"]
    e2["E"]
    e3["E"]
    e4["E"]
    e5["E"]
    m1["M"]
    m2["M"]
    space:21

    E4M3["fp8"]
    space:4
    sss1["S"]
    eee1["E"]
    eee2["E"]
    eee3["E"]
    eee4["E"]
    mmm1["M"]
    mmm2["M"]
    mmm3["M"]
    space:21

    classDef name fill:#00000000, stroke:#00000000
    class FP32,BF16,FP16,E4M3,E5M2 name

    classDef sign fill:#EE0000, stroke:#00000000
    class S1,SS1,s1,ss1,sss1 sign

    classDef exp fill:#00EE00, stroke:#00000000
    class E1,E2,E3,E4,E5,E6,E7,E8 exp
    class EE1,EE2,EE3,EE4,EE5,EE6,EE7,EE8 exp
    class e1,e2,e3,e4,e5,e6,e7,e8 exp
    class ee1,ee2,ee3,ee4,ee5,ee6,ee7,ee8 exp
    class eee1,eee2,eee3,eee4,eee5,eee6,eee7,eee8 exp</code></pre> <h3 id=13-fp8>1.3. FP8 有两种格式<a class=headerlink href=#13-fp8 title="Permanent link">&para;</a></h3> <p>随着浮点数位数从 16 位进一步降低到 8 位，动态范围不足的问题逐渐显现。因此 Nvidia、Arm 和 Intel 在 FP8 规范中设计了两种浮点数类型<sup id=fnref:fp8><a class=footnote-ref href=#fn:fp8>1</a></sup>：E4M3 和 E5M2</p> <table> <thead> <tr> <th></th> <th>E4M3</th> <th>E5M2</th> </tr> </thead> <tbody> <tr> <td>format(s/e/m)</td> <td>1:4:3</td> <td>1:5:2</td> </tr> <tr> <td>Exponent bias</td> <td>7</td> <td>15</td> </tr> <tr> <td>Infinities</td> <td>N/A</td> <td>S.11111.00</td> </tr> <tr> <td>NaN</td> <td>S.1111.111</td> <td>S.11111.{01,10,11}</td> </tr> <tr> <td>Zeros</td> <td>S.0000.000</td> <td>S.00000.00</td> </tr> <tr> <td>Max normal</td> <td>S.1111.110 = <span class=arithmatex>\(1.75 \times 2^8\)</span> = 448</td> <td>S.11110.11 = <span class=arithmatex>\(1.75 \times 2^15\)</span> = 57.344</td> </tr> <tr> <td>Min normal</td> <td>S.0001.0000 = <span class=arithmatex>\(2^{-6}\)</span></td> <td>S.00001.00 = <span class=arithmatex>\(2^{-14}\)</span></td> </tr> <tr> <td>Max subnorm</td> <td>S.0000.111 = <span class=arithmatex>\(0.875 \times 2^{-6}\)</span></td> <td>S.00000.11 = <span class=arithmatex>\(0.75\times 2^{-14}\)</span></td> </tr> <tr> <td>Min subnorm</td> <td>S.0000.001 = <span class=arithmatex>\(2^{-9}\)</span></td> <td>S.00000.01 = $ 2^{-16}$</td> </tr> </tbody> </table> <p>浮点数都会分配一些二进制表达来表示特殊值**NaN**和 <strong><span class=arithmatex>\(\mathbb{\pm}\)</span>Inf</strong>，IEEE 754 规范约定使用指数位全**1**的二进制表达来表示这些特殊值。对于 E4M3 格式来说，若严格遵循 IEEE 754 规范，会 8 个二进制表达。因此在定义 E4M3 规范时对这些二进制表达进行了额外开发，仅在指数位尾数位同时全为 <strong>1</strong> 时才表示 <strong>NaN</strong>，全为 <strong>0</strong> 的时候表示 <strong><span class=arithmatex>\(\pm\)</span>Inf</strong>。</p> <p>H100 的 Tensor Core 提供 3 倍 A100 FP16 性能，若启用 FP8 算力能够再次翻倍。</p> <p><a class=glightbox href=https://developer-blogs.nvidia.com/zh-cn-blog/wp-content/uploads/sites/2/2024/04/%E8%A1%A81-1536x819.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt src=https://developer-blogs.nvidia.com/zh-cn-blog/wp-content/uploads/sites/2/2024/04/%E8%A1%A81-1536x819.png></a></p> <!-- more --> <h2 id=2-transformer-engine-fp8>2. Transformer Engine 的 FP8 方案<a class=headerlink href=#2-transformer-engine-fp8 title="Permanent link">&para;</a></h2> <p>Transformer Engine 是 Nvidia 为 FP8 训练开发加速库，提供了从<code>Linear</code>、<code>Attention</code>到<code>LayerNorm</code>等基础组件的 FP8 实现。下图展示了 TransformerEngine 支持 FP8 矩阵乘法的方案：</p> <p><a class=glightbox href=https://developer-blogs.nvidia.com/zh-cn-blog/wp-content/uploads/sites/2/2024/04/%E5%9B%BE8-1-1536x847.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt src=https://developer-blogs.nvidia.com/zh-cn-blog/wp-content/uploads/sites/2/2024/04/%E5%9B%BE8-1-1536x847.png></a></p> <p>权重和梯度均使用高精度存储，FP8 仅用来进行矩阵乘运算。在矩阵乘之前需要通过 cast 操作将高精度的权重和激活转换为 FP8 格式。矩阵乘的输出仍为高精度，不影响 bias 操作与激活操作。在前向计算中使用 E4M3 进行运算，在反向过程中使用 E5M2 进行计算。</p> <p>TransformerEngine 通过融合算子来降低模型训练时的显存开销。对比 BF16 训练，FP8 训练能够带来 30% 端到端性能提升，但因为权重和激活仍然使用高精度存储，无法带来额外的显存节约，反而会因为 checkpoint 中存储额外的 scaling 值，导致 5%的额外显存占用。</p> <p>综合评价一下 TransformerEngine 的 FP8 方案，FP8 训练有三部分理论收益：</p> <ol> <li>计算性能翻倍；</li> <li>显存开销减半；</li> <li>通信吞吐减半；</li> </ol> <p>TransformerEngine 所使用的 FP8 方案实际上只拿到了 30%的计算性能收益。但是（古尔丹）代价是什么呢：</p> <ol> <li>精度风险：很多实验发现不小于 2%的 loss diff；</li> <li>计算流变复杂：在传统同步/异步范式之外设计支持延迟缩放的新型时序控制逻辑；</li> <li>内存格式变复杂：需要同时支持 E4M3 和 E5M2 两种格式，并确保每块儿显存的格式选择正确；</li> </ol> <h2 id=3-deepseek-v3-fp8>3. DeepSeek V3 的 FP8 方案<a class=headerlink href=#3-deepseek-v3-fp8 title="Permanent link">&para;</a></h2> <p>DeepSeek 团队使用了完全不同的 FP8 训练方案，与 TransformerEngine 的主要不同之处在于：</p> <ol> <li>权重（weight）使用 FP8 存储；</li> <li>全部采用 E4M3 浮点格式；</li> <li>使用块级量化（block-wise scaling），而不是张量级量化（tensor-wise scaling）；</li> </ol> <p>整体架构如下：</p> <p><a class=glightbox href=../../../../imgs/fp8/dsv3_fp8_arch.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/dsv3_fp8_arch.png></a></p> <p>master weight、权重梯度使用 FP32,激活梯度、优化器状态使用 BF16。这些高精度数据会被切分到不同的 DP rank 上，因此对整体显存开销影响可以控制得比较好。</p> <h3 id=31>3.1. 降低显存开销与通信开销<a class=headerlink href=#31 title="Permanent link">&para;</a></h3> <ul> <li><strong>低精度优化器状态：</strong> AdamW 优化器的一阶动量与二阶动量使用 BF16 存储来降低显存压力，但是 master weight 与 main grad 仍然使用 FP32 存储;</li> <li><strong>低精度激活值：</strong> 针对不同的激活值使用了不同的精度</li> <li>Attention 之后的 Linear 层，由于 attention 的梯度计算对精度敏感，这些激活使用了 E5M6 数据类型。并且为了避免引入额外的量化误差，会使用幂次缩放因子；</li> <li><strong>MoE 中 SwiGLU 的输入：</strong> 引入 recompute 策略，使用 FP8 格式缓存输入；</li> <li>低精度通信：MoE 的通信是训练过程中最主要的瓶颈之一。为了降低这部分开销，将 MoE 前向 up-projection 操作与反向的 down-projection 操作前的激活/激活梯度进行 FP8 量化，之后再进行 dispatch 逻辑，量化过程使用幂次缩放因子。对于 combine 模块，使用 BF16 来保证训练精度。</li> </ul> <h3 id=32>3.2. 细粒度量化方法<a class=headerlink href=#32 title="Permanent link">&para;</a></h3> <p>FP8 两种数据类型，E5M2 保留动态范围但是缩减了尾数精度，E4M3 多保留了一位尾数精度但是牺牲了动态范围。不管那一种都会加剧训练过程中的上溢和下溢问题。好在 LLM 模型训练时，权重与激活的动态性研究发现大多数数值分布集中在 0 附近，但会带有明显的少数 outlier。集中在 0 附近意味着我们可以通过缩放因子（scaling factor）对数值进行缩放，从而更好的利用有限的动态范围。但 outlier 的存在导致很难在整个 tensor 层面选取出适当的缩放因子。</p> <p>为了平衡数值的整体分布与少数 outlier 的分布，可以引入分块量化策略：将数据分成 1x128 或者 128x128 的 block，并对每个 block 选取一个缩放因子。对于大多数 block 可以选择较大的缩放因子来更好的利用动态范围，而对于存在 outlier 的 block 可以使用较小的缩放因子来避免出现上溢。</p> <pre class=mermaid><code>block-beta
    columns 16
    a1["0.01"] a2["0.02"] a3["0.01"] a4["..."] a5["→"] a6["100 x"] a7["1"] a8["2"] a9["1"] a10["..."] space:6
    b1["0.02"] b2["0.03"] b3["1.0"] b4["..."] b5["→"] b6["1 x"] b7["0.02"] b8["0.03"] b9["1.0"] b10["..."]

    style a4 fill:#FFFFFF, stroke:#FFFFFF
    style b4 fill:#FFFFFF, stroke:#FFFFFF
    style a10 fill:#FFFFFF, stroke:#FFFFFF
    style b10 fill:#FFFFFF, stroke:#FFFFFF
    style a5 fill:#FFFFFF, stroke:#FFFFFF
    style b5 fill:#FFFFFF, stroke:#FFFFFF
    style a6 fill:#EE0000, stroke:#FFFFFF
    style b6 fill:#00EE00, stroke:#FFFFFF</code></pre> <p>使用块粒度量化后的矩阵乘运算如下图所示：</p> <p><a class=glightbox href=../../../../imgs/fp8/blockwise_scaling_gemm.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/blockwise_scaling_gemm.png></a></p> <ul> <li> <p><strong>高精度 Accumulation：</strong> Nvidia H800 GPU 的 Tensor Core 在执行 FP8 矩阵计算时，累积精度被限制在大约 14 位，远小于 FP32 精度。当 LLM 训练的权重矩阵规模与输入规模变大时，这个问题会越来越显著。DeepSeek 团队的测试中，大小为 4096 的矩阵乘运算因为累积精度问题出现了最大 2%的相对误差。为了解决这个问题，引入了分级累加：使用 Tensor Core 默认的累加精度进行计算，当累积一定次数后，再将这个部分累加结果搬运到 CUDA Core 上进行 FP32 累加。</p> </li> <li> <p><strong>E4M3 与在线量化：</strong> 引入细粒度量化后，不再需要同时维护 E4M3 和 E5M2 两种精度，因此 DeepSeek 团队只使用 E4M3 数据格式。同时，为了维护好分块量化的缩放因子，并简化框架，使用算子内部的在线量化代替 TransformerEngine 中的延迟量化。</p> </li> </ul> <p>DeepSeek 团队在 FP8 精度上对算子和框架作了大幅度的优化，这些优化需要对模型框架、训练过程的动态性以及硬件的实现细节都有充分的了解。整体方案在计算、显存和通信上都有不小的收益：</p> <ul> <li>计算：DeepSeek 方案与 TransformerEngine 方案都能加速 Linear 相关的三次矩阵计算（前向，权重反向和激活反向），因此能够拿到的收益应该与 TransformerEngine 的 30%类似。少了一次权重的 cast，但是多出了块量化与高精度累加操作；</li> <li>存储：使用 16 位优化器状态，这部分显存开销降低一半。Attention 后的激活使用 12 bit 存储，对比 BF16 降低 25%。SwiGLU 部分激活通过 recompute 降低 4 倍，通过 FP8 再降低一半；</li> <li>通信：使用 FP8 将通信数据量降低一倍；</li> </ul> <h2 id=4-fp8>4. FP8 精度问题的系统分析<a class=headerlink href=#4-fp8 title="Permanent link">&para;</a></h2> <p>DeepSeek 团队已经分析了影响精度的一个主要因素——矩阵规模。在此基础上，我们进一步探讨影响精度的其他因素。算子精度涉及各种不同的数据类型、矩阵形状和运算类型(attention，matmul 等)，导致精度分析问题变得极为复杂。为了简化精度分析，我们通过对模型中的各种运算进行分析，发现大多数计算可以拆解成向量内积操作。因此，通过研究向量内积操作的精度，可以对算子精度进行全面分析。以 LLM 模型为例，主要运算如下：</p> <ol> <li>Attention 计算 Score 包含 Q 和 K 的内积计算；</li> </ol> <div class=arithmatex>\[ score = \text{Softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) =\text{Softmax}\left( \frac{ \begin{bmatrix} Q_0 \cdot K_0^T &amp; Q_0 \cdot K_1^T &amp; \dots \\ Q_1 \cdot K_0^T &amp; Q_1 \cdot K_1^T &amp; \dots \\ \vdots &amp; \vdots &amp; \ddots \\ \end{bmatrix} }{\sqrt{d_k}} \right) \]</div> <ol> <li>Attention 计算根据 Score 加权 V，本质是 score 向量与 V 不同 channel 向量的内积计算；</li> </ol> <div class=arithmatex>\[ Output = score \cdot V = score \cdot \begin{bmatrix} c_0 \\ c_1 \\ \vdots \end{bmatrix} = \begin{bmatrix} score \cdot c_0 \\ score \cdot c_1 \\ \vdots \end{bmatrix} \]</div> <ol start=3> <li>矩阵计算可以拆解成很多向量内积计算：</li> </ol> <div class=arithmatex>\[ A \times B = \begin{bmatrix} a_0 \\ a_1 \\ \vdots \end{bmatrix} \times \begin{bmatrix} b_0 ; b_1; \dots \end{bmatrix} = \begin{bmatrix} a_0 b_0 &amp; a_0 b_1 &amp; \dots \\ a_1 b_0 &amp; a_1 b_1 &amp; \dots \\ \vdots &amp; \vdots &amp; \ddots \\ \end{bmatrix} \]</div> <p>所以，研究 FP8 对 LLM 精度的影响，首先需要研究其对向量内积的数值精度的影响。经过实验，发现影响这个数值精度的主要因素有两个：</p> <ol> <li>向量长度的影响，向量长度越长，意味着 LLM 中权重矩阵规模越大；</li> <li>向量相关性的影响，LLM 中的 Attention 计算会涉及大量强相关的向量内积；</li> </ol> <h3 id=41>4.1. 向量长度的影响<a class=headerlink href=#41 title="Permanent link">&para;</a></h3> <p>先通过一个简单的实验观察下向量长度（矩阵规模）对误差的影响，在实验中构造满足分布 <span class=arithmatex>\(x_i \sim \mathcal{N}(0, 0.01)\)</span> 的随机向量<span class=arithmatex>\([x_i]\)</span>。将两个 float64 向量的内积作为对照组；将两个向量量化为 FP8 E4M3 格式在计算向量内积作为实验组。实验引入 tensor-wise scaling factor 来模拟 TransformerEngine 的行为，测试三组不同的向量长度下，scaling factor 对量化误差的影响。重复实验 200 次，绘制量化误差的直方分布如下：</p> <p><a class=glightbox href=../../../../imgs/fp8/scale_size.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/scale_size.png></a></p> <p>上图横轴为量化误差，纵轴为样本计数。随着向量长度的增大，内积计算中的量化误差呈现上升趋势。这可以归因于内积运算中，各项的量化误差在累加过程中不断叠加，导致整体误差明显增加。scaling factor 能够一定程度上改善量化误差的分布，量化误差分布更集中在零附近。为了更合理的评估量化误差的大小，可以引入信噪比概念<sup id=fnref:snr><a class=footnote-ref href=#fn:snr>2</a></sup>：</p> <div class=arithmatex>\[ SNR = 10 \times \log\_{10} \frac{\sum{signal_i^2} }{\sum{nosie_i^2}}\]</div> <p>其中：</p> <ul> <li>分子 <span class=arithmatex>\(\sum signal_i^2\)</span> 表示真实信号的能量；</li> <li>分母 <span class=arithmatex>\(\sum noise_i^2\)</span> 则代表量化误差（噪声）的能量；</li> </ul> <p>通过信噪比，我们可以快速确定信号比噪声能量大多少，比如 20 dB 代表着 <span class=arithmatex>\(10 \times \log_{10} 10^2\)</span>，信号能量比噪声能量大 100 倍，40 db 代表<span class=arithmatex>\(10 \times \log_{10} 10^4\)</span>，信号能量比噪声能量大 10000 倍。</p> <div class="admonition note"> <p class=admonition-title>如何解读信噪比数据</p> <ul> <li> <p><strong>正值 SNR：</strong> 说明信号能量大于噪声能量，较高的 SNR 表示量化误差对整体计算影响较小，数值更接近高精度计算的结果；</p> </li> <li> <p><strong>SNR 接近 0：</strong> 信号与噪声能量接近平衡，表示量化误差已经达到与信号能量相当的规模，此时运算结果可能开始出现较大偏差；</p> </li> <li> <p><strong>负值 SNR：</strong> 意味着累积的量化噪声能量超过了信号能量，信号在噪声中“被淹没”；</p> </li> </ul> </div> <h4 id=tensor-wise-scaling>Tensor-wise Scaling 的作用<a class=headerlink href=#tensor-wise-scaling title="Permanent link">&para;</a></h4> <p>通过信噪比来评估 TransformerEngine 中 tensor-wise scaling factor 的作用：</p> <p><a class=glightbox href=../../../../imgs/fp8/scale_size_snr.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/scale_size_snr.png></a></p> <p>在不进行 scaling 的情况下（scaling=1），内积计算的信噪比在在 0 附近，意味着信号与噪声能量差不多，计算误差极大。而引入 scaling 后，可显著改善信噪比的分布，大量样本的向量内积信噪比显著高于零，具有比较高的计算精度，但仍有大量 case 信噪比小于 0，即信号被噪声淹没。从这个实验可以看出，<strong>TransformerEngine 的 tensor-wise scaling 方案能够改善向量内积计算的信噪比。</strong> 接下来我们来分析 DeepSeek V3 的 FP8 方案在精度方面有哪些不同，是否能够带来更多收益。</p> <h4 id=_1>高精度累加的作用<a class=headerlink href=#_1 title="Permanent link">&para;</a></h4> <p>高精度累加是 DeepSeek V3 的 FP8 方案中的重要一环，是指使用较高的精度（32bit）进行向量内积的累加操作。受限于 Nvidia GPU 硬件的限制，TensorCore 默认累加使用 14 bit 累加器。为了提高累加精度，DeepSeek V3 使用了分 chunk 的累加方案：在 chunk 内部使用 tensor core 自带的低精度进行累加，在 chunk 间使用高精度累加，实验结果如下</p> <p><a class=glightbox href=../../../../imgs/fp8/scale_size_snr_chunk.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/scale_size_snr_chunk.png></a></p> <p>实验使用 tensor-wise scaling 系数 64，chunk=none 为不使用分 chunk 的高精度累加，等价于 TransformerEngine 的方案，可以作为对比参考。在引入分 chunk 高精度累加后，整体的信噪比分布有了较大的改善，chunk 越小，改善幅度越大。但是仍然存在大量 case，信噪比小于零。我们需要进一步优化 FP8 量化方案。</p> <h4 id=block-wise-scaling>Block-wise Scaling 的作用<a class=headerlink href=#block-wise-scaling title="Permanent link">&para;</a></h4> <p>Block-wise Scaling 是 DeepSeek V3 对 FP8 方案的另一个主要优化，是指对向量进行分块，并对每一个分块单独计算最优的缩放因子。对不同的分块儿 chunksize 进行实验，结果如下：</p> <p><a class=glightbox href=../../../../imgs/fp8/scale_size_snr_dynamic_scaling.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/scale_size_snr_dynamic_scaling.png></a></p> <p>这里 chunk=none 为不分 chunk，整个 tensor 计算一个最优的 scaling 系数，即退化为 tensor-wise scaling。对比完全没有动态 scaling 的结果，信噪比得到了大幅改进。另外两组实验分别在大小为 512 和 128 的 chunk 内计算最优的 scaling 系数，并进行缩放后在做内积计算。可以发现，通过引入 chunk 内 scaling 系数，可以大幅改进信噪比。chunk 越细，信噪比越好。引入 Block-wise 动态 Scaling 后，基本能够避免信噪比小于零（即信号被噪声淹没的情况）的情况。</p> <p>最后，我们看下 DeepSeek 团队是如何一步步达成 V3 中的 FP8 量化方案的：</p> <p><a class=glightbox href=../../../../imgs/fp8/dsv3_fp8.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/dsv3_fp8.png></a></p> <p>上图图展示了如何通过一步步优化，可以看到通过高精度累加，分块动态 scaling 与精心选择 chunk 大小，DeepSeekV3 的 FP8 方案较为完美的控制住了向量内积运算的信噪比。若单独分析每一项优化，其带来的提升从直方图上来看并不是特别的突出和显著。整体信噪比对比 Nvidia 的 TransformerEngine 方案具备优势，但没有特别显著的差异。</p> <h3 id=42>4.2. 向量相关性的影响<a class=headerlink href=#42 title="Permanent link">&para;</a></h3> <p>向量内积时影响结果的一个重要因素是两个向量的相关性，这里我们先看下当两个输入向量的相关性<span class=arithmatex>\(\rho\)</span>不同时，量化误差是否存在差别：</p> <p><a class=glightbox href=../../../../imgs/fp8/corr_raw.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/corr_raw.png></a></p> <p>实验测试了三个不同的向量长度下(128、1024 和 4096)，向量相关性对 FP8 内积计算误差的影响。可以发现，不管是哪一个向量长度，相关性越大，计算误差越大。而向量越长，相关性的影响也越大。为了更好的进行分析，我们仍然使用信噪比来度量误差。下图为直方图，横轴为信噪比，纵轴为样本计数：</p> <p><a class=glightbox href=../../../../imgs/fp8/corr_boxplot.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/corr_boxplot.png></a></p> <p>从上图可以看到，向量相关性对增强时，量化误差出现比较大的变化，会从比较发散的分布变成更加集中的分布。这有两方面的影响：一方面改进了最坏情况，信噪比小于 0 的情况在相关性<span class=arithmatex>\(\rho\)</span>增大时逐渐消失了；另一方面，信噪比的上限也出现了明显的下降。通过上述实验我们发现向量相关性对向量内积的量化误差具有很大影响，接下来我们分析 DeepSeek V3 的 FP8 优化在这种情况下的作用。</p> <h4 id=_2>高精度累加的作用<a class=headerlink href=#_2 title="Permanent link">&para;</a></h4> <p>我们测试长度为 4096 的向量在不同向量相关性情况下，内积的量化误差受分块高精度累加的影响如下图所示：</p> <p><a class=glightbox href=../../../../imgs/fp8/corr_boxplot_block_acc.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/corr_boxplot_block_acc.png></a></p> <p>从上图中可以明显看到，相关性越强信噪比的分布越集中，越偏向低信噪比。而使用分块高精度累加后，信噪比的分布得到了极大改善。相关性越强这种改善越为明显。</p> <h4 id=block-wise-scaling_1>Block-wise Scaling<a class=headerlink href=#block-wise-scaling_1 title="Permanent link">&para;</a></h4> <p>测试长度为 4096 的向量在不同向量相关性情况下，内积的量化误差受 Block-wise Scaling 的影响如下图所示：</p> <p><a class=glightbox href=../../../../imgs/fp8/corr_boxplot_block_scaling.png data-type=image data-width=auto data-height=auto data-desc-position=bottom><img alt="alt text" src=../../../../imgs/fp8/corr_boxplot_block_scaling.png></a></p> <p>从实验结果可以看出，在存在向量相关性的情况下，分块 scaling 策略能够显著提高信噪比。向量相关性越强，提升的效果越显著。</p> <p>这些发现为 LLM 训练中的 FP8 优化提供了重要参考：在处理 Attention 这样的高相关性计算时，应该优先考虑采用合适大小的分块策略，而不是简单地调整全局 scaling factor。</p> <h3 id=43>4.3. 结论<a class=headerlink href=#43 title="Permanent link">&para;</a></h3> <p>我们讨论了 TransformerEngine 与 DeepSeek V3 使用的两种不同的 FP8 训练方案，并通过向量内积的量化噪声分析，对比了两者在随机向量与相关向量下的表现，以解释 DeepSeek 的 FP8 训练方案成功的原因。在完全随机向量的情况下，通过一步步引入优化，逐步提高内积计算的信噪比。尽管无法确定 DeepSeek 团队在设计其 FP8 方案时是否使用了类似的量化分析手段，但这种分析方法也许可以帮助我们进一步改进 FP8 或者其他低精度方案。</p> <p>当进一步分析相关向量的内积信噪比时，迎来了一个关键的顿悟时刻（Aha Moment）。对比 TransformerEngine，DeepSeek 的方案虽有所优势，但还不够惊艳。然而，当考虑向量相关性后，观察到了非常明显的信噪比提升。这种显著的提升并非偶然，而是 DeepSeek 团队精心设计与验证的结果。毕竟，在真实任务上验证完整的 FP8 方案需要巨额的投入，很难想象 DeepSeek 团队会直接在模型上进行端到端训练来优化方案的消融实验。关于 FP8 的完整故事应该是这样的：</p> <ol> <li>影响模型精度的主要因素是矩阵规模与向量相关性；</li> <li>TransformerEngine 通过引入 tensor-wise scaling，比较好的解决了大 Tensor 矩阵乘法的精度问题，但是忽视了 LLM 训练过程中的数值分布层面的动力学特征（Training Dynamic）。在训练过程中，权重内部会逐渐呈现出相关性，导致矩阵运算频繁面临高相关性的情况。因此，Transformer Engine 的 FP8 方案一直存在训练误差，未能被 LLM 厂商广泛采用。；</li> <li>DeepSeek 团队创造性的引入了 block-wise scaling，并且对 Training Dynamic 有细致的分析，通过结合 block-wise scaling 与高精度分块累加，很好的控制了高相关情况下的量化噪声，并因此取得了成功。</li> </ol> <p>本文实验代码可见：<a href=https://github.com/reiase/ieee754_simulation/blob/master/simfloat_fp8_e4m3_v2.ipynb>https://github.com/reiase/ieee754_simulation/blob/master/simfloat_fp8_e4m3_v2.ipynb</a>。若对 INT8 训练感兴趣，可以参考本文姊妹篇：<a href=../../../../../../../2025/02/15/int8_training>INT8 也能训练</a>。</p> <div class=footnote> <hr> <ol> <li id=fn:fp8> <p><a href=https://arxiv.org/pdf/2209.05433>FP8 FORMATS FOR DEEP LEARNING</a>&#160;<a class=footnote-backref href=#fnref:fp8 title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:snr> <p><a href=https://www.mathworks.com/help//releases/R2021a/signal/ref/snr.html>SNR 信噪比定义</a>&#160;<a class=footnote-backref href=#fnref:snr title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> <li id=fn:huang_law_1> <p><a href=https://eprints.maths.manchester.ac.uk/2774/1/fhmp20.pdf>Numerical Behavior of NVIDIA Tensor Cores</a>&#160;<a class=footnote-backref href=#fnref:huang_law_1 title="Jump back to footnote 3 in the text">&#8617;</a></p> </li> <li id=fn:huang_law_2> <p><a href=https://blogs.nvidia.cn/blog/huangs-law-dally-hot-chips/ >聚焦黄氏定律：NVIDIA 首席科学家 Bill Dally 介绍推动 GPU 性能提升的关键因素</a>&#160;<a class=footnote-backref href=#fnref:huang_law_2 title="Jump back to footnote 4 in the text">&#8617;</a></p> </li> <li id=fn:nvidia_gpu> <p><a href=https://spectrum.ieee.org/nvidia-gpu>The Secret to Nvidia’s AI Success</a>&#160;<a class=footnote-backref href=#fnref:nvidia_gpu title="Jump back to footnote 5 in the text">&#8617;</a></p> </li> <li id=fn:ieee754> <p><a href=https://en.wikipedia.org/wiki/IEEE_754>IEEE 754</a>&#160;<a class=footnote-backref href=#fnref:ieee754 title="Jump back to footnote 6 in the text">&#8617;</a></p> </li> </ol> </div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title=最后更新> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年2月23日</span> </span> <span class=md-source-file__fact> <span class=md-icon title=创建日期> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2025年2月13日</span> </span> </aside> <h2 id=__comments>评论</h2> <script src=https://giscus.app/client.js data-repo=reiase/reiase.github.io data-repo-id=R_kgDOHRMrag data-category=[在此输入分类名] data-category-id="[在此输入分类 ID]" data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async>
  </script> <!-- Synchronize Giscus theme with palette --> <script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../../09/rl_ds_r1/ class="md-footer__link md-footer__link--prev" aria-label="上一页: 从强化学习到DeepSeek R1"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> 从强化学习到DeepSeek R1 </div> </div> </a> <a href=../../15/int8_training/ class="md-footer__link md-footer__link--next" aria-label="下一页: INT8也能训练"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> INT8也能训练 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2023 - 2025 Reiase </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/reiase target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../../assets/javascripts/bundle.f1b6f286.min.js></script> <script src=../../../../assets/_markdown_exec_pyodide.js></script> <script src=../../../../javascripts/mathjax.js></script> <script src=https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/mathjax/3.2.0/es5/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>