---
title: 关于分布式模型并行的分正式评论
date: 2024-01-25
tags:
  - LLM
  - Training
categories:
  - LLM
  - Training
slug: review-dist-train
---
分布式并行训练主要解决两类问题：
1. 模型分片：模型大小远超单节点存储上来，需要多节点分片存储和计算担；
2. 并行训练：提高单位时间内的算力密度，进而降低整体训练时间；
分布式并行训练几乎总是会引入昂贵的成本，比如增加了昂贵的多节点通信、引入了额外的多机稳定性问题、以及额外的开发与调试成本等，因此我们应该尽量避免引入分布式并行训练。而不得不引入分布式训练的场景。

## 3D模型并行

![[dp_tp_pp.png]]

## Tensor 并行
## Pipeline并行
## Sequence并行
