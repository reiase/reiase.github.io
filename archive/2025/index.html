<!DOCTYPE html><html class=no-js lang=zh> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="Overfitting: Where we deliberately dive too deep into ML algorithms,  mathematical principles and hardware implementations." name=description><meta content="reiase <reiase@gmail.com>" name=author><link href=https://reiase.github.io/archive/2025/ rel=canonical><link href=../../category/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/ rel=prev><link href=../2024/ rel=next><link href=../../assets/favicon.png rel=icon><meta content="mkdocs-1.6.1, mkdocs-material-9.6.21" name=generator><title>2025 - Overfitting: From Algorithms to Silicon</title><link href=../../assets/stylesheets/main.2a3383ac.min.css rel=stylesheet><link href=../../assets/stylesheets/palette.06af60db.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Source+Han+Sans+SC:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Source Han Sans SC";--md-code-font:"Roboto Mono"}</style><link href=../../assets/_markdown_exec_pyodide.css rel=stylesheet><link href=../../extra.css rel=stylesheet><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LMDTYMWWTF"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LMDTYMWWTF",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LMDTYMWWTF",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script> <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#2025> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav aria-label=页眉 class="md-header__inner md-grid"> <a aria-label="Overfitting: From Algorithms to Silicon" class="md-header__button md-logo" data-md-component=logo href=../.. title="Overfitting: From Algorithms to Silicon"> <svg viewbox="0 0 89 89" xmlns=http://www.w3.org/2000/svg> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z></path> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"></path> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z></path> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"></path> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Overfitting: From Algorithms to Silicon </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 2025 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary=indigo data-md-color-scheme=default id=__palette_0 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"></path></svg> </label> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary=black data-md-color-scheme=slate id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_0 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=搜索 autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=搜索 required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label=查找 class=md-search__options> <a aria-label=分享 class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=分享> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button aria-label=清空当前内容 class="md-search__icon md-icon" tabindex=-1 title=清空当前内容 type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix tabindex=0> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> <nav aria-label=标签 class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a class=md-tabs__link href=../..> 博客 </a> </li> <li class=md-tabs__item> <a href=../../tags/ class=md-tabs__link> 标签 </a> </li> <li class=md-tabs__item> <a href=../../nodes/AdamW/ class=md-tabs__link> 代码片段 </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=导航栏 class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="Overfitting: From Algorithms to Silicon" class="md-nav__button md-logo" data-md-component=logo href=../.. title="Overfitting: From Algorithms to Silicon"> <svg viewbox="0 0 89 89" xmlns=http://www.w3.org/2000/svg> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z></path> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"></path> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z></path> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"></path> </svg> </a> Overfitting: From Algorithms to Silicon </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_1 type=checkbox> <div class="md-nav__link md-nav__container"> <a class=md-nav__link href=../..> <span class=md-ellipsis> 博客 </span> </a> <label class=md-nav__link for=__nav_1 id=__nav_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav aria-expanded=true aria-labelledby=__nav_1_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> 博客 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_1_2 type=checkbox> <label class=md-nav__link for=__nav_1_2 id=__nav_1_2_label tabindex> <span class=md-ellipsis> 归档 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_1_2_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_1_2> <span class="md-nav__icon md-icon"></span> 归档 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 2025 </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2025 </span> </a> <nav aria-label=目录 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#nvidia> <span class=md-ellipsis> 深度解析NVIDIA的超节点架构演进 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#probingprofiling> <span class=md-ellipsis> Probing分布式探针开发随笔（三）：分布式训练的Profiling </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#probing> <span class=md-ellipsis> Probing分布式探针开发随笔（二）：探针机制 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#probing> <span class=md-ellipsis> Probing分布式探针开发随笔（一）：背景与设计理念 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#training-dynamicsoutlierllm> <span class=md-ellipsis> 从Training Dynamics到Outlier——LLM模型训练过程中的数值特性分析 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#int8> <span class=md-ellipsis> INT8也能训练 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#deepseek-v3fp8> <span class=md-ellipsis> 从DeepSeek V3看FP8训练的挑战 </span> </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#deepseek-r1> <span class=md-ellipsis> 从强化学习到DeepSeek R1 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" id=__nav_1_3 type=checkbox> <label class=md-nav__link for=__nav_1_3 id=__nav_1_3_label tabindex> <span class=md-ellipsis> 分类 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_1_3_label class=md-nav data-md-level=2> <label class=md-nav__title for=__nav_1_3> <span class="md-nav__icon md-icon"></span> 分类 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/ai%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/ class=md-nav__link> <span class=md-ellipsis> AI基础设施 </span> </a> </li> <li class=md-nav__item> <a href=../../category/fp8/ class=md-nav__link> <span class=md-ellipsis> FP8 </span> </a> </li> <li class=md-nav__item> <a href=../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../category/rl/ class=md-nav__link> <span class=md-ellipsis> RL </span> </a> </li> <li class=md-nav__item> <a href=../../category/training/ class=md-nav__link> <span class=md-ellipsis> Training </span> </a> </li> <li class=md-nav__item> <a href=../../category/training-dynamics/ class=md-nav__link> <span class=md-ellipsis> Training Dynamics </span> </a> </li> <li class=md-nav__item> <a href=../../category/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/ class=md-nav__link> <span class=md-ellipsis> 分布式训练 </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../tags/ class=md-nav__link> <span class=md-ellipsis> 标签 </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../nodes/AdamW/ class=md-nav__link> <span class=md-ellipsis> 代码片段 </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=2025>2025<a class=headerlink href=#2025 title="Permanent link">¶</a></h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-10-01 00:00:00+00:00">2025年10月1日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/ai%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/ class=md-meta__link>AI基础设施</a>, <a href=../../category/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/ class=md-meta__link>分布式训练</a></li> <li class=md-meta__item> 需要 12 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=nvidia><a href=../../2025/10/01/superpod-ai-architecture/ class=toclink>深度解析NVIDIA的超节点架构演进</a></h2> <p><em>本文为原创文章，版权归作者所有。未经许可，禁止转载。</em></p> <h3 id=_1><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_1>什么是超节点</a></h3> <p>超节点是借助高速无损互联技术，突破传统计算节点以CPU和PCIe总线为核心的通信边界，构建的新一代计算架构。<strong>在硬件互联层面</strong>，超节点采用NVLink、CXL或专用交换网络等先进互连协议，在加速卡（GPU/NPU）之间构建了高带宽、低延迟的直接通信域（Scale-Up Domain，或称高带宽域，High Bandwidth Domain, HBD）。这种设计实现了计算单元间的大规模高效互连，缓解了传统架构中因GPU间通信必须经由CPU与PCIe总线而形成的性能瓶颈，为海量数据并行处理奠定了物理基础。<strong>在软件与系统层面</strong>，其资源管理范式也随之转变：硬件间的高速通信通常直接绕过（bypass）操作系统内核繁复的协议栈，转而通过用户态集合通信库（如NCCL、HCCL）进行调度，从而显著降低通信开销。</p> <h4 id=nvidia_1><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#nvidia_1>NVIDIA超节点产品</a></h4> <p>2020年，NVIDIA在其推出的HGX-A100系统中，通过第二代NVSwitch将两个八卡A100以背板方式连接，构成一个16卡系统。2022年，随Hopper架构推出的第三代NVSwitch支持更灵活的组网方式，能够实现32颗GH200（32x GPU）的互联（NVL32）<sup id=fnref2:gh200><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:gh200>1</a></sup>，最大可实现256颗GH100的互联（NVL256）。2024年Blackwell发布时，第四代NVSwitch能够实现36个GB200超级芯片（共72颗GPU）的互联（NVL72）<sup id=fnref3:gb200><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:gb200>2</a></sup>，最大支持288个GB200超级芯片（共576颗GPU）的互联。未来的Vera Rubin系列将进一步实现144个超级芯片的互联。以下是Hopper与Blackwell两代GPU所对应的超节点产品：</p> <table> <thead> <tr> <th style="text-align: right;">参数</th> <th style="text-align: center;">NVL32</th> <th style="text-align: center;">GH200 SuperPod</th> <th style="text-align: center;">NVL72</th> <th style="text-align: center;">GB200 SuperPod</th> </tr> </thead> <tbody> <tr> <td style="text-align: right;"><strong>架构</strong></td> <td style="text-align: center;">Hopper</td> <td style="text-align: center;">Hopper</td> <td style="text-align: center;">Blackwell</td> <td style="text-align: center;">Blackwell</td> </tr> <tr> <td style="text-align: right;"><strong>HBM 大小</strong></td> <td style="text-align: center;">32 x 144GB = 4.6 TB</td> <td style="text-align: center;">256 x 96GB = 24.5 TB</td> <td style="text-align: center;">36 x 384GB = 13.8 TB</td> <td style="text-align: center;">288 x 384GB = 110 TB</td> </tr> <tr> <td style="text-align: right;"><strong>LPDDR5X 大小</strong></td> <td style="text-align: center;">32 x 480GB = 15.4 TB</td> <td style="text-align: center;">256 x 480GB = 123 TB</td> <td style="text-align: center;">36 x 480GB = 17.3 TB</td> <td style="text-align: center;">288 x 480GB = 138 TB</td> </tr> <tr> <td style="text-align: right;"><strong>HBM 带宽</strong></td> <td style="text-align: center;">3.35 TB/s</td> <td style="text-align: center;">4.8 TB/s</td> <td style="text-align: center;">8 TB/s</td> <td style="text-align: center;">8 TB/s</td> </tr> <tr> <td style="text-align: right;"><strong>FP16 (FLOPS)</strong></td> <td style="text-align: center;">32 PetaFLOPS</td> <td style="text-align: center;">256 PetaFLOPS</td> <td style="text-align: center;">180 PetaFLOPS</td> <td style="text-align: center;">1440 PetaFLOPS</td> </tr> <tr> <td style="text-align: right;"><strong>INT8 (OPS)</strong></td> <td style="text-align: center;">64 PetaOPS</td> <td style="text-align: center;">64 PetaOPS</td> <td style="text-align: center;">360 PetaOPS</td> <td style="text-align: center;">2880 PetaOPS</td> </tr> <tr> <td style="text-align: right;"><strong>FP8 (FLOPS)</strong></td> <td style="text-align: center;">64 PetaFLOPS</td> <td style="text-align: center;">64 PetaFLOPS</td> <td style="text-align: center;">360 PetaFLOPS</td> <td style="text-align: center;">2880 PetaFLOPS</td> </tr> <tr> <td style="text-align: right;"><strong>FP6 (FLOPS)</strong></td> <td style="text-align: center;">N/A</td> <td style="text-align: center;">N/A</td> <td style="text-align: center;">360 PetaFLOPS</td> <td style="text-align: center;">2880 PetaFLOPS</td> </tr> <tr> <td style="text-align: right;"><strong>FP4 (FLOPS)</strong></td> <td style="text-align: center;">N/A</td> <td style="text-align: center;">N/A</td> <td style="text-align: center;">720 PetaFLOPS</td> <td style="text-align: center;">5760 PetaFLOPS</td> </tr> <tr> <td style="text-align: right;"><strong>GPU-GPU 带宽</strong></td> <td style="text-align: center;">0.9 TB/s</td> <td style="text-align: center;">0.9 TB/s</td> <td style="text-align: center;">1.8 TB/s</td> <td style="text-align: center;">1.8 TB/s</td> </tr> <tr> <td style="text-align: right;"><strong>NVSwitch</strong></td> <td style="text-align: center;">Gen3 64 Port</td> <td style="text-align: center;">Gen3 64 Port</td> <td style="text-align: center;">Gen4 72 Port</td> <td style="text-align: center;">Gen4 72 Port</td> </tr> <tr> <td style="text-align: right;"><strong>NVLink 带宽</strong></td> <td style="text-align: center;">36 x 0.9 TB/s = 32 TB/s</td> <td style="text-align: center;">256 x 0.9 TB/s = 230 TB/s</td> <td style="text-align: center;">72 x 1.8 TB/s = 130 TB/s</td> <td style="text-align: center;">576 x 1.8 TB/s = 1 PB/s</td> </tr> <tr> <td style="text-align: right;"><strong>Ethernet 带宽</strong></td> <td style="text-align: center;">16 x 200 Gb/s</td> <td style="text-align: center;">256 x 200 Gb/s</td> <td style="text-align: center;">18 x 400 Gb/s</td> <td style="text-align: center;">576 x 400 Gb/s</td> </tr> <tr> <td style="text-align: right;"><strong>IB 带宽</strong></td> <td style="text-align: center;">32 x 400 Gb/s</td> <td style="text-align: center;">256 x 400 Gb/s</td> <td style="text-align: center;">72 x 800 Gb/s</td> <td style="text-align: center;">576 x 800 Gb/s</td> </tr> <tr> <td style="text-align: right;"><strong>GPUs Power</strong></td> <td style="text-align: center;">32 x 1 kW = 32 kW</td> <td style="text-align: center;">256 x 1 kW = 256 kW</td> <td style="text-align: center;">36 x 2.7 kW = 97.2 kW</td> <td style="text-align: center;">Not provided</td> </tr> </tbody> </table> <h4 id=_2><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_2>超节点技术趋势分析</a></h4> <p>在2022年Hopper架构发布之际，NVIDIA提出了十年内GPU算力增长1000倍的“黄氏定律” (Huang’s Law)<sup id=fnref3:huangs_law><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:huangs_law>3</a></sup>。其中，低精度数值格式、Tensor Core和工艺进步分别贡献了约16倍、12倍和2.5倍的算力提升。这揭示出NVIDIA是一家系统供应商而非单纯的芯片供应商，其算力增长并非仅依赖芯片本身。</p> <p>回顾从Volta到Rubin系列的演进，NVIDIA的技术战略非常清晰：<strong>通过算力、互联、存储和封装等多个维度的协同创新，实现系统层面的指数级性能增长</strong> 。其目标是每两年提供约6倍的系统算力提升，并计划在十年内实现7000倍的增长（若考虑芯片在低精度和稀疏上能力的进步，这个增长可能超过10000倍）。这种复合式增长并非依赖单一技术突破，而是通过一套精心设计的“组合策略”实现：</p> <ul> <li><strong>单芯片算力</strong>：每代提升约3倍。</li> <li><strong>Scale-Up域</strong>：互联规模和带宽同步翻倍。</li> <li><strong>内存系统</strong>：HBM带宽翻倍，容量提升3倍。</li> </ul> <p>从Blackwell架构开始，<strong>先进封装</strong> 成为其算力增长的又一关键。通过NV-HBI（NVIDIA High-Bandwidth Interface）技术<sup id=fnref2:nvhbi><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:nvhbi>4</a></sup>，NVIDIA将两颗GPU裸片（Die）高速互联，提供高达10 TB/s的双向带宽，使它们在逻辑上可作为单一、统一的GPU工作。这标志着NVIDIA的增长引擎已从单纯提升单点指标（如芯片算力或互联速率），全面转向以系统为单位的整体工程优化，从而确保稳定且可预测的性能飞跃。</p> <table> <thead> <tr> <th>系列</th> <th>Volta</th> <th>Ampere</th> <th>Hopper</th> <th>Blackwell</th> <th>Rubin</th> </tr> </thead> <tbody> <tr> <td>SKU</td> <td>V100</td> <td>A100(HGX)</td> <td>GH200(CPU+GPU)</td> <td>GB200(CPU+2GPU)</td> <td>?VR200(CPU+2GPU)</td> </tr> <tr> <td>单卡算力（FP16）</td> <td>0.125 PFLOPS</td> <td>0.312 PFLOPS</td> <td>0.9 PFLOPS</td> <td>2.25 PFLOPS x 2</td> <td></td> </tr> <tr> <td>系统规模</td> <td>8</td> <td>16(OEM提供)<sup id=fnref2:hgx_a100><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:hgx_a100>10</a></sup></td> <td>32</td> <td>72</td> <td></td> </tr> <tr> <td>系统总算力</td> <td>1 PFLOPS</td> <td>5 PFLOPS</td> <td>32 PFLOPS</td> <td>180 PFLOPS</td> <td></td> </tr> <tr> <td>系统HBM总容量</td> <td>8 x 32GB = 0.256 TB</td> <td>16 x 80GB = 1.28 TB</td> <td>32 x 144GB = 4.6 TB|</td> <td>36 x 384GB = 13.8 TB</td> <td></td> </tr> </tbody> </table> <p>上述代际指标体现的是“系统级（按FP16口径）”的复合放大轨迹。结合黄氏定理（Huang’s Law）所给出的十年≈1000倍增长拆解：低精度数值格式≈16×、Tensor Core与矩阵引擎≈12×、制程与微架构≈2.5×<sup id=fnref4:huangs_law><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:huangs_law>3</a></sup>，可以得出一个结论：<strong>单纯依赖芯片工艺或单点架构优化已无法支撑指数级提升，必须依靠多维协同的系统工程</strong>。</p> <p>超节点正是这一“系统化方法”的工程化载体：以FP16系统算力观测，每两年约≈6×，十年区间潜在≈7000×；若叠加Blackwell世代引入的FP6/FP4等更低精度执行路径，其有效可用算力（在可接受精度与稀疏/混合精度策略前提下）还可能进一步放大到“&gt;10000×”数量级（例如：FP4 聚合算力 720 PetaFLOPS（NVL72）至 5760 PetaFLOPS（GB200 SuperPod）相较 Hopper FP8 64 PetaFLOPS 的 11–90× 跨代放大）。注：这里不同“倍数”来源于不同统计口径（纯FP16 vs. 引入低精度/稀疏优化后的可达理论峰值）。<strong>对于任何希望在超节点方向与NVIDIA竞争的厂商而言，系统性协同是必须的战略</strong> 。</p> <p>因此，超节点已不仅是硬件形态命名，而是一个将“互联-存储-算力-编程模型”联动优化的系统工程范式。竞争策略若缺失系统性视角，极易在实际大模型训练/推理场景中遭遇利用率折损与规模效率崖降。下文将分别从硬件基石与软件生态两个层面继续拆解其内在结构。</p> <h3 id=_3><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_3>超节点硬件系统</a></h3> <p>如前文所述，超节点通过系统级创新来支撑算力曲线的持续指数级增长。之所以要突破传统的”Scale-Out”扩展AI集群的范式，主要原因在于：</p> <ul> <li><strong>通信瓶颈</strong>：PCIe与以太网的带宽增长难以匹配NVLink每两年带宽翻倍的快速增长。以H100为例，NVLink 4.0提供900GB/s的GPU间带宽，而PCIe 5.0 x16仅128GB/s，相差7倍。随着模型规模增长，这种差距会成为整个AI算力集群的核心瓶颈。</li> <li><strong>内存墙问题</strong>：单节点GPU的显存容量有限，单纯HBM的迭代节奏难以满足大模型因模型规模和序列长度而快速增长的显存需求。GPT-4级别的模型需要数TB的显存，远超单节点8卡GPU的容量。</li> <li><strong>扩展效率递减</strong>：在Scale-Out域，算力利用率随节点规模的增长而衰减。</li> </ul> <p>超节点通过”Scale-Up”架构创新地解决了这些问题：在机柜级别构建高速无损互联域，将数十甚至数百个GPU通过NVLink/NVSwitch直接互联，形成一个逻辑上的”超级GPU”。这种架构转变不仅是技术参数的升级，更是AI基础设施设计理念的根本性变革。为了系统性地理解超节点硬件架构，我们需要从多芯片系统的互联层次出发，分析从微观到宏观的技术栈如何协同工作，最终支撑起大规模AI训练的需求。下图展示了从处理单元到集群的完整硬件构建蓝图，其中超节点最大的创新在于引入了”SuperPod”这一新层级，彻底改变了传统AI集群的互联拓扑。</p> <div style=display:flex;gap:1rem;flex-wrap:wrap;> <div style=flex:1;min-width:200px;> <pre class=mermaid><code>block-beta
    columns 4
    PE0["PE"] space:2 PE1["PE"]
    space:4
    Die0["Die"] space:2 Die1["Die"]
    space:4
    Chip0["Chip"] space:2 Chip1["Chip"]
    space:4
    Node0["Node"] space:2 Node1["Node"]
    space:4
    Cluster0["Cluster"] space:2 Cluster1["Cluster"]

    PE0&lt;-- "NOC" --&gt;PE1
    Die0&lt;-- "D2D" --&gt;Die1
    Chip0&lt;-- "C2C" --&gt;Chip1
    Node0&lt;-- "RDMA" --&gt;Node1
    Cluster0&lt;-- "DCN" --&gt;Cluster1

    PE0-- "NOC" --&gt;Die0
    PE1-- "NOC" --&gt;Die1
    Die0-- "Chiplet" --&gt;Chip0
    Die1-- "Chiplet" --&gt;Chip1
    Chip0-- "PCIE" --&gt;Node0
    Chip1-- "PCIE" --&gt;Node1
    Node0-- "RDMA" --&gt;Cluster0
    Node1-- "RDMA" --&gt;Cluster1</code></pre> <center> 常规AI算力集群 </center> </div> <div style=flex:1;min-width:200px;> <pre class=mermaid><code>block-beta
    columns 4
    PE0["PE"] space:2 PE1["PE"]
    space:4
    Die0["Die"] space:2 Die1["Die"]
    space:4
    Chip0["Chip"] space:2 Chip1["Chip"]
    space:4
    Node0["Node"] space:2 Node1["Node"]
    space:4
    Pod0["SuperPod"] space:2 Pod1["SuperPod"]
    space:4
    Cluster0["Cluster"] space:2 Cluster1["Cluster"]

    PE0&lt;-- "NOC" --&gt;PE1
    Die0&lt;-- "D2D" --&gt;Die1
    Chip0&lt;-- "C2C" --&gt;Chip1
    Node0&lt;-- "C2C" --&gt;Node1
    Pod0&lt;-- "RDMA" --&gt;Pod1
    Cluster0&lt;-- "DCN" --&gt;Cluster1

    PE0-- "NOC" --&gt;Die0
    PE1-- "NOC" --&gt;Die1
    Die0-- "Chiplet" --&gt;Chip0
    Die1-- "Chiplet" --&gt;Chip1
    Chip0-- "PCIE" --&gt;Node0
    Chip1-- "PCIE" --&gt;Node1
    Node0-- "C2C" --&gt;Pod0
    Node1-- "C2C" --&gt;Pod1
    Pod0-- "RDMA" --&gt;Cluster0
    Pod1-- "RDMA" --&gt;Cluster1</code></pre> <center> 超节点AI算力集群 </center> </div> </div> <p>现代AI超算系统的硬件架构遵循自底向上的层次化设计原则，每一层都通过特定的互联技术将计算单元组织成更大规模的计算资源。这种层次化设计不仅体现了硬件工程的物理约束，也反映了系统架构师从微观到宏观的资源组织思维：</p> <ol> <li><strong>层级1 - 芯粒内部 (Die)</strong>: 系统的最基本计算单元是 <strong>处理单元 (PE)</strong> ，例如GPU中的流式多处理器(SM)。在单个硅片(<strong>Die</strong>)上，众多的PE通过 <strong>片上网络 (NoC)</strong> 高效互联。</li> <li><strong>层级2 - 芯片 (Chip)</strong>: 借助先进封装技术(Chiplet)，多个独立的 <strong>芯粒 (Die)</strong> 被封装在一起，构成一个完整的 <strong>芯片 (Chip)</strong> 。它们之间通过高速的 <strong>Die-to-Die (D2D)</strong> 接口（如NV-HBI, UCIe）通信，使其在逻辑上表现得像一个单片大芯片。</li> <li><strong>层级3 - 节点 (Node)</strong>: 一个服务器 <strong>节点 (Node)</strong> 通常包含多个 <strong>芯片 (Chip)</strong>（如多块GPU）和CPU。节点内的GPU之间通过 <strong>芯片间互联 (C2C)</strong> 技术（如NVLink + NVSwitch）构建高速通信域，而GPU与CPU之间则主要通过 <strong>PCIe总线</strong> 连接。</li> <li><strong>层级4 - 集群 (Cluster)</strong>: 多个 <strong>节点 (Node)</strong> 组合成一个 <strong>集群 (Cluster)</strong>。节点之间的通信（Scale-Out）依赖于 <strong>数据中心网络</strong> ，通常使用基于 <strong>RDMA</strong> 的InfiniBand或RoCE高速网络。</li> </ol> <h4 id=_4><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_4>硬件互联技术</a></h4> <p>超节点的核心特征在于通过高速无损互联技术构建大规模计算域。其互联体系遵循从微观到宏观的层次化结构，从芯片内部的片上网络到集群级的数据中心网络，形成了完整的互联技术栈。典型的多芯片计算系统互联结构如下图所示：</p> <p><img alt=芯片互联.drawio.svg src=../../imgs/superpod/%E8%8A%AF%E7%89%87%E4%BA%92%E8%81%94.drawio.svg title=芯片互联.drawio.svg></p> <p>芯片（Chip）通常由不同的芯粒（Die）组成，比如CCD（Core Complex Die）和IOD（I/O Die）。其中前者通常包含CPU或GPU的主要计算单元，而后者则负责PCIe、内存控制器等I/O相关功能。通过CCD与IOD不同数量与规格的组合，可以制造出不同规格的芯片产品。在每个Die内部，通常使用片上网络（Network-on-Chip, NOC）来将处理单元（Processing Element, PE）组织起来，形成GPU上的计算单元，比如流式多处理器（Streaming Multiprocessor, SM）。而Die之间通过Die-to-Die（D2D）互联进行通信。芯片之间则通过Chip-to-Chip（C2C）互联进行通信。<br> 不论是片上的NOC通信，还是D2D、C2C通信，都可以在逻辑上分为三层抽象：</p> <ul> <li><strong>物理层</strong>：负责定义互联的物理形态、连接拓扑与信号的电气特性（频率、电压等）。</li> <li><strong>链路层</strong>：负责定义数据如何进行高效可靠的传递，不负责理解数据的内容。</li> <li><strong>事务层</strong>：负责定义事务处理逻辑，比如读写请求、内存一致性等。</li> </ul> <p>为了方便理解，我们通过一张表格描绘当前NOC、D2D和C2C三层通信在物理、链路和事务三层之上有哪些选择：</p> <table> <thead> <tr> <th></th> <th>物理层</th> <th>链路层</th> <th>事务层</th> </tr> </thead> <tbody> <tr> <td>Die内通信</td> <td>自定义的金属走线<br>使用数字信号</td> <td>NoC网络<br>包含路由算法的定义<br>流控机制和数据包定义</td> <td>片上总线协议，比如<br>AMBA AXI和AMBA CHI</td> </tr> <tr> <td>Die间通信</td> <td>- <strong>BoW (Bunch of Wires)</strong>：OCP定义的简化的die-to-die互连物理层协议<br>- <strong>UCIe PHY</strong>：UCIE的物理层规范<br>- 私有规范，比如AMD Infinity Fabric PHY, Intel AIB/Foveros PHY</td> <td><strong>UCIe D2D Adapter</strong>: 负责链路训练、管理、CRC校验、重传机制<br><strong>私有实现</strong>: AMD/Intel的私有链路管理逻辑</td> <td><strong>私有实现</strong>: AMD Infinity Fabric Protocol (支持一致性)<br><strong>UCIe Protocol Layer</strong>: 承载 <strong>PCIe</strong>, <strong>CXL</strong>, 和其他原生流协议(Streaming)</td> </tr> <tr> <td>片间通信</td> <td>- <strong>CEI</strong>: OIF制定的高速电气I/O规范，包含25.6G/56G/112G等多种规范<br>- <strong>PCIe PHY</strong>: 遵循或参考CEI<br>- <strong>NVLink PHY</strong>: NVIDIA私有SerDes实现<br>- <strong>以太网PHY</strong>: 如以太网光/电模块</td> <td><strong>PCIe Link Layer</strong>: 流量控制、ACK/NAK、数据包排序<br><strong>CXL.io Link Layer</strong>: 复用PCIe的链路层<br><strong>NVLink Link Layer</strong>: NVIDIA私有链路管理和流控<br><strong>Ethernet MAC Layer / RoCE</strong></td> <td><strong>PCIe Transaction Layer</strong>: 内存读写(RW)、配置、消息<br><strong>CXL (.cache &amp; .mem)</strong>: 实现缓存一致性、内存扩展<br><strong>NVLink Protocol</strong>: GPU间P2P内存访问、原子操作<br><strong>TCP/IP, RoCEv2</strong>: 基于以太网的应用层协议</td> </tr> </tbody> </table> <h4 id=_5><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_5>电互联关键技术基础</a></h4> <h5 id=laneserdes><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#laneserdes>基础物理单元：Lane与SerDes</a></h5> <p>当前高速数字信号通信普遍采用串行通信技术。其基础物理单元是 <strong>Lane</strong>，一个 Lane 通常由两对差分信号线组成：一对用于发送（Tx），一对用于接收（Rx），从而实现全双工通信。无论是 PCIe、NVLink 还是高速以太网，都是通过将多个 Lane 聚合（bonding）来获得更高总带宽。</p> <p>芯片内部的数据通常是并行的（如 64 位或 128 位），为了在 Lane 上进行高速串行传输，需要一种专门的电路负责并行数据与串行信号之间的转换。这种电路被称为 <strong>SerDes</strong> (Serializer/Deserializer，串行器/解串器)。SerDes 将芯片内部的并行数据转换为高速串行信号发送出去，并在接收端将串行信号转换回并行数据。因此，单条 Lane 的传输速率直接受 SerDes 的能力和传输介质（线缆、PCB走线）的物理特性限制。</p> <h5 id=oif-cei><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#oif-cei>标准化接口：OIF-CEI规范</a></h5> <p>为了确保不同厂商设备间的互联互通，光互联论坛（OIF）制定了通用电气接口（CEI）规范，对电气接口的物理形态、电压、频率以及信号调制方式等进行了标准化。例如，CEI-56G/112G/224G 等规范定义了单通道（per-lane）在 56Gbps、112Gbps、224Gbps 速率下的接口标准，其中广泛使用了 PAM4（4-Level Pulse Amplitude Modulation）等高级调制技术来提升数据速率。这些规范被 PCIe、CXL、NVLink 和以太网等主流互联协议广泛采纳或参考，作为其物理层设计的基础。</p> <h5 id=cei><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#cei>主流协议的CEI实现</a></h5> <p>PCIe 和 NVLink 作为两种主流的片间互联协议，其核心创新在于链路层与事务层，而在物理层则高度依赖成熟的电气标准。它们的物理层设计与 CEI 规范密切相关，通常会选择某个版本的 CEI 规范作为其 SerDes 设计的电气基础，从而在确保信号可靠性的前提下，专注于上层协议的优化。例如：</p> <ul> <li><strong>PCIe</strong>: PCIe 5.0 的 32 GT/s 速率在电气特性上参考了 OIF CEI-28G 标准。而到了 PCIe 6.0，其 64 GT/s 的速率采用了 PAM4 调制技术，其电气特性与 OIF CEI-56G 系列标准的设计原则更为接近。</li> <li><strong>NVLink</strong>: NVIDIA H100 GPU 使用的第四代 NVLink，其单 Lane 单向速率为 100 Gbps（采用PAM4调制，波特率为50Gbaud），这与 OIF CEI-56G-PAM4 标准的电气特性一致。未来的 NVLink 版本预计将继续跟进更高速率的 CEI-112G/224G 标准。</li> </ul> <p>通过这种方式，PCIe 和 NVLink 等协议可以复用业界成熟的电气标准，专注于其上层协议的创新。因此，OIF-CEI 规范的演进路线图，也成为了预判整个互联技术发展的关键风向标。目前CEI规范的主要节点如下：</p> <table> <thead> <tr> <th style="text-align: left;">规范系列</th> <th style="text-align: left;">发布年份 (约)</th> <th style="text-align: left;">单通道速率 (Gbps)</th> <th style="text-align: left;">调制方式</th> <th style="text-align: left;">典型应用/参考协议</th> </tr> </thead> <tbody> <tr> <td style="text-align: left;"><strong>CEI-28G</strong></td> <td style="text-align: left;">~2011</td> <td style="text-align: left;">28</td> <td style="text-align: left;">NRZ</td> <td style="text-align: left;">100G 以太网 (4x25G), PCIe 4.0/5.0, InfiniBand EDR</td> </tr> <tr> <td style="text-align: left;"><strong>CEI-56G</strong></td> <td style="text-align: left;">~2017</td> <td style="text-align: left;">56</td> <td style="text-align: left;">PAM4</td> <td style="text-align: left;">200G/400G 以太网, PCIe 6.0, NVLink 4.0 (H100)</td> </tr> <tr> <td style="text-align: left;"><strong>CEI-112G</strong></td> <td style="text-align: left;">~2022</td> <td style="text-align: left;">112</td> <td style="text-align: left;">PAM4</td> <td style="text-align: left;">800G 以太网, CXL 3.0, 下一代 NVLink</td> </tr> <tr> <td style="text-align: left;"><strong>CEI-224G</strong></td> <td style="text-align: left;">-</td> <td style="text-align: left;">224</td> <td style="text-align: left;">PAM4</td> <td style="text-align: left;">1.6T/3.2T 以太网, 未来高速互联</td> </tr> </tbody> </table> <p><small><br> <em>注：NRZ (Non-Return-to-Zero) 每符号传输 1 bit 数据，PAM4 (Pulse Amplitude Modulation 4-level) 每符号传输 2 bit 数据，在相同波特率下可实现双倍数据速率。</em><br> </small></p> <p>OIF-CEI规范通常5-6年更新一代，每次发布新版本时速率翻倍。但上文提到的时间为定稿时间，在定稿前行业内就会有较多的讨论和实现，因此不能完全以规范的时间来推测相关产品的面世时间。</p> <h4 id=_6><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_6>电互联交换芯片的演进</a></h4> <p>交换芯片(Switch ASIC)是构建互联网络的核心，其交换容量和SerDes速率直接决定网络总带宽和端口密度。根据应用场景不同，主要分为数据中心网络交换芯片和GPU专用交换芯片两类。</p> <h5 id=_7><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_7>数据中心网络交换芯片</a></h5> <p>以Broadcom Tomahawk系列为代表的数据中心交换芯片，其演进与OIF-CEI规范紧密相关：</p> <table> <thead> <tr> <th style="text-align: left;">交换容量 (Tbps)</th> <th style="text-align: left;">SerDes 速率 (每Lane)</th> <th style="text-align: left;">CEI 代际对应</th> <th style="text-align: left;">代表芯片 (发布年)</th> <th style="text-align: left;">可支持的典型 4-Lane 端口 (示例)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left;">3.2T</td> <td style="text-align: left;">25G NRZ</td> <td style="text-align: left;">CEI-28G</td> <td style="text-align: left;">Tomahawk (2014)</td> <td style="text-align: left;">32 x 100G (4x25G)</td> </tr> <tr> <td style="text-align: left;">6.4T</td> <td style="text-align: left;">25G NRZ</td> <td style="text-align: left;">CEI-28G</td> <td style="text-align: left;">Tomahawk 2 (2016)</td> <td style="text-align: left;">64 x 100G (4x25G)</td> </tr> <tr> <td style="text-align: left;">12.8T</td> <td style="text-align: left;">50G PAM4</td> <td style="text-align: left;">CEI-56G</td> <td style="text-align: left;">Tomahawk 3 (2018)</td> <td style="text-align: left;">64 x 200G (4x50G)</td> </tr> <tr> <td style="text-align: left;">25.6T</td> <td style="text-align: left;">100G PAM4</td> <td style="text-align: left;">CEI-112G</td> <td style="text-align: left;">Tomahawk 4 (2020)</td> <td style="text-align: left;">64 x 400G (4x100G)</td> </tr> <tr> <td style="text-align: left;">51.2T</td> <td style="text-align: left;">100G PAM4</td> <td style="text-align: left;">CEI-112G</td> <td style="text-align: left;">Tomahawk 5 (2022)</td> <td style="text-align: left;">128 x 400G (4x100G)</td> </tr> <tr> <td style="text-align: left;">102.4T (预测)</td> <td style="text-align: left;">200G PAM4</td> <td style="text-align: left;">CEI-224G</td> <td style="text-align: left;">下一代 (≈2025+)</td> <td style="text-align: left;">128 x 800G (4x200G) 或 64 x 1.6T (8x200G, 非 4-Lane)</td> </tr> </tbody> </table> <p><small><br> <em>注：上表中的“4-Lane 端口”指由 4 个 SerDes 通道组成的逻辑端口，在常见的国产GPU厂商中比较常见此种配置。例如，一个 100G 端口由 4x25G 组成，一个 400G 端口可由 4x100G 组成。随着 SerDes 速率翻倍，单芯片的总交换容量和端口速率也随之翻倍，这是网络技术演进的核心驱动力。</em><br> </small></p> <p>Broadcom的高端交换芯片通常两年一代，交换容量翻倍。而从芯片产品面世到广泛被交换机使用，大约也需要1-2年时间。此外，从Broadcom芯片规格的发展也能看出，行业领先厂商对CEI标准的实现和落地通常会先于规范的正式定稿。</p> <h5 id=gpunvswitch><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#gpunvswitch>GPU专用交换芯片：NVSwitch</a></h5> <p>NVSwitch 负责在单机或机柜域内构建 GPU 全互连（all-to-all / non-blocking）通信结构，其能力随 NVLink 代际一起提升。提升路径主要有两种：增加每 GPU 可用的 NVLink 数量（link fan-out）与提高单条 NVLink 的速率。下表采用 “GB/s (双向)” 口径（NVIDIA 官方常用聚合带宽，等于 2 × 单方向带宽；1 GB/s ≈ 8 Gb/s）。本表中：† 表示来自官方白皮书 / 数据手册 / Keynote；* 表示根据公开演讲、产品图片、拓扑描述或带宽反推的推测/半推测值；未标记默认为公开资料已多点交叉验证。</p> <table> <thead> <tr> <th style="text-align: left;">代际</th> <th style="text-align: left;">GPU 架构</th> <th style="text-align: left;">发布（约）</th> <th style="text-align: left;">NVLink 版本</th> <th style="text-align: left;">每 Link Lane 数</th> <th style="text-align: left;">每 Lane 速率 (Gbps)</th> <th style="text-align: left;">每 Link 双向带宽 (GB/s)</th> <th style="text-align: left;">每 GPU Link 数</th> <th style="text-align: left;">每 GPU 聚合双向带宽 (GB/s)</th> <th style="text-align: left;">典型单机/底板 GPU 数</th> <th style="text-align: left;">可扩展最大 NVLink 域（官方宣称）</th> <th style="text-align: left;">备注</th> </tr> </thead> <tbody> <tr> <td style="text-align: left;">第1代</td> <td style="text-align: left;">Volta</td> <td style="text-align: left;">2018</td> <td style="text-align: left;">2.0</td> <td style="text-align: left;">8*</td> <td style="text-align: left;">25†</td> <td style="text-align: left;">50†</td> <td style="text-align: left;">6†</td> <td style="text-align: left;">300†</td> <td style="text-align: left;">16†</td> <td style="text-align: left;">16†</td> <td style="text-align: left;">16‑GPU DGX‑2 NVSwitch 拓扑；Lane 数依据公开拆解/布线推测 [^nvswitch_volta]</td> </tr> <tr> <td style="text-align: left;">第2代</td> <td style="text-align: left;">Ampere</td> <td style="text-align: left;">2020</td> <td style="text-align: left;">3.0</td> <td style="text-align: left;">4*</td> <td style="text-align: left;">50†</td> <td style="text-align: left;">50†</td> <td style="text-align: left;">12†</td> <td style="text-align: left;">600†</td> <td style="text-align: left;">8†</td> <td style="text-align: left;">16†</td> <td style="text-align: left;">HGX A100（8-GPU板）官方聚合 600GB/s；Lane 数由 Link 速率与功耗折中推测 [^nvlink3]</td> </tr> <tr> <td style="text-align: left;">第3代</td> <td style="text-align: left;">Hopper</td> <td style="text-align: left;">2022</td> <td style="text-align: left;">4.0</td> <td style="text-align: left;">4*</td> <td style="text-align: left;">100†</td> <td style="text-align: left;">100†</td> <td style="text-align: left;">18†</td> <td style="text-align: left;">1800†</td> <td style="text-align: left;">8†</td> <td style="text-align: left;">256†</td> <td style="text-align: left;">NVLink 4.0 单 GPU 900GB/s(单向450)→双向1800；最大 NVL256 由官方架构介绍 [^nvlink4] <sup id=fnref2:nvl32><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:nvl32>11</a></sup></td> </tr> <tr> <td style="text-align: left;">第4代</td> <td style="text-align: left;">Blackwell</td> <td style="text-align: left;">2025</td> <td style="text-align: left;">5.0</td> <td style="text-align: left;">4*</td> <td style="text-align: left;">200†</td> <td style="text-align: left;">200†</td> <td style="text-align: left;">18†</td> <td style="text-align: left;">3600†</td> <td style="text-align: left;">72†</td> <td style="text-align: left;">576*</td> <td style="text-align: left;">NVLink 5.0 进 Keynote 公布速率；NVL72 / 576 为 GTC 公布 + 对 NVLink Fabric 分层规模推测 [^nvlink5] <sup id=fnref4:gb200><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:gb200>2</a></sup></td> </tr> </tbody> </table> <p><small><br> 说明：</small></p> <ol> <li>“每 Link 双向带宽”遵循 NVIDIA 常用口径（聚合双向）。</li> <li>Lane 数由于官方文档通常未直接公布，表中以 * 标示；其值通过已知总带宽、单 Lane 速率与历代封装约束反向推断。如未来出现官方差异数据，应以官方为准。</li> <li>“最大 NVLink 域”指官方在对应代际提及的（或 Keynote 展示的）最大可支持互联规模；Blackwell 576 带 * 为推测（推断自 72×GB200 NVL72 单元 × 互联分层聚合）。</li> <li>Volta 时代 NVLink 2.0 在 NVSwitch 中实现的 Fan-out、Lane 数量公开资料缺少直接原始表格，使用多来源拆解/演讲交叉验证。<br> </li> </ol> <h4 id=_8><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_8>互联拓扑</a></h4> <p>互联拓扑定义了计算节点（或GPU）间的物理连接方式与逻辑结构，它直接决定了整个系统的通信性能，包括带宽、延迟、扩展性和成本。在超节点架构中，选择合适的拓扑是构建高效、无阻塞通信域的基石。当前主流技术路径为电子分组交换（Electronic Packet Switching，EPS），而光学电路交换（Optical Circuit Switching，OCS）作为前沿方向也备受关注。我们首先以最为常见的 <strong>Full Mesh拓扑</strong> 为例来分析GPU的互联拓扑，以及互联拓扑有哪些关键性质。</p> <p><strong>Full Mesh（全互联）</strong> ，或称全连接（All-to-All），是理论上最理想的通信拓扑结构。网络中的每一个节点都与其他所有节点建立直接的点对点连接。</p> <pre class=mermaid><code>block-beta
    columns 7
    space:2 N0 space N1 space:2
    space:7
    N7 space:5 N2
    space:7
    N6 space:5 N3
    space:7
    space:2 N5 space N4

    N0 --- N1
    N0 --- N2
    N0 --- N3
    N0 --- N4
    N0 --- N5
    N0 --- N6
    N0 --- N7
    N1 --- N2
    N1 --- N3
    N1 --- N4
    N1 --- N5
    N1 --- N6
    N1 --- N7
    N2 --- N3
    N2 --- N4
    N2 --- N5
    N2 --- N6
    N2 --- N7
    N3 --- N4
    N3 --- N5
    N3 --- N6
    N3 --- N7
    N4 --- N5
    N4 --- N6
    N4 --- N7
    N5 --- N6
    N5 --- N7
    N6 --- N7</code></pre> <p>为了能够定量比较不同的互联拓扑，需要引入一些关键指标：</p> <ul> <li><strong>网络直径（Network Diameter）</strong>：指网络中任意两个节点间的最短路径长度，刻画了最坏情况下的通信延迟。Full Mesh拓扑中任意两个节点直接连接，因此 <strong>网络直径为1</strong> 。在Fat-Tree拓扑中，任意两个节点间的通信最多只需经过“上行-下行”三跳（Leaf-Spine-Leaf），因此 <strong>网络直径为3</strong> 。</li> <li><strong>二分带宽（Bisection Bandwidth）</strong>：指当网络被切成两半时，连接这两半部分的总通信能力。它反映了网络在最坏通信场景下的最大数据传输能力。在Full Mesh拓扑中，一半网络有 <span class=arithmatex>\(N/2\)</span> 个节点，每个节点有 <span class=arithmatex>\(N/2\)</span> 条链路连接到另一半网络，因此Full Mesh拓扑的二分带宽为 <span class=arithmatex>\(\dfrac{N}{2} \times \dfrac{N}{2} \times B = \dfrac{N^2}{4} B\)</span> 。在Fat-Tree拓扑下，一半网络有 <span class=arithmatex>\(N/2\)</span> 个节点，每个节点都能与另一半网络全速率通信，因此Fat-Tree拓扑的二分带宽为 <span class=arithmatex>\(\dfrac{N}{2} B\)</span>。</li> <li><strong>径向扩展度（Radix Scalability）</strong>：指在给定交换芯片 radix 条件下的最大无阻塞集群规模 <span class=arithmatex>\(N_{max}\)</span> 。它衡量了在给定交换芯片端口数（R_switch）和终端设备接口数（R_dev）的条件下，一个拓扑理论上能构建的最大无阻塞集群规模。Full Mesh拓扑下，互联规模 <span class=arithmatex>\(N_{max} = R_{dev} + 1\)</span> ，Fat-Tree拓扑下 <span class=arithmatex>\(N_{max} = R_{switch}^2 / 2 / R_{dev}\)</span> 。</li> </ul> <p>接下来介绍构建Scale-Up域用的拓扑结构。</p> <p><strong>Fat-Tree（胖树）</strong> 拓扑是当前构建大规模、高性能计算和AI集群最主流的架构。它是一种分层的网络结构，其核心设计思想是：从网络边缘（终端节点）向核心（根方向）上行时，链路带宽逐级增加，确保任意两个节点间的通信都拥有充足的带宽资源，从而实现或逼近“无阻塞”通信。典型的胖树网络由“叶交换机”（Leaf Switches）和“脊交换机”（Spine Switches）组成。计算节点直接连接到叶交换机，每个叶交换机又与所有脊交换机相连。这种结构保证了任意两个不同叶交换机下的节点通信最多只需经过“上行-下行”三跳（Leaf-Spine-Leaf）。Fat-Tree拓扑具备如下特点：</p> <ul> <li><strong>高带宽与无阻塞</strong>：通过精心设计上行与下行链路的带宽比例（收敛比），胖树网络可以为All-to-All等复杂通信模式提供接近线速的聚合带宽。</li> <li><strong>良好的扩展性</strong>：可以通过增加Spine交换机或增加网络层级来平滑地扩展集群规模。</li> <li><strong>高容错性</strong>：任意一条链路或交换机故障，流量可以被重新路由到其他路径，不会导致单点失效。</li> </ul> <p>NVIDIA的DGX SuperPOD架构本质上就是一个精心设计的两级Fat-Tree网络。</p> <ul> <li><strong>第一级</strong>：在单个节点内部，多颗NVSwitch芯片构建了一个单级的、逻辑上完全无阻塞的全互联网络（Full Mesh），将域内所有GPU全互联起来。</li> <li><strong>第二级</strong>： 第一级的NVSwitch再通过第二级的NVSwitch进行互联，形成一个32卡的Scale-Up通信域。</li> </ul> <p><img alt="alt text" src=../../imgs/superpod/NVL32.png width=68%></p> <p><strong>Dragonfly拓扑</strong><sup id=fnref2:dragonfly><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:dragonfly>5</a></sup> 是一种为超大规模计算设计的、旨在降低网络直径和成本的拓扑结构。它将路由器（交换机）和与之相连的计算节点组织成“组”（Group）。组内，路由器之间实现全互联（All-to-All）。组间，通过长距离的“全局链路”进行稀疏连接。Dragonfly拓扑具备如下特点：</p> <ul> <li><strong>低网络直径</strong>：任意两个节点间的通信路径非常短，通常最多只需一跳组内路由和一跳全局路由。</li> <li><strong>成本效益</strong>：相比于同等规模的全连接胖树，Dragonfly所需的全局链路和交换机端口更少，成本更低。</li> </ul> <p><img alt="alt text" src=../../imgs/superpod/dragonfly.png width=68%></p> <p>Dragonfly的挑战在于，全局链路相对稀疏，就像一个城市的主干道有限。如果路由策略不佳，所有流量都涌向少数几条主干道，就会造成严重的拥堵。因此，它必须依赖智能的、能感知全局负载的路由算法，动态地为数据包规划‘行车路线’，才能发挥其低延迟和成本优势。</p> <p><strong>Torus拓扑</strong><sup id=fnref2:torus><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:torus>6</a></sup> 是一种规则的格状拓扑，在多维（如2D、3D、6D）网格的每个维度上都带有“环绕式”连接。每个节点都与其在各个维度上的“邻居”直接相连。Torus拓扑具备如下特点：</p> <ul> <li><strong>优异的局部性</strong>：非常适合具有邻近通信模式的科学计算应用（如气象模拟、流体力学），因为相邻节点间通信延迟极低。</li> <li><strong>二分带宽较低</strong>：将其切成两半时，横跨切面的链路数量相对较少，这意味着其全局All-to-All通信性能不如胖树。</li> <li><strong>扩展性受限</strong>：高维Torus布线复杂，扩展成本高。</li> </ul> <p><strong>SlimFly拓扑</strong> <sup id=fnref2:slimfly><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:slimfly>7</a></sup>：作为Dragonfly的演进，SlimFly是一种在给定交换机端口数下，能够以更少的网络直径和接近最优的二分带宽连接更多节点的拓扑结构。它在理论上被证明是构建超大规模网络最高效的拓扑之一，但其不规则的连接方式对物理布线和路由算法设计提出了极高挑战，目前更多处于学术研究和前沿探索阶段。</p> <p>对于以All-to-All和All-Reduce为主导通信模式的AI大模型训练而言，胖树拓扑因其优越的全局带宽特性与确定的网络直径而成为事实上的标准选择。但另一方面，Fat-Tree所能达到的互联规模也受限于交换机容量。在超节点继续演进的过程中，对互联拓扑的探索也尤为重要：</p> <table> <thead> <tr> <th>拓扑</th> <th>径向扩展度</th> <th>网络直径</th> <th>二分带宽</th> </tr> </thead> <tbody> <tr> <td>Full Mesh</td> <td><span class=arithmatex>\(R_{dev} + 1\)</span></td> <td>1</td> <td><span class=arithmatex>\(\dfrac{N^2}{4} B\)</span></td> </tr> <tr> <td>Fat-Tree</td> <td><span class=arithmatex>\(\dfrac{R_{switch}^2}{2 R_{dev}}\)</span></td> <td>3</td> <td><span class=arithmatex>\(\dfrac{N}{2} B\)</span></td> </tr> <tr> <td>Dragonfly</td> <td><span class=arithmatex>\(\dfrac{R_{switch}^4}{81 R_{dev}}\)</span></td> <td><span class=arithmatex>\(\le 3\)</span></td> <td><span class=arithmatex>\(\approx \dfrac{N}{2}B\)</span></td> </tr> <tr> <td>Torus</td> <td>无上界</td> <td><span class=arithmatex>\(D \cdot \dfrac{k}{2}\)</span></td> <td></td> </tr> <tr> <td>Slim Fly</td> <td><span class=arithmatex>\(\dfrac{32}{243} \times \dfrac{R_{switch}^3}{R_{dev}}\)</span></td> <td>2–3</td> <td>设计为接近满二分带宽</td> </tr> </tbody> </table> <h3 id=_9><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_9>软件系统</a></h3> <p>软件系统架构的根本决定因素是内存模型。传统操作系统内核围绕CPU访存权限进行设计，通过虚拟内存管理、页表机制和TLB等组件构建了现代计算的基础。同样，PCIe引入的MMIO（Memory Mapped I/O）机制决定了内核驱动的编程范式，设备寄存器被映射到统一的地址空间，通过内存访问指令进行控制。</p> <p>超节点的出现从根本上改变了这一范式。通过NVLink/NVSwitch等高速互联技术构建的HBD（High Bandwidth Domain）通信域，使得远程GPU显存能够以接近本地访问的性能被直接访问。为了实现高效通信，HBD域上正在构建一个能 <strong>绕过（Bypass）CPU和操作系统内核</strong> 的、由GPU主导的通信架构。在这个新架构下，访存的控制权发生了转移：</p> <ol> <li><strong>从CPU中心到GPU中心</strong>：跨GPU的内存访问不再需要CPU作为中介，而是由GPU的MMU（内存管理单元）直接发起，由硬件（如NVSwitch）进行路由。</li> <li><strong>从内核态仲裁到用户态直通</strong>：上层应用（如CUDA Kernel）通过统一虚拟地址（UVA）操作远程数据，其地址翻译和路由完全在硬件层面透明完成，内核仅在初始设置和资源管理时介入。</li> </ol> <p>为了更好地理解超节点带来的这一变革，接下来从GPU访存体系入手来分析软件系统的演进路径。</p> <h4 id=gpu><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#gpu>现代GPU访存体系</a></h4> <p>NVIDIA超节点在软件系统架构上与其 <strong>单个节点内部</strong> 的GPU访存体系一脉相承，均围绕 <strong>UVA（Unified Virtual Addressing）</strong> 技术构建。与传统CPU类似，现代GPU也配备了完整的内存管理单元（MMU），负责虚拟地址到物理地址的转换。UVA技术的引入，将GPU显存、CPU内存等不同物理内存统一映射到单一的虚拟地址空间中。上一章我们讨论了NVLink和NVSwitch等物理互联技术，它们构建了数据传输的高速公路。软件层面利用这条高速公路的核心便是统一虚拟寻址（UVA）与对等内存访问（Peer Memory Access）：</p> <ol> <li><strong>发现与连接建立</strong>：系统中的GPU通过物理总线（如NVLink或PCIe）互相发现，建立对等连接并分配唯一的Peer ID。</li> </ol> <pre class=mermaid><code>graph LR
    A[GPU-A] ---|NVLINK/PCIe| B[GPU-B]
    A --&gt; C[发现 Peer GPU]
    B --&gt; D[建立 Peer 连接]
    C --&gt; E[分配 Peer ID]
    D --&gt; E</code></pre> <ol start=2> <li><strong>Aperture选择与地址映射</strong> ：驱动程序分配UVA地址，并根据连接类型选择不同的Aperture通道：<br> - <strong>本地显存 (VID Memory)</strong> ：同一GPU内的内存访问<br> - <strong>对等内存 (Peer Memory)</strong> ：通过NVLink直接访问远程GPU显存<br> - <strong>系统内存 (SYS Memory)</strong> ：通过PCIe访问CPU主存<br> - <strong>Fabric内存</strong> ：在NVSwitch环境下的专用地址空间</li> <li><strong>硬件透明的远程访存</strong> ：当CUDA Kernel访问虚拟地址时，GPU MMU自动完成地址翻译和路由。硬件MMU通常提供4-5级页表，支持4K-128K页大小。考虑到MMU页表中的地址可能并非全部是内存地址，也包含部分IO地址。因此GPU MMU的表项也会标识是否支持缓存一致性。</li> </ol> <p>以下是GPU上访问UVA地址的流程：</p> <p><pre class=mermaid><code>sequenceDiagram
    participant App as 应用程序&lt;br&gt;(CUDA Kernel)
    participant LocalMMU as 本地GPU MMU
    participant Interconnect as NVLink/NVSwitch
    participant RemoteGPU as 远程GPU

    App-&gt;&gt;LocalMMU: 1. 访问虚拟地址 0x1000
    activate LocalMMU
    LocalMMU--&gt;&gt;LocalMMU: 2. 查页表, 发现PTE指向Peer
    LocalMMU-&gt;&gt;Interconnect: 3. 发起硬件路由请求
    deactivate LocalMMU

    activate Interconnect
    Interconnect-&gt;&gt;RemoteGPU: 4. 转发内存访问请求
    deactivate Interconnect

    activate RemoteGPU
    RemoteGPU--&gt;&gt;RemoteGPU: 5. 访问本地物理显存
    Note right of RemoteGPU: 数据被读取/写入
    RemoteGPU--&gt;&gt;App: 6. (经由硬件)返回结果
    deactivate RemoteGPU</code></pre><br> UVA在节点内实现的编程透明性与硬件高效性，为构建更大规模、跨节点的统一地址空间奠定了范式基础。随着硬件演进，软件侧的地址模型与编程范式也在逐步向CPU成熟的体系靠拢。总体趋势是：CPU与OS内核正从“关键数据路径”中解放出来，转而扮演“控制平面”的角色，通过配置虚拟地址与MMU来管理访存与通信，而非直接参与每一次操作。</p> <h4 id=_10><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_10>超节点访存体系</a></h4> <p>传统的UVA和PCIe P2P机制的边界仅限于单个PCIe根联合体（Root Complex），无法原生支持跨物理服务器节点的直接访存。以下是PCIe总线上节点内部的访存体系：</p> <p><img alt=mmio src=../../imgs/superpod/MMIO.drawio.svg></p> <ol> <li>主存、设备的控制寄存器和设备内置存储（比如显存）都会通过PCIE RC映射到一个统一的MMIO地址空间（Memory Mapped I/O）；</li> <li>在内核态，设备驱动通过MMIO地址操作设备寄存器，从而实现对设备的初始化、控制与中断处理；</li> <li>在用户态，可以直接将设备的存储映射到用户态地址空间，从而实现用户态对设备的直接读写，让数据路径by pass内核。GPU Direct RDMA技术即将部分显存映射到用户态，再交给RDMA网卡去访问。RDMA网卡的doorbell寄存器（用于通知网卡有工作需要处理）也可以通过这种映射交给GPU去访问，从而实现IBGDA（GPU异步RDMA数据发送）；</li> </ol> <p>在传统的基于PC的AI算力服务器上，上述软件技术架构已成为事实标准。然而，在超节点中，上述访存体系面临本质缺陷：PCIe通信域无法纳管其他节点的设备，因此也无法提供跨节点的统一访存地址空间。</p> <p>超节点通过NVSwitch Fabric等技术，将“节点内”的P2P模型扩展至整个机柜乃至多个机柜。其关键在于引入了一个由 <strong>Fabric Manager</strong> 管理的<sup id=fnref2:fabricmanager><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:fabricmanager>8</a></sup> <strong>47-bit的 全局物理地址空间</strong> ：</p> <ol> <li><strong>全局地址分配</strong>：Fabric Manager为HBD域内的每个GPU分配一个唯一的、在全局范围内无冲突的物理地址（PA）范围。</li> <li><strong>VA到全局PA的映射</strong>：当一个GPU需要访问远程GPU时，其驱动程序不再映射到对端的PCIe BAR地址，而是将用户态的虚拟地址（VA）通过页表（PTE）映射到这个全局物理地址（PA）。</li> <li><strong>硬件路由</strong>：当GPU的MMU翻译VA并得到这个全局PA时，它会生成一个带有目标GPU ID的硬件请求。该请求被发送到NVSwitch网络，由交换芯片根据地址和ID，像路由器转发IP包一样，精准地将读写操作路由到目标GPU的物理显存上。</li> </ol> <h4 id=_11><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_11>软件编程模型</a></h4> <p>超节点的硬件发展催生了软件编程模型的演进。当前主流的SPMD范式正面临挑战，而以PGAS为代表的新范式则为解决这些挑战提供了清晰的路径。</p> <h5 id=spmd-bsp><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#spmd-bsp>主流范式及其瓶颈：SPMD 与 BSP</a></h5> <p>当前几乎所有大规模AI训练都构建于 <strong>SPMD (Single Program, Multiple Data)</strong> 模型之上。所有GPU运行相同程序，处理不同数据分片，并通过<code>NCCL</code>等库进行高效的集合通信。</p> <p>其成功源于其遵循了 <strong>BSP (Bulk Synchronous Parallel, 块同步并行)</strong> 的计算范式。BSP将并行计算清晰地分为三个阶段：1) 本地计算，2) 全局通信，3) <strong>同步屏障 (Barrier)</strong>。所有GPU必须在屏障处等待，直到最慢的那个GPU完成后，才能一起进入下一阶段。</p> <p><img alt="alt text" src=../../imgs/superpod/bsp.png></p> <p>这种严格的同步正是其最大的弱点。在大规模集群中，“慢节点” (Straggler) 问题几乎无法避免，而BSP模型会放大其负面影响。一个节点的延迟会拖慢整个集群。慢节点产生的原因多种多样：</p> <ul> <li><strong>硬件层面</strong>：微小的时钟频率差异、功耗墙限制、网络瞬时拥塞。</li> <li><strong>软件层面</strong>：操作系统后台进程的干扰 (Jitter)。</li> <li><strong>数据层面</strong>：这在现代AI负载中愈发关键。例如，在 <strong>MoE模型</strong> 中，不同的数据（Token）会被路由到不同的专家，导致专家负载不均；在 <strong>强化学习</strong> 或 <strong>长序列推理</strong> 中，输出序列长度不定，导致各GPU计算时间不同。</li> </ul> <p>在这些场景下，BSP的同步屏障会导致大量GPU处于空闲等待状态，极大地降低了计算效率。正是为了解决这一根本性矛盾，业界开始转向更为灵活的编程模型。</p> <h5 id=pgas><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#pgas>PGAS 与混合编程</a></h5> <p><strong>PGAS (Partitioned Global Address Space)</strong> 是解决上述瓶颈的未来方向之一。它通过 <strong>单边通信 (One-Sided Communication)</strong> ，从根本上改变了数据交换的模式。</p> <table> <thead> <tr> <th style="text-align: left;">特性</th> <th style="text-align: left;">SPMD + 集合通信 (NCCL)</th> <th style="text-align: left;">PGAS + 单边通信 (NVSHMEM)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left;"><strong>通信模型</strong></td> <td style="text-align: left;"><strong>双边 (Two-Sided)</strong>：<code>Send</code>与<code>Recv</code>需配对，依赖集体同步。</td> <td style="text-align: left;"><strong>单边 (One-Sided)</strong>：<code>Put</code>/<code>Get</code>无需对方协同，可异步执行。</td> </tr> <tr> <td style="text-align: left;"><strong>编程抽象</strong></td> <td style="text-align: left;"><strong>通信是集体操作</strong>：<code>AllReduce(data)</code></td> <td style="text-align: left;"><strong>通信如内存访问</strong>：<code>put(&amp;dest, &amp;src, size)</code></td> </tr> <tr> <td style="text-align: left;"><strong>适用负载</strong></td> <td style="text-align: left;"><strong>静态、均匀</strong> (如稠密模型训练)</td> <td style="text-align: left;"><strong>动态、非均匀</strong> (如MoE, 推理)</td> </tr> </tbody> </table> <p>在PGAS模型中，整个分布式系统由众多PE（Processing Elements）组成。为了实现SHMEM语义，对每个PE的地址空间进行如下划分<sup id=fnref2:openshmem><a class=footnote-ref href=../../2025/10/01/superpod-ai-architecture/#fn:openshmem>9</a></sup>：</p> <p><img alt="alt text" src=../../imgs/superpod/pgas.png width=68%></p> <ol> <li>每个PE都有自己独立的地址空间，这能够映射到不同节点的不同进程中去；</li> <li>进程的内存地址空间分为两部分：</li> <li>Private地址空间，这部分为进程私有，可以被进程随意使用；</li> <li>Symmetric地址空间，这部分为远程可访问的地址空间，所有进程保持一致；</li> </ol> <p>进程可以通过PE的rank与Symmetric Address来读写其他PE的内存，从而实现单边通信。这种单边通信能较好解决“慢节点”问题：它将 <strong>同步的“通信握手”</strong> 变成了 <strong>异步的“内存操作”</strong> 。在双边通信中，一个“快节点”的<code>Send</code>操作必须等待“慢节点”准备好<code>Recv</code>，从而被强制阻塞。而在单边通信中，“快节点”只需知道目标地址，就可以通过<code>Put</code>操作直接将数据写入“慢节点”的内存，然后立即返回执行后续任务。数据写入后，“慢节点”何时去处理它，并不会反过来阻塞“快节点”的进度。这种异步、解耦的机制打破了BSP模型的全局同步枷锁，从而极大地提升了系统在非均匀负载下的计算效率。</p> <h3 id=_12><a class=toclink href=../../2025/10/01/superpod-ai-architecture/#_12>参考文献</a></h3> <div class=footnote> <hr> <ol> <li id=fn:gh200> <p><a href=https://resources.nvidia.com/en-us-data-center-overview-mc/en-us-data-center-overview/grace-hopper-superchip-datasheet-partner>NVIDIA GH200 Grace Hopper<br> Superchip</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:gh200 title="Jump back to footnote 1 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:gh200 title="Jump back to footnote 1 in the text">↩</a></p> </li> <li id=fn:gb200> <p><a href=https://resources.nvidia.com/en-us-dgx-systems/dgx-superpod-gb200-datasheet>NVIDIA DGX GB200</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:gb200 title="Jump back to footnote 2 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:gb200 title="Jump back to footnote 2 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref3:gb200 title="Jump back to footnote 2 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref4:gb200 title="Jump back to footnote 2 in the text">↩</a></p> </li> <li id=fn:huangs_law> <p><a href=https://spectrum.ieee.org/nvidia-gpu>Huang’s Law</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:huangs_law title="Jump back to footnote 3 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:huangs_law title="Jump back to footnote 3 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref3:huangs_law title="Jump back to footnote 3 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref4:huangs_law title="Jump back to footnote 3 in the text">↩</a></p> </li> <li id=fn:nvhbi> <p><a href=https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/ >Inside NVIDIA Blackwell Ultra: The Chip Powering the AI Factory Era</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:nvhbi title="Jump back to footnote 4 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:nvhbi title="Jump back to footnote 4 in the text">↩</a></p> </li> <li id=fn:dragonfly> <p><a href=https://ieeexplore.ieee.org/abstract/document/4556717>Technology-Driven, Highly-Scalable Dragonfly Topology</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:dragonfly title="Jump back to footnote 5 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:dragonfly title="Jump back to footnote 5 in the text">↩</a></p> </li> <li id=fn:torus> <p><a href=http://datasys.cs.iit.edu/reports/2014_GCASR14_paper-torus.pdf>Understanding Torus Network Performance through Simulations</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:torus title="Jump back to footnote 6 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:torus title="Jump back to footnote 6 in the text">↩</a></p> </li> <li id=fn:slimfly> <p><a href=https://arxiv.org/pdf/1912.08968>Slim Fly: A Cost Effective Low-Diameter Network Topology</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:slimfly title="Jump back to footnote 7 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:slimfly title="Jump back to footnote 7 in the text">↩</a></p> </li> <li id=fn:fabricmanager> <p><a href=https://github.com/NVIDIA/open-gpu-kernel-modules/blob/2b436058a616676ec888ef3814d1db6b2220f2eb/kernel-open/nvidia-uvm/uvm_gpu.h#L1292>Nvidia Fabric Manager</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:fabricmanager title="Jump back to footnote 8 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:fabricmanager title="Jump back to footnote 8 in the text">↩</a></p> </li> <li id=fn:openshmem> <p><a href=http://openshmem.org/site/sites/default/site_files/OpenSHMEM-1.5.pdf>OpenSHMEM</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:openshmem title="Jump back to footnote 9 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:openshmem title="Jump back to footnote 9 in the text">↩</a></p> </li> <li id=fn:hgx_a100> <p><a href=https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/HGX/a100-80gb-hgx-a100-datasheet-us-nvidia-1485640-r6-web.pdf>NVIDIA HGX A100</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:hgx_a100 title="Jump back to footnote 10 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:hgx_a100 title="Jump back to footnote 10 in the text">↩</a></p> </li> <li id=fn:nvl32> <p><a href="https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper?ncid=no-ncid">NVIDIA GH200 Grace Hopper Superchip Architecture</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:nvl32 title="Jump back to footnote 11 in the text">↩</a><a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref2:nvl32 title="Jump back to footnote 11 in the text">↩</a></p> </li> <li id=fn:nvshmem> <p><a href=https://static.rainfocus.com/nvidia/gtcs24/sess/1693876934119001DqDe/FinalPresPDF/S61368_1710778532525001A5Z4.pdf>Magnum IO GPUDirect, NCCL, NVSHMEM, and GDA-KI on Grace Hopper and Hopper systems</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:nvshmem title="Jump back to footnote 12 in the text">↩</a></p> </li> <li id=fn:pathways> <p><a href=https://arxiv.org/abs/2203.12533>Pathways: Asynchronous Distributed Dataflow for ML</a> <a class=footnote-backref href=../../2025/10/01/superpod-ai-architecture/#fnref:pathways title="Jump back to footnote 13 in the text">↩</a></p> </li> </ol> </div> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-04-28 00:00:00+00:00">2025年4月28日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/llm/ class=md-meta__link>LLM</a>, <a href=../../category/training/ class=md-meta__link>Training</a></li> <li class=md-meta__item> 需要 4 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=probingprofiling><a href=../../2025/04/28/dist_probe_3/ class=toclink>Probing分布式探针开发随笔（三）：分布式训练的Profiling</a></h2> <p>在前两篇系列文章中，介绍了 Probing 分布式探针的核心理念与技术探索，包括其应对 ABI 兼容性挑战的动态注入机制，以及基于 DataFusion 构建的可扩展查询引擎。虽然仍处于技术原型阶段，但也确实看到了实现一个“完美”工具的可能性。最近在解决某千卡训练项目时，浪费了大量时间在实验、抓数据与复现等工作上，越来越感觉到传统工具的限制，也越来越急迫地需要将Probing推向生产。</p> <h3 id=profiler><a class=toclink href=../../2025/04/28/dist_probe_3/#profiler>传统Profiler的困境</a></h3> <p>Profiling是性能优化工程师最为主要的优化手段，为了分析性能我们有形形色色的Profiler工具。大到Intel VTune和Nvidia Insight这种系列工具，有着完备的分析工具与可视化手段，很多问题都能一目了然；小到<code>perf top</code>这样”简陋”的调用栈采样工具，得边看边猜整个系统的行为。但是这些工具有一个共同的问题：他们都是单机工具，并不能很好的解决分布式系统中的性能问题。比如，PyTorch提供了<code>torch.profiler</code>，一个强大的内置性能分析工具，使用也极为方便：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a href=#__codelineno-8-1 id=__codelineno-8-1 name=__codelineno-8-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.profiler</span><span class=w> </span><span class=kn>import</span> <span class=n>profile</span><span class=p>,</span> <span class=n>CPU</span><span class=p>,</span> <span class=n>CUDA</span>
</span><span id=__span-8-2><a href=#__codelineno-8-2 id=__codelineno-8-2 name=__codelineno-8-2></a>
</span><span id=__span-8-3><a href=#__codelineno-8-3 id=__codelineno-8-3 name=__codelineno-8-3></a><span class=k>with</span> <span class=n>profile</span><span class=p>(</span><span class=n>activities</span><span class=o>=</span><span class=p>[</span><span class=n>CPU</span><span class=p>,</span> <span class=n>CUDA</span><span class=p>])</span> <span class=k>as</span> <span class=n>prof</span><span class=p>:</span>
</span><span id=__span-8-4><a href=#__codelineno-8-4 id=__codelineno-8-4 name=__codelineno-8-4></a>    <span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>steps</span><span class=p>):</span>
</span><span id=__span-8-5><a href=#__codelineno-8-5 id=__codelineno-8-5 name=__codelineno-8-5></a>        <span class=n>train_one_step</span><span class=p>()</span>
</span><span id=__span-8-6><a href=#__codelineno-8-6 id=__codelineno-8-6 name=__codelineno-8-6></a>        <span class=n>prof</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></code></pre></div> <p><code>torch.profiler</code>能够抓取PyTorch中的算子执行与显存分配行为，并且可以通过Tensorboard对结果进行可视化。但对于千卡规模的分布式训练，<code>torch.profiler</code>还远远不够用：</p> <ol> <li>性能开销问题：profiler会显著影响性能，导致Profiling结果不准确；</li> <li>数据爆炸：单卡长生上G的数据，千卡需要上T存储；</li> <li>缺乏协调：各节点数据相互独立，难以进行关联分析，特别是引入模型并行之后；</li> </ol> <h3 id=timeline><a class=toclink href=../../2025/04/28/dist_probe_3/#timeline>思路转变：从Timeline到统计方法</a></h3> <h4 id=timeline_1><a class=toclink href=../../2025/04/28/dist_probe_3/#timeline_1>Timeline困境</a></h4> <p>单节点的性能分析中，Timeline技术备受追捧。原因无他：直观，每个阶段的执行，开销的资源与消耗的时间都可以在一个时间轴上精确展示出来。但是Timeline数据庞大，并且借助浏览器渲染时速度也欠佳。几个G的Timeline数据很快会让你的笔记本成为一个小火炉。Timeline也存在明显的局限性：</p> <ul> <li>一般只能分析单个节点单个Step：</li> <li>多节点多Step的数据，看不过来（虽然有些人在此会很倔强）；</li> <li>每个Step都有差异，导致难以给出结论（可以给定性结论，但结论复现存在难度）；</li> <li>难以捕捉这个系统的随机性与不确定性：</li> <li>单个节点单个Step是确定的，但是一千个节点的同一个Step，充满了随机性；</li> <li>Timeline无法刻画出整个系统性能层面的统计特性，比如耗时的99线；</li> <li>忽略了负载不均衡现象：</li> <li>在经典的Dense LLM中，因为模型并行会导致每个节点实际负载各不相同；</li> <li>在流行的MoE LLM中，专家路由也会导致计算负载不均衡问题；</li> </ul> <p>上述这些问题难以在Timeline框架下靠修修补补来解决：</p> <ol> <li> <p>在由上千节点与数千线缆构成的复杂计算集群中，节点与节点间互联必然存在随机性与不确定性，这正是分布式系统的核心挑战。因此，分布式系统的性能分析需要从单节点上精准timeline的个体样本方法**转向能够描述随机性与整体特性的统计方法**。</p> </li> <li> <p>单节点的Profiling数据量非常巨大，却又缺乏有效的数据压缩与处理手段。单机尚能撑住，扩展到千卡集群就直接原地爆炸。分布式Profiling必然需要**转向现代化的数据基建，全面拥抱分布式的数据存储与分析技术**。</p> </li> <li> <p>分布式系统中很难保证时间一致与时间精度，多机timeline很难进行对齐，也很难可视化分析（可以想象下一千张卡的timelime等你去看）。<strong>如何在不依赖精准时间戳的情况下进行数据关联分析、识别性能异常节点(Stragglers)，也成为分布式系统性能分析的关键挑战</strong>。</p> </li> </ol> <h5 id=_1><a class=toclink href=../../2025/04/28/dist_probe_3/#_1>分布式系统的统计思想</a></h5> <p>分布式系统最常见的性能分析范式是分布式Tracing，如OpenTelemetry这类系统已在微服务领域取得了成功，这些系统的核心理念可以适配到分布式训练环境：</p> <ol> <li> <p><strong>借鉴Span概念</strong>：将训练过程分解自顶向下的、嵌套的span。前向传播、反向传播作为顶级Span，每个layer的计算作为子Span。这种层次化的视图只需要明确层次关系，而无须精确的时间戳对齐。</p> </li> <li> <p><strong>优化采样策略</strong>：不同于timeline的全量采样，分布式profiling可以通过设计采样策略来控制开销：<br> - 结构化采样：根据模型结构进行采样而非完全随机采样；<br> - 分布式采样：将采样操作分布到不同的节点，降低每个节点的采样量；</p> </li> <li> <p><strong>分析效率</strong>：模型训练中每个span内的计算量与通信量可以精确计算，结合span计时即可分析每一段时间的硬件利用率与瓶颈，而无须像timeline那样精需要精准的时间信息。</p> </li> <li> <p><strong>统计视角替代精确时间线</strong>：关注分布特性（均值、中位数、百分位数）而非单个精确时间点，使问题分析更符合分布式系统的随机性特质。</p> </li> </ol> <p>不过分布式训练的通信模式是集合通信而非调用树，可以尝试为训练系统单独设计一套分布式Profiling方案。</p> <h3 id=profiling><a class=toclink href=../../2025/04/28/dist_probe_3/#profiling>基于探针的分布式Profiling</a></h3> <p>训练系统分布式Profiling需要克服的主要困难有两个：</p> <ol> <li>没有配套的数据系统：训练过程中的数据大多数没有业务价值，不会配套专门的数据处理与存储系统；</li> <li>数据量庞大：每个GPU在一个训练Step内就会产生数万个事件，而总数据量会随着step树与节点数增长而快速爆炸；</li> </ol> <p>Probing 的解决方案是：<strong>本地化存储数据 + 分布式查询分析</strong>，将数据存储和分析的压力分散到每个节点上。以下是一个简单的示意图，用于说明理想情况下probing如何工作：</p> <pre class=mermaid><code>---
title: Probing 分布式 Profiling 架构
---
graph TD
    subgraph "控制平面 (用户)"
        UI[Web UI]
        CLI[命令行]
        API[SQL查询+HTTP协议]
        UI &amp; CLI --&gt; API
    end

    subgraph "分布式训练集群"
        direction LR
        subgraph "Node 1 (Rank 0)"
            P1[训练进程 Rank 0]
            PR1[Probe]
            H1[采集Hooks e.g., PyTorch]
            P1 --&gt; H1 -- 本地数据 --&gt; PR1
        end
        subgraph "Node 2 (Rank 1)"
            P2[训练进程 Rank 1]
            PR2[Probe]
            H2[采集Hooks e.g., PyTorch]
            P2 --&gt; H2 -- 本地数据 --&gt; PR2
        end
        subgraph "Node N (Rank N-1)"
            PN[训练进程 Rank N-1]
            PRN[Probe]
            HN[采集Hooks e.g., PyTorch]
            PN --&gt; HN -- 本地数据 --&gt; PRN
        end
    end

    API -- SQL查询 --&gt; PR1;
    PR1 -- 分布式查询协调 --&gt; PR2;
    PR1 -- 分布式查询协调 --&gt; PRN;
    PR2 -- 本地查询/聚合 --&gt; PR2;
    PRN -- 本地查询/聚合 --&gt; PRN;
    PR2 -- 部分结果 --&gt; PR1;
    PRN -- 部分结果 --&gt; PR1;
    PR1 -- 最终聚合 --&gt; API;

    style P1 fill:#f9f,stroke:#333
    style P2 fill:#f9f,stroke:#333
    style PN fill:#f9f,stroke:#333
    style PR1 fill:#bfb,stroke:#333
    style PR2 fill:#bfb,stroke:#333
    style PRN fill:#bfb,stroke:#333
    style H1 fill:#ccf,stroke:#333
    style H2 fill:#ccf,stroke:#333
    style HN fill:#ccf,stroke:#333</code></pre> <p>在这个架构下，可以借助分布式查询系统，将过滤、采样与聚合操作下推到每个节点去执行，并结合良好设计的采样机制与策略来平衡性能分析的精度与开销。接下来是在这个架构下设计数据采集、存储和分析的链路</p> <h4 id=_2><a class=toclink href=../../2025/04/28/dist_probe_3/#_2>采集链路</a></h4> <h5 id=_3><a class=toclink href=../../2025/04/28/dist_probe_3/#_3>基于钩子的数据采集</a></h5> <p>虽然修改代码加日志是最直观的数据采集手段，也日志往往过于随意、缺乏设计，为后续的分析与使用带来困难。不修改代码采集数据就需要对代码进行自动插桩。好在PyTorch提供了钩子（Hooks）机制，能够”不侵入”代码的情况下完成插桩。</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a href=#__codelineno-9-1 id=__codelineno-9-1 name=__codelineno-9-1></a><span class=kn>from</span><span class=w> </span><span class=nn>torch.optim.optimizer</span><span class=w> </span><span class=kn>import</span> <span class=n>register_optimizer_step_post_hook</span>
</span><span id=__span-9-2><a href=#__codelineno-9-2 id=__codelineno-9-2 name=__codelineno-9-2></a>
</span><span id=__span-9-3><a href=#__codelineno-9-3 id=__codelineno-9-3 name=__codelineno-9-3></a><span class=n>register_optimizer_step_post_hook</span><span class=p>(</span><span class=n>optimizer_step_post_hook</span><span class=p>)</span>
</span></code></pre></div> <p><code>register_optimizer_step_post_hook</code> 帮我们向torch注册一个钩子函数，在每个Optimzier完成<code>step()</code>调用后执行。这个插桩时机极为关键：</p> <ol> <li>模型已完成构建，可获取完整模型定义</li> <li>前向传播、反向传播与优化器都已完成预热</li> </ol> <p>接下来，借助Python的垃圾回收(GC)机制与反射能力来捕获进程中的模型结构：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a href=#__codelineno-10-1 id=__codelineno-10-1 name=__codelineno-10-1></a><span class=k>def</span><span class=w> </span><span class=nf>get_toplevel_module</span><span class=p>():</span>
</span><span id=__span-10-2><a href=#__codelineno-10-2 id=__codelineno-10-2 name=__codelineno-10-2></a>    <span class=kn>import</span><span class=w> </span><span class=nn>gc</span>
</span><span id=__span-10-3><a href=#__codelineno-10-3 id=__codelineno-10-3 name=__codelineno-10-3></a>
</span><span id=__span-10-4><a href=#__codelineno-10-4 id=__codelineno-10-4 name=__codelineno-10-4></a>    <span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-10-5><a href=#__codelineno-10-5 id=__codelineno-10-5 name=__codelineno-10-5></a>
</span><span id=__span-10-6><a href=#__codelineno-10-6 id=__codelineno-10-6 name=__codelineno-10-6></a>    <span class=n>objs</span> <span class=o>=</span> <span class=p>[</span><span class=n>obj</span> <span class=k>for</span> <span class=n>obj</span> <span class=ow>in</span> <span class=n>gc</span><span class=o>.</span><span class=n>get_objects</span><span class=p>()</span> <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>obj</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>)]</span>
</span><span id=__span-10-7><a href=#__codelineno-10-7 id=__codelineno-10-7 name=__codelineno-10-7></a>    <span class=n>is_child</span> <span class=o>=</span> <span class=nb>set</span><span class=p>()</span>
</span><span id=__span-10-8><a href=#__codelineno-10-8 id=__codelineno-10-8 name=__codelineno-10-8></a>    <span class=k>for</span> <span class=n>obj</span> <span class=ow>in</span> <span class=n>objs</span><span class=p>:</span>
</span><span id=__span-10-9><a href=#__codelineno-10-9 id=__codelineno-10-9 name=__codelineno-10-9></a>        <span class=k>for</span> <span class=n>child</span> <span class=ow>in</span> <span class=n>obj</span><span class=o>.</span><span class=n>children</span><span class=p>():</span>
</span><span id=__span-10-10><a href=#__codelineno-10-10 id=__codelineno-10-10 name=__codelineno-10-10></a>            <span class=n>is_child</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=nb>id</span><span class=p>(</span><span class=n>child</span><span class=p>))</span>
</span><span id=__span-10-11><a href=#__codelineno-10-11 id=__codelineno-10-11 name=__codelineno-10-11></a>    <span class=k>return</span> <span class=p>[</span><span class=n>obj</span> <span class=k>for</span> <span class=n>obj</span> <span class=ow>in</span> <span class=n>objs</span> <span class=k>if</span> <span class=nb>id</span><span class=p>(</span><span class=n>obj</span><span class=p>)</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>is_child</span><span class=p>]</span>
</span></code></pre></div> <p>通过<code>gc</code>模块我们可以获得当前进程中的全部Python对象列表，再通过反射调用<code>isinstance(obj, torch.nn.Module)</code>找出全部<code>torch.nn.Module</code>对象。最后再根据module之间的父子关系来发现顶层Module。</p> <p>获取顶层Module后，我们可以注册完整的前向/反向传播钩子链，完成接下来的插桩：</p> <ol> <li>Module.register_forward_pre_hook - 前向传播开始前</li> <li>Module.register_forward_hook - 前向传播完成后</li> <li>Module.register_full_backward_pre_hook - 反向传播开始前</li> <li>Module.register_full_backward_hook - 反向传播完成后</li> <li>Optimizer.register_step_pre_hook - 优化器步骤开始前</li> <li>Optimizer.register_step_post_hook - 优化器步骤完成后</li> </ol> <p>这些钩子构成了训练过程中的完整监控链，允许我们精确测量模型各组件的执行性能。</p> <h5 id=_4><a class=toclink href=../../2025/04/28/dist_probe_3/#_4>结构化采样</a></h5> <p>考虑到PyTorch模型包含大量嵌套子模块，对每个模块都执行计时操作会带来显著性能开销。随机采样虽然能够降低插桩的开销，但需要等待较长时间才能保证采样充分。这里我们引入一种结构化采样方法来加速性能数据的采集：</p> <ol> <li>span分解：将模型执行分解为一系列span，每个module的前向和反向传播分别构成独立span</li> <li>层次化排序：按照嵌套关系对span进行排序<ul> <li>粗粒度span（如整个模型的前向传播）排序靠前</li> <li>细粒度span（如单个卷积层的操作）排序靠后</li> </ul> </li> <li>自适应采样：从粗到细逐步采样<ul> <li>命中采样时，记录当前span计时，并移至下一个span</li> <li>未命中采样时，跳过计算以减少开销</li> </ul> </li> </ol> <p>这种结构化采样确保每个训练步骤只对一个特定粒度的span进行采样，使模型性能分析由粗到细逐步进行，在控制开销的同时提供全面性能视图。</p> <h4 id=cuda-event><a class=toclink href=../../2025/04/28/dist_probe_3/#cuda-event>基于CUDA Event的精确计时</a></h4> <p>GPU上异步执行的计时通常通过CUDA Event来实现。CUDA Event能保证在CUDA Stream上的执行顺序，并且是测量GPU操作时间的最准确方式。一个CUDA Event的生命周期包括以下几个阶段：</p> <ol> <li>创建(Create)：通过torch.cuda.Event()或CUDA原生API创建Event对象</li> <li>记录(Record)：通过event.record()将Event标记到特定CUDA Stream的当前位置</li> <li>同步(Synchronize)：通过event.synchronize()等待Event标记的操作完成</li> <li>查询(Query)：通过event.query()非阻塞地检查Event是否完成</li> <li>计时(Elapsed Time)：通过start_event.elapsed_time(end_event)计算两个Event之间的时间差</li> </ol> <p>在实际应用中，同步(Synchronize)操作会导致GPU等待并强制Stream清空，可能显著影响性能。为解决这一问题，我们采用延迟计时(Delayed Timing)策略，将时间读取推迟到优化器执行完成后进行。这种方法有效降低了计时操作对训练性能的干扰，特别适合分布式训练环境。</p> <h3 id=_5><a class=toclink href=../../2025/04/28/dist_probe_3/#_5>基于统计的性能/故障分析方法</a></h3> <p>在大规模分布式训练环境中，我们面临的不仅是如何采集数据，更重要的是如何有效利用这些数据发现并解决问题。Probing采用统计分析方法，将分散在各节点的性能数据转化为可操作的洞察。</p> <h4 id=_6><a class=toclink href=../../2025/04/28/dist_probe_3/#_6>分布式训练中的常见性能问题</a></h4> <p>在实践中，分布式训练的性能问题通常表现为以下几种典型模式：</p> <ol> <li>慢节点(Straggler)问题：个别节点显著慢于集群平均水平，拖慢整体训练进度</li> <li>负载不均衡：计算或内存负载在节点间分布不均，导致资源利用率低下</li> <li>通信瓶颈：节点间数据交换速度不足，制约训练效率提升</li> <li>异常波动：性能指标在时间维度上出现突发性异常</li> <li>集群分层：性能根据硬件配置或网络拓扑自然分层，形成性能梯队利用统计数据定位问题</li> </ol> <h4 id=_7><a class=toclink href=../../2025/04/28/dist_probe_3/#_7>节点性能差异分析</a></h4> <p>通过简单SQL查询，我们可以快速识别集群中的异常节点：</p> <div class="language-SQL highlight"><pre><span></span><code><span id=__span-11-1><a href=#__codelineno-11-1 id=__codelineno-11-1 name=__codelineno-11-1></a><span class=c1>-- 查找前向传播耗时异常的节点</span>
</span><span id=__span-11-2><a href=#__codelineno-11-2 id=__codelineno-11-2 name=__codelineno-11-2></a><span class=k>SELECT</span><span class=w> </span>
</span><span id=__span-11-3><a href=#__codelineno-11-3 id=__codelineno-11-3 name=__codelineno-11-3></a><span class=w>    </span><span class=n>rank</span><span class=p>,</span><span class=w> </span>
</span><span id=__span-11-4><a href=#__codelineno-11-4 id=__codelineno-11-4 name=__codelineno-11-4></a><span class=w>    </span><span class=k>AVG</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>avg_forward_time</span><span class=p>,</span>
</span><span id=__span-11-5><a href=#__codelineno-11-5 id=__codelineno-11-5 name=__codelineno-11-5></a><span class=w>    </span><span class=k>COUNT</span><span class=p>(</span><span class=o>*</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>sample_count</span><span class=p>,</span>
</span><span id=__span-11-6><a href=#__codelineno-11-6 id=__codelineno-11-6 name=__codelineno-11-6></a><span class=w>    </span><span class=p>(</span><span class=k>AVG</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=o>-</span><span class=w> </span>
</span><span id=__span-11-7><a href=#__codelineno-11-7 id=__codelineno-11-7 name=__codelineno-11-7></a><span class=w>     </span><span class=p>(</span><span class=k>SELECT</span><span class=w> </span><span class=k>AVG</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>torch_traces</span><span class=w> </span><span class=k>WHERE</span><span class=w> </span><span class=k>operation</span><span class=o>=</span><span class=s1>'forward'</span><span class=p>))</span><span class=w> </span>
</span><span id=__span-11-8><a href=#__codelineno-11-8 id=__codelineno-11-8 name=__codelineno-11-8></a><span class=w>     </span><span class=o>/</span><span class=w> </span><span class=p>(</span><span class=k>SELECT</span><span class=w> </span><span class=n>STDDEV</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>torch_traces</span><span class=w> </span><span class=k>WHERE</span><span class=w> </span><span class=k>operation</span><span class=o>=</span><span class=s1>'forward'</span><span class=p>)</span><span class=w> </span>
</span><span id=__span-11-9><a href=#__codelineno-11-9 id=__codelineno-11-9 name=__codelineno-11-9></a><span class=w>     </span><span class=k>as</span><span class=w> </span><span class=n>z_score</span>
</span><span id=__span-11-10><a href=#__codelineno-11-10 id=__codelineno-11-10 name=__codelineno-11-10></a><span class=k>FROM</span><span class=w> </span><span class=n>python</span><span class=p>.</span><span class=n>torch_traces</span>
</span><span id=__span-11-11><a href=#__codelineno-11-11 id=__codelineno-11-11 name=__codelineno-11-11></a><span class=k>WHERE</span><span class=w> </span><span class=k>operation</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>'forward'</span><span class=w> </span><span class=k>AND</span><span class=w> </span><span class=n>step_id</span><span class=w> </span><span class=k>BETWEEN</span><span class=w> </span><span class=mi>100</span><span class=w> </span><span class=k>AND</span><span class=w> </span><span class=mi>200</span>
</span><span id=__span-11-12><a href=#__codelineno-11-12 id=__codelineno-11-12 name=__codelineno-11-12></a><span class=k>GROUP</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>rank</span>
</span><span id=__span-11-13><a href=#__codelineno-11-13 id=__codelineno-11-13 name=__codelineno-11-13></a><span class=k>HAVING</span><span class=w> </span><span class=n>z_score</span><span class=w> </span><span class=o>&gt;</span><span class=w> </span><span class=mi>2</span><span class=p>.</span><span class=mi>0</span><span class=w>  </span><span class=c1>-- 标准差超过2倍的视为异常</span>
</span><span id=__span-11-14><a href=#__codelineno-11-14 id=__codelineno-11-14 name=__codelineno-11-14></a><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>avg_forward_time</span><span class=w> </span><span class=k>DESC</span><span class=p>;</span>
</span></code></pre></div> <p>这种查询允许我们立即发现性能显著偏离集群平均水平的节点，而无需手动检查每个节点的timeline。</p> <h4 id=_8><a class=toclink href=../../2025/04/28/dist_probe_3/#_8>层次性能分布图</a></h4> <p>分布式训练中，模型的不同组件在不同节点上的性能表现极具研究价值。Probing通过层次性能分布图直观展示这种多维度性能数据，帮助工程师快速定位瓶颈。通过Probing可以采集如下格式的数据：</p> <div class="language-SQL highlight"><pre><span></span><code><span id=__span-12-1><a href=#__codelineno-12-1 id=__codelineno-12-1 name=__codelineno-12-1></a><span class=n>ts</span><span class=p>:</span><span class=w> </span><span class=err>事件时间戳</span>
</span><span id=__span-12-2><a href=#__codelineno-12-2 id=__codelineno-12-2 name=__codelineno-12-2></a><span class=n>node</span><span class=err>：节点名称</span>
</span><span id=__span-12-3><a href=#__codelineno-12-3 id=__codelineno-12-3 name=__codelineno-12-3></a><span class=n>module</span><span class=err>：模块名称</span>
</span><span id=__span-12-4><a href=#__codelineno-12-4 id=__codelineno-12-4 name=__codelineno-12-4></a><span class=n>stage</span><span class=err>：阶段名称，比如</span><span class=n>forward或者backward</span>
</span><span id=__span-12-5><a href=#__codelineno-12-5 id=__codelineno-12-5 name=__codelineno-12-5></a><span class=n>mem_allocated</span><span class=p>:</span><span class=w> </span><span class=err>已经分配的显存</span>
</span><span id=__span-12-6><a href=#__codelineno-12-6 id=__codelineno-12-6 name=__codelineno-12-6></a><span class=n>mem_cached</span><span class=p>:</span><span class=w> </span><span class=err>已经缓存的显存</span>
</span><span id=__span-12-7><a href=#__codelineno-12-7 id=__codelineno-12-7 name=__codelineno-12-7></a><span class=n>duration</span><span class=err>：时间开销</span>
</span></code></pre></div> <p>通过对采集的结构化数据进行多维度聚合与可视化，我们可以构建如下分析图表：</p> <div class="language-SQL highlight"><pre><span></span><code><span id=__span-13-1><a href=#__codelineno-13-1 id=__codelineno-13-1 name=__codelineno-13-1></a><span class=c1>-- 分析每个模型层在不同节点上的性能分布</span>
</span><span id=__span-13-2><a href=#__codelineno-13-2 id=__codelineno-13-2 name=__codelineno-13-2></a><span class=k>SELECT</span>
</span><span id=__span-13-3><a href=#__codelineno-13-3 id=__codelineno-13-3 name=__codelineno-13-3></a><span class=w>    </span><span class=n>module</span><span class=p>,</span>
</span><span id=__span-13-4><a href=#__codelineno-13-4 id=__codelineno-13-4 name=__codelineno-13-4></a><span class=w>    </span><span class=n>node</span><span class=p>,</span>
</span><span id=__span-13-5><a href=#__codelineno-13-5 id=__codelineno-13-5 name=__codelineno-13-5></a><span class=w>    </span><span class=k>AVG</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>avg_duration</span><span class=p>,</span>
</span><span id=__span-13-6><a href=#__codelineno-13-6 id=__codelineno-13-6 name=__codelineno-13-6></a><span class=w>    </span><span class=n>PERCENTILE_CONT</span><span class=p>(</span><span class=mi>0</span><span class=p>.</span><span class=mi>5</span><span class=p>)</span><span class=w> </span><span class=n>WITHIN</span><span class=w> </span><span class=k>GROUP</span><span class=w> </span><span class=p>(</span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>median</span><span class=p>,</span>
</span><span id=__span-13-7><a href=#__codelineno-13-7 id=__codelineno-13-7 name=__codelineno-13-7></a><span class=w>    </span><span class=n>PERCENTILE_CONT</span><span class=p>(</span><span class=mi>0</span><span class=p>.</span><span class=mi>95</span><span class=p>)</span><span class=w> </span><span class=n>WITHIN</span><span class=w> </span><span class=k>GROUP</span><span class=w> </span><span class=p>(</span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>p95</span><span class=p>,</span>
</span><span id=__span-13-8><a href=#__codelineno-13-8 id=__codelineno-13-8 name=__codelineno-13-8></a><span class=w>    </span><span class=k>COUNT</span><span class=p>(</span><span class=o>*</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>samples</span>
</span><span id=__span-13-9><a href=#__codelineno-13-9 id=__codelineno-13-9 name=__codelineno-13-9></a><span class=k>FROM</span><span class=w> </span><span class=n>python</span><span class=p>.</span><span class=n>torch_traces</span>
</span><span id=__span-13-10><a href=#__codelineno-13-10 id=__codelineno-13-10 name=__codelineno-13-10></a><span class=k>WHERE</span><span class=w> </span><span class=k>operation</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>'forward'</span><span class=w> </span><span class=k>AND</span><span class=w> </span><span class=n>step_id</span><span class=w> </span><span class=k>BETWEEN</span><span class=w> </span><span class=mi>1000</span><span class=w> </span><span class=k>AND</span><span class=w> </span><span class=mi>2000</span>
</span><span id=__span-13-11><a href=#__codelineno-13-11 id=__codelineno-13-11 name=__codelineno-13-11></a><span class=k>GROUP</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>module</span><span class=p>,</span><span class=w> </span><span class=n>node</span>
</span><span id=__span-13-12><a href=#__codelineno-13-12 id=__codelineno-13-12 name=__codelineno-13-12></a><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>module</span><span class=p>,</span><span class=w> </span><span class=n>avg_duration</span><span class=w> </span><span class=k>DESC</span><span class=p>;</span>
</span></code></pre></div> <p>这种查询能够生成深度学习模型中每个组件在集群不同节点上的性能热力图，通过这种热力图，我们可以立即观察到：</p> <ul> <li>水平方向：同一节点上不同模型层的相对性能</li> <li>垂直方向：同一模型层在不同节点上的性能差异</li> <li>热点区域：特定节点-组件组合的性能异常</li> </ul> <p>在一次实际分析中，我们通过层次性能分布图发现了某DNN模型中有趣的性能模式：</p> <ul> <li>组件级差异：Attention层在所有节点上都比其他层耗时更长（水平模式）</li> <li>节点级差异：特定的4个节点在处理卷积层时显著慢于其他节点（垂直模式）</li> <li>交互效应：某些节点仅在处理特定类型的层时出现性能下降（局部热点）</li> </ul> <h4 id=_9><a class=toclink href=../../2025/04/28/dist_probe_3/#_9>时间维度的性能演变分析</a></h4> <p>分布式训练的性能问题常常随时间动态变化。通过跟踪关键指标的时间序列，我们可以发现潜在问题：</p> <div class="language-SQL highlight"><pre><span></span><code><span id=__span-14-1><a href=#__codelineno-14-1 id=__codelineno-14-1 name=__codelineno-14-1></a><span class=c1>-- 分析训练过程中的性能趋势</span>
</span><span id=__span-14-2><a href=#__codelineno-14-2 id=__codelineno-14-2 name=__codelineno-14-2></a><span class=k>SELECT</span><span class=w> </span>
</span><span id=__span-14-3><a href=#__codelineno-14-3 id=__codelineno-14-3 name=__codelineno-14-3></a><span class=w>    </span><span class=n>FLOOR</span><span class=p>(</span><span class=n>step_id</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=mi>50</span><span class=p>)</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=mi>50</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>step_bucket</span><span class=p>,</span><span class=w>  </span><span class=c1>-- 按50步为单位分桶</span>
</span><span id=__span-14-4><a href=#__codelineno-14-4 id=__codelineno-14-4 name=__codelineno-14-4></a><span class=w>    </span><span class=k>AVG</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>avg_duration</span><span class=p>,</span>
</span><span id=__span-14-5><a href=#__codelineno-14-5 id=__codelineno-14-5 name=__codelineno-14-5></a><span class=w>    </span><span class=n>STDDEV</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=k>AVG</span><span class=p>(</span><span class=n>duration_ms</span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>cv</span><span class=w>  </span><span class=c1>-- 变异系数</span>
</span><span id=__span-14-6><a href=#__codelineno-14-6 id=__codelineno-14-6 name=__codelineno-14-6></a><span class=k>FROM</span><span class=w> </span><span class=n>torch_traces</span>
</span><span id=__span-14-7><a href=#__codelineno-14-7 id=__codelineno-14-7 name=__codelineno-14-7></a><span class=k>WHERE</span><span class=w> </span><span class=k>operation</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s1>'forward'</span><span class=w> </span>
</span><span id=__span-14-8><a href=#__codelineno-14-8 id=__codelineno-14-8 name=__codelineno-14-8></a><span class=k>GROUP</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>step_bucket</span>
</span><span id=__span-14-9><a href=#__codelineno-14-9 id=__codelineno-14-9 name=__codelineno-14-9></a><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>step_bucket</span><span class=p>;</span>
</span></code></pre></div> <p>通过这种分析，我们可以发现：</p> <ul> <li>训练初期的预热效应</li> <li>性能随时间的逐渐劣化</li> <li>可能的内存泄漏或资源竞争问题</li> <li>周期性波动（如系统GC或后台任务影响）</li> </ul> <h4 id=_10><a class=toclink href=../../2025/04/28/dist_probe_3/#_10>分布式系统的层次化分析</a></h4> <p>在大型集群中，仅分析个体节点往往不够。Probing支持按网络拓扑、硬件型号等进行分组分析：</p> <div class="language-SQL highlight"><pre><span></span><code><span id=__span-15-1><a href=#__codelineno-15-1 id=__codelineno-15-1 name=__codelineno-15-1></a><span class=w> </span><span class=o>#</span><span class=w> </span><span class=err>按网络拓扑分组分析通信性能</span>
</span><span id=__span-15-2><a href=#__codelineno-15-2 id=__codelineno-15-2 name=__codelineno-15-2></a><span class=n>rack_perf</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>probe</span><span class=p>.</span><span class=k>sql</span><span class=p>(</span><span class=ss>"""</span>
</span><span id=__span-15-3><a href=#__codelineno-15-3 id=__codelineno-15-3 name=__codelineno-15-3></a><span class=ss>    SELECT </span>
</span><span id=__span-15-4><a href=#__codelineno-15-4 id=__codelineno-15-4 name=__codelineno-15-4></a><span class=ss>        CASE </span>
</span><span id=__span-15-5><a href=#__codelineno-15-5 id=__codelineno-15-5 name=__codelineno-15-5></a><span class=ss>            WHEN src_rank / 8 = dst_rank / 8 THEN 'same_node'</span>
</span><span id=__span-15-6><a href=#__codelineno-15-6 id=__codelineno-15-6 name=__codelineno-15-6></a><span class=ss>            WHEN src_rank / 32 = dst_rank / 32 THEN 'same_rack'</span>
</span><span id=__span-15-7><a href=#__codelineno-15-7 id=__codelineno-15-7 name=__codelineno-15-7></a><span class=ss>            ELSE 'cross_rack'</span>
</span><span id=__span-15-8><a href=#__codelineno-15-8 id=__codelineno-15-8 name=__codelineno-15-8></a><span class=ss>        END as topology,</span>
</span><span id=__span-15-9><a href=#__codelineno-15-9 id=__codelineno-15-9 name=__codelineno-15-9></a><span class=ss>        AVG(bytes_per_sec) as avg_bandwidth,</span>
</span><span id=__span-15-10><a href=#__codelineno-15-10 id=__codelineno-15-10 name=__codelineno-15-10></a><span class=ss>        COUNT(*) as sample_count</span>
</span><span id=__span-15-11><a href=#__codelineno-15-11 id=__codelineno-15-11 name=__codelineno-15-11></a><span class=ss>    FROM comm_events</span>
</span><span id=__span-15-12><a href=#__codelineno-15-12 id=__codelineno-15-12 name=__codelineno-15-12></a><span class=ss>    GROUP BY topology</span>
</span><span id=__span-15-13><a href=#__codelineno-15-13 id=__codelineno-15-13 name=__codelineno-15-13></a><span class=ss>"""</span><span class=p>).</span><span class=n>fetchall</span><span class=p>()</span>
</span><span id=__span-15-14><a href=#__codelineno-15-14 id=__codelineno-15-14 name=__codelineno-15-14></a>
</span><span id=__span-15-15><a href=#__codelineno-15-15 id=__codelineno-15-15 name=__codelineno-15-15></a><span class=k>for</span><span class=w> </span><span class=k>row</span><span class=w> </span><span class=k>in</span><span class=w> </span><span class=n>rack_perf</span><span class=p>:</span>
</span><span id=__span-15-16><a href=#__codelineno-15-16 id=__codelineno-15-16 name=__codelineno-15-16></a><span class=w>    </span><span class=n>print</span><span class=p>(</span><span class=n>f</span><span class=ss>"{row.topology}: {row.avg_bandwidth/1e9:.2f} GB/s ({row.sample_count} samples)"</span><span class=p>)</span>
</span><span id=__span-15-17><a href=#__codelineno-15-17 id=__codelineno-15-17 name=__codelineno-15-17></a><span class=o>#</span><span class=w> </span><span class=err>输出</span><span class=p>:</span>
</span><span id=__span-15-18><a href=#__codelineno-15-18 id=__codelineno-15-18 name=__codelineno-15-18></a><span class=o>#</span><span class=w> </span><span class=n>same_node</span><span class=p>:</span><span class=w> </span><span class=mi>87</span><span class=p>.</span><span class=mi>32</span><span class=w> </span><span class=n>GB</span><span class=o>/</span><span class=n>s</span><span class=w> </span><span class=p>(</span><span class=mi>12453</span><span class=w> </span><span class=n>samples</span><span class=p>)</span>
</span><span id=__span-15-19><a href=#__codelineno-15-19 id=__codelineno-15-19 name=__codelineno-15-19></a><span class=o>#</span><span class=w> </span><span class=n>same_rack</span><span class=p>:</span><span class=w> </span><span class=mi>23</span><span class=p>.</span><span class=mi>76</span><span class=w> </span><span class=n>GB</span><span class=o>/</span><span class=n>s</span><span class=w> </span><span class=p>(</span><span class=mi>8721</span><span class=w> </span><span class=n>samples</span><span class=p>)</span><span class=w> </span>
</span><span id=__span-15-20><a href=#__codelineno-15-20 id=__codelineno-15-20 name=__codelineno-15-20></a><span class=o>#</span><span class=w> </span><span class=n>cross_rack</span><span class=p>:</span><span class=w> </span><span class=mi>11</span><span class=p>.</span><span class=mi>89</span><span class=w> </span><span class=n>GB</span><span class=o>/</span><span class=n>s</span><span class=w> </span><span class=p>(</span><span class=mi>5432</span><span class=w> </span><span class=n>samples</span><span class=p>)</span>
</span></code></pre></div> <p>这种分析揭示了网络拓扑对通信性能的影响，启发我们优化通信算法和数据分片策略以减少跨机架通信。</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-03-28 00:00:00+00:00">2025年3月28日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/llm/ class=md-meta__link>LLM</a>, <a href=../../category/training/ class=md-meta__link>Training</a></li> <li class=md-meta__item> 需要 4 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=probing><a href=../../2025/03/28/dist_probe_2/ class=toclink>Probing分布式探针开发随笔（二）：探针机制</a></h2> <h3 id=_1><a class=toclink href=../../2025/03/28/dist_probe_2/#_1>引言</a></h3> <p>在前一篇文章中，我们介绍了探针思路的设计理念，以及 Probing 分布式探针系统的整体架构。本文将详细介绍 Probing 的探针机制，包括探针的动态注入与运行时加载，以及如何规避 C/C++常见的 ABI 兼容性问题。</p> <p>为何探针的动态注入能力尤为重要？因为故障和性能问题的发生总是不期而至，我们无法保证每次出现问题时都能提前部署探针。因此，任何需要提前部署的工具都迫使工程师必须”复现”问题才能进行分析，这无疑大大增加了诊断难度和时间成本。而分布式场景下，复现的成本与难度更是倍增，毕竟难以预留千卡或者万卡资源来复现问题。</p> <p>异构计算则是另一个让复现问题变得更加困难的因素。在异构计算中，程序状态不再单纯地保存在 CPU 的内存中，而是同时分布在 GPU、TPU 等计算单元的内存中。这些计算设备的内存中不存在类似调用栈这种结构化数据，我们无法简单地通过 dump 调用栈来捕获故障时刻的状态，而是需要 dump 整个计算设备的内存内容。对于常见的单机八卡配置，完整 dump 一次设备内存需要占用 640GB 的存储空间，这无疑是一个巨大的挑战。而管理这些数据的元数据通常存储在 Python 解释器中，这意味着必须开发一个跨设备、跨语言的调试工具，才能实现完整的故障诊断。</p> <p>探针则是尝试另一种解决问题的思路：</p> <ul> <li>通过动态注入，即可实现在任意条件下调试与诊断；</li> <li>借助探针动态操作目标进程的 Python 解释器，利用其自然可以实现跨语言、跨设备的调试能力；</li> </ul> <h3 id=_2><a class=toclink href=../../2025/03/28/dist_probe_2/#_2>探针机制</a></h3> <p>探针注入的关键在于在目标进程的代码逻辑之外，额外向进程植入一段代码。常见的代码植入方式有两种：</p> <ol> <li><code>LD_PRELOAD</code>方法：通过<code>LD_PRELOAD</code>环境变量，可以让 ld.so 在加载目标进程的时候，优先加载指定的动态链接库从而实现代码植入。这种方法的优点是简单易用，但是只能在进程启动时生效，无法在进程运行时动态注入；</li> <li><code>ptrace</code>方法：通过<code>ptrace</code>系统调用，可以在进程运行时动态修改进程的内存，从而实现代码植入。这种方法的优点是可以在进程运行时动态注入，但是需要对目标进程有一定的权限，且对目标进程的性能影响较大。</li> </ol> <p>本文重点介绍<code>ptrace</code>方法的实现，<code>LD_PRELOAD</code>方法介绍的文章很多，本文不再赘述。</p> <h4 id=ptrace><a class=toclink href=../../2025/03/28/dist_probe_2/#ptrace><code>ptrace</code>系统调用介绍</a></h4> <p><code>ptrace</code>是一个 Linux 系统调用，用于监控和控制另一个进程。<code>ptrace</code>的调用方式如下：</p> <div class="language-c highlight"><pre><span></span><code><span id=__span-11-1><a href=#__codelineno-11-1 id=__codelineno-11-1 name=__codelineno-11-1></a><span class=cp>#include</span><span class=w> </span><span class=cpf>&lt;sys/ptrace.h&gt;</span>
</span><span id=__span-11-2><a href=#__codelineno-11-2 id=__codelineno-11-2 name=__codelineno-11-2></a>
</span><span id=__span-11-3><a href=#__codelineno-11-3 id=__codelineno-11-3 name=__codelineno-11-3></a><span class=kt>long</span><span class=w> </span><span class=nf>ptrace</span><span class=p>(</span><span class=k>enum</span><span class=w> </span><span class=n>__ptrace_request</span><span class=w> </span><span class=n>op</span><span class=p>,</span><span class=w> </span><span class=kt>pid_t</span><span class=w> </span><span class=n>pid</span><span class=p>,</span>
</span><span id=__span-11-4><a href=#__codelineno-11-4 id=__codelineno-11-4 name=__codelineno-11-4></a><span class=w>            </span><span class=kt>void</span><span class=w> </span><span class=o>*</span><span class=n>addr</span><span class=p>,</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=o>*</span><span class=n>data</span><span class=p>);</span>
</span></code></pre></div> <p><code>ptrace</code> 提供了一种控制目标进程执行的方法，它可以让调试器与目标进程进行交互，从而实现调试功能。__ptrace_request 常用的取值如下：</p> <ul> <li><code>PTRACE_ATTACH</code>: 附加到目标进程，使其成为当前进程的 tracee；</li> <li><code>PTRACE_INTERRUPT</code>: 暂停目标 tracee；</li> <li><code>PTRACE_CONT</code>: 让目标进程继续执行；</li> <li><code>PTRACE_DETACH</code>: 释放目标 tracee；</li> <li><code>PTRACE_GETREGS/PTRACE_SETREGS</code>: 读写目标进程寄存器；</li> <li><code>PTRACE_PEEKDATA/PTRACE_POKEDATA</code>： 读写目标进程内存，一次一个 WORD；</li> <li><code>/proc/&lt;pid&gt;/mem</code>: 大块读写内存；</li> </ul> <p>常见的一个 debugger 的工作流程如下：</p> <ol> <li>attach 到目标进程；</li> <li>通过读写目标进程 TEXT 段插入断点；</li> <li>恢复目标进程执行，并用<code>waitpid</code>等待目标进程断点暂停；</li> <li>等到目标进程暂停，通过读写内存查看信息；</li> </ol> <h4 id=_3><a class=toclink href=../../2025/03/28/dist_probe_2/#_3>探针注入流程</a></h4> <p>这里参考了 <a href=https://github.com/Artemis21/ptrace-inject>https://github.com/Artemis21/ptrace-inject</a> 项目，进行了一些修改。注入流程如下：</p> <ol> <li>通过<code>PTRACE_ATTACH</code>附加到目标进程；</li> </ol> <p>Rust 中可以通过<code>pete</code>库对<code>ptrace</code>的封装来使用<code>ptrace</code>系统调用：</p> <div class="language-rust highlight"><pre><span></span><code><span id=__span-12-1><a href=#__codelineno-12-1 id=__codelineno-12-1 name=__codelineno-12-1></a><span class=kd>let</span><span class=w> </span><span class=k>mut</span><span class=w> </span><span class=n>tracer</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>pete</span><span class=p>::</span><span class=n>Ptracer</span><span class=p>::</span><span class=n>new</span><span class=p>();</span>
</span><span id=__span-12-2><a href=#__codelineno-12-2 id=__codelineno-12-2 name=__codelineno-12-2></a><span class=n>tracer</span>
</span><span id=__span-12-3><a href=#__codelineno-12-3 id=__codelineno-12-3 name=__codelineno-12-3></a><span class=w>    </span><span class=p>.</span><span class=n>attach</span><span class=p>((</span><span class=o>&amp;</span><span class=n>proc</span><span class=p>).</span><span class=n>into</span><span class=p>())</span>
</span><span id=__span-12-4><a href=#__codelineno-12-4 id=__codelineno-12-4 name=__codelineno-12-4></a><span class=w>    </span><span class=p>.</span><span class=n>context</span><span class=p>(</span><span class=s>"failed to attach to given process"</span><span class=p>)</span><span class=o>?</span><span class=p>;</span>
</span><span id=__span-12-5><a href=#__codelineno-12-5 id=__codelineno-12-5 name=__codelineno-12-5></a><span class=n>log</span><span class=p>::</span><span class=n>trace</span><span class=o>!</span><span class=p>(</span><span class=s>"Attached to process with PID {}"</span><span class=p>,</span><span class=w> </span><span class=n>proc</span><span class=p>);</span>
</span></code></pre></div> <ol start=2> <li>写入 shellcode 到目标进程的内存中；</li> </ol> <p>首先找到一处合适的内存地址，具有执行权限，可以写入 shellcode。这里我们通过读取目标进程的内存映射信息，找到一个具有执行权限的内存区域：</p> <div class="language-rust highlight"><pre><span></span><code><span id=__span-13-1><a href=#__codelineno-13-1 id=__codelineno-13-1 name=__codelineno-13-1></a><span class=sd>/// Find a suitable address to inject the shellcode into.</span>
</span><span id=__span-13-2><a href=#__codelineno-13-2 id=__codelineno-13-2 name=__codelineno-13-2></a><span class=k>pub</span><span class=p>(</span><span class=k>crate</span><span class=p>)</span><span class=w> </span><span class=k>fn</span><span class=w> </span><span class=nf>find_executable_space</span><span class=p>(</span><span class=o>&amp;</span><span class=bp>self</span><span class=p>)</span><span class=w> </span><span class=p>-&gt;</span><span class=w> </span><span class=nb>Result</span><span class=o>&lt;</span><span class=kt>u64</span><span class=o>&gt;</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-13-3><a href=#__codelineno-13-3 id=__codelineno-13-3 name=__codelineno-13-3></a><span class=w>    </span><span class=n>log</span><span class=p>::</span><span class=n>trace</span><span class=o>!</span><span class=p>(</span><span class=s>"Finding executable space in target process"</span><span class=p>);</span>
</span><span id=__span-13-4><a href=#__codelineno-13-4 id=__codelineno-13-4 name=__codelineno-13-4></a><span class=w>    </span><span class=bp>self</span><span class=p>.</span><span class=mi>0</span>
</span><span id=__span-13-5><a href=#__codelineno-13-5 id=__codelineno-13-5 name=__codelineno-13-5></a><span class=w>        </span><span class=p>.</span><span class=n>maps</span><span class=p>()</span><span class=w> </span><span class=c1>// 读取 /proc/&lt;pid&gt;/maps 文件，获取进程的内存映射信息</span>
</span><span id=__span-13-6><a href=#__codelineno-13-6 id=__codelineno-13-6 name=__codelineno-13-6></a><span class=w>        </span><span class=p>.</span><span class=n>context</span><span class=p>(</span><span class=s>"failed to read process memory maps to find executable region"</span><span class=p>)</span><span class=o>?</span>
</span><span id=__span-13-7><a href=#__codelineno-13-7 id=__codelineno-13-7 name=__codelineno-13-7></a><span class=w>        </span><span class=p>.</span><span class=n>into_iter</span><span class=p>()</span>
</span><span id=__span-13-8><a href=#__codelineno-13-8 id=__codelineno-13-8 name=__codelineno-13-8></a><span class=w>        </span><span class=p>.</span><span class=n>find</span><span class=p>(</span><span class=o>|</span><span class=n>m</span><span class=o>|</span><span class=w> </span><span class=n>m</span><span class=p>.</span><span class=n>perms</span><span class=p>.</span><span class=n>contains</span><span class=p>(</span><span class=n>process</span><span class=p>::</span><span class=n>MMPermissions</span><span class=p>::</span><span class=n>EXECUTE</span><span class=p>))</span>
</span><span id=__span-13-9><a href=#__codelineno-13-9 id=__codelineno-13-9 name=__codelineno-13-9></a><span class=w>        </span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=o>|</span><span class=n>m</span><span class=o>|</span><span class=w> </span><span class=n>m</span><span class=p>.</span><span class=n>address</span><span class=p>.</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-13-10><a href=#__codelineno-13-10 id=__codelineno-13-10 name=__codelineno-13-10></a><span class=w>        </span><span class=p>.</span><span class=n>ok_or_else</span><span class=p>(</span><span class=o>||</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-13-11><a href=#__codelineno-13-11 id=__codelineno-13-11 name=__codelineno-13-11></a><span class=w>            </span><span class=n>anyhow</span><span class=p>::</span><span class=n>anyhow</span><span class=o>!</span><span class=p>(</span><span class=s>"could not find an executable region in the target process"</span><span class=p>)</span>
</span><span id=__span-13-12><a href=#__codelineno-13-12 id=__codelineno-13-12 name=__codelineno-13-12></a><span class=w>        </span><span class=p>})</span>
</span><span id=__span-13-13><a href=#__codelineno-13-13 id=__codelineno-13-13 name=__codelineno-13-13></a><span class=p>}</span>
</span></code></pre></div> <p>上述代码通过读取<code>/proc/&lt;pid&gt;/maps</code>文件，获取进程的内存映射信息，找到一个具有执行权限的内存区域。接下来我们先保存这个内存区域的内容，然后写入 shellcode：</p> <div class="language-rust highlight"><pre><span></span><code><span id=__span-14-1><a href=#__codelineno-14-1 id=__codelineno-14-1 name=__codelineno-14-1></a><span class=c1>// 打开 /proc/&lt;pid&gt;/mem 文件，供后续读写内存使用</span>
</span><span id=__span-14-2><a href=#__codelineno-14-2 id=__codelineno-14-2 name=__codelineno-14-2></a><span class=kd>let</span><span class=w> </span><span class=n>mem</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>fs</span><span class=p>::</span><span class=n>OpenOptions</span><span class=p>::</span><span class=n>new</span><span class=p>().</span><span class=n>read</span><span class=p>(</span><span class=kc>true</span><span class=p>).</span><span class=n>write</span><span class=p>(</span><span class=kc>true</span><span class=p>)</span>
</span><span id=__span-14-3><a href=#__codelineno-14-3 id=__codelineno-14-3 name=__codelineno-14-3></a><span class=w>    </span><span class=p>.</span><span class=n>open</span><span class=p>(</span><span class=s>"/proc/&lt;pid&gt;/mem"</span><span class=p>)</span><span class=o>?</span><span class=p>;</span>
</span><span id=__span-14-4><a href=#__codelineno-14-4 id=__codelineno-14-4 name=__codelineno-14-4></a>
</span><span id=__span-14-5><a href=#__codelineno-14-5 id=__codelineno-14-5 name=__codelineno-14-5></a><span class=c1>// 根据偏移量，读取目标进程的内存</span>
</span><span id=__span-14-6><a href=#__codelineno-14-6 id=__codelineno-14-6 name=__codelineno-14-6></a><span class=kd>let</span><span class=w> </span><span class=n>len</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>mem</span><span class=p>.</span><span class=n>read_at</span><span class=p>(</span><span class=n>data</span><span class=p>,</span><span class=w> </span><span class=n>addr</span><span class=p>)</span><span class=o>?</span><span class=p>;</span>
</span><span id=__span-14-7><a href=#__codelineno-14-7 id=__codelineno-14-7 name=__codelineno-14-7></a>
</span><span id=__span-14-8><a href=#__codelineno-14-8 id=__codelineno-14-8 name=__codelineno-14-8></a><span class=c1>// 将shellcode写入目标进程的内存</span>
</span><span id=__span-14-9><a href=#__codelineno-14-9 id=__codelineno-14-9 name=__codelineno-14-9></a><span class=kd>let</span><span class=w> </span><span class=n>len</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>mem</span><span class=p>.</span><span class=n>write_at</span><span class=p>(</span><span class=n>shellcode</span><span class=p>,</span><span class=w> </span><span class=n>addr</span><span class=p>)</span><span class=o>?</span><span class=p>;</span>
</span></code></pre></div> <p>其中<code>data</code>是一个<code>[u8; 1024]</code>大小的数组，用于保存原内存区域的内容；<code>shellcode</code>是我们要写入的 shellcode，内容如下</p> <div class="language-rust highlight"><pre><span></span><code><span id=__span-15-1><a href=#__codelineno-15-1 id=__codelineno-15-1 name=__codelineno-15-1></a><span class=sd>/// The x64 shellcode that will be injected into the tracee.</span>
</span><span id=__span-15-2><a href=#__codelineno-15-2 id=__codelineno-15-2 name=__codelineno-15-2></a><span class=k>const</span><span class=w> </span><span class=n>SHELLCODE</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=kt>u8</span><span class=p>;</span><span class=w> </span><span class=mi>6</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-15-3><a href=#__codelineno-15-3 id=__codelineno-15-3 name=__codelineno-15-3></a><span class=w>    </span><span class=c1>// Nop slide to make up for the fact that jumping is imprecise.</span>
</span><span id=__span-15-4><a href=#__codelineno-15-4 id=__codelineno-15-4 name=__codelineno-15-4></a><span class=w>    </span><span class=mh>0x90</span><span class=p>,</span><span class=w> </span><span class=mh>0x90</span><span class=p>,</span><span class=w> </span><span class=c1>// nop; nop</span>
</span><span id=__span-15-5><a href=#__codelineno-15-5 id=__codelineno-15-5 name=__codelineno-15-5></a><span class=w>    </span><span class=c1>// The tracer does most of the work by putting the arguments into the</span>
</span><span id=__span-15-6><a href=#__codelineno-15-6 id=__codelineno-15-6 name=__codelineno-15-6></a><span class=w>    </span><span class=c1>// relevant registers, and the function pointer into `r9`.</span>
</span><span id=__span-15-7><a href=#__codelineno-15-7 id=__codelineno-15-7 name=__codelineno-15-7></a><span class=w>    </span><span class=mh>0x41</span><span class=p>,</span><span class=w> </span><span class=mh>0xff</span><span class=p>,</span><span class=w> </span><span class=mh>0xd1</span><span class=p>,</span><span class=w> </span><span class=c1>// call r9</span>
</span><span id=__span-15-8><a href=#__codelineno-15-8 id=__codelineno-15-8 name=__codelineno-15-8></a><span class=w>    </span><span class=c1>// Trap so that the tracer can set up the next call.</span>
</span><span id=__span-15-9><a href=#__codelineno-15-9 id=__codelineno-15-9 name=__codelineno-15-9></a><span class=w>    </span><span class=mh>0xcc</span><span class=p>,</span><span class=w> </span><span class=c1>// int3</span>
</span><span id=__span-15-10><a href=#__codelineno-15-10 id=__codelineno-15-10 name=__codelineno-15-10></a><span class=p>];</span>
</span></code></pre></div> <p>shellcode 主要由三部分组成：</p> <ul> <li>两个<code>nop</code>指令，避免跳转时的不精确性带来问题；</li> <li>一个<code>call r9</code>指令，调用<code>r9</code>寄存器中的函数指针，此处调用会遵循 X86_64 下的标准调用协议，通过寄存器传参；</li> <li>一个<code>int3</code>指令，触发中断，控制流程回到 tracer；</li> </ul> <ol start=3> <li>通过设置寄存器调用目标函数：</li> </ol> <p>在 tracer 中设置寄存器，让目标进程调用函数：</p> <div class="language-rust highlight"><pre><span></span><code><span id=__span-16-1><a href=#__codelineno-16-1 id=__codelineno-16-1 name=__codelineno-16-1></a><span class=bp>self</span><span class=p>.</span><span class=n>tracee</span>
</span><span id=__span-16-2><a href=#__codelineno-16-2 id=__codelineno-16-2 name=__codelineno-16-2></a><span class=w>    </span><span class=p>.</span><span class=n>set_registers</span><span class=p>(</span><span class=n>pete</span><span class=p>::</span><span class=n>Registers</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-16-3><a href=#__codelineno-16-3 id=__codelineno-16-3 name=__codelineno-16-3></a><span class=w>        </span><span class=n>rip</span><span class=p>:</span><span class=w> </span><span class=nc>shellcode_address</span><span class=p>,</span>
</span><span id=__span-16-4><a href=#__codelineno-16-4 id=__codelineno-16-4 name=__codelineno-16-4></a><span class=w>        </span><span class=c1>// shellcode会通过r9寄存器调用函数</span>
</span><span id=__span-16-5><a href=#__codelineno-16-5 id=__codelineno-16-5 name=__codelineno-16-5></a><span class=w>        </span><span class=n>r9</span><span class=p>:</span><span class=w> </span><span class=nc>fn_address</span><span class=p>,</span>
</span><span id=__span-16-6><a href=#__codelineno-16-6 id=__codelineno-16-6 name=__codelineno-16-6></a><span class=w>        </span><span class=c1>// 根据x86-64 ABI要求，将函数入参传递到寄存器中</span>
</span><span id=__span-16-7><a href=#__codelineno-16-7 id=__codelineno-16-7 name=__codelineno-16-7></a><span class=w>        </span><span class=n>rdi</span><span class=p>,</span>
</span><span id=__span-16-8><a href=#__codelineno-16-8 id=__codelineno-16-8 name=__codelineno-16-8></a><span class=w>        </span><span class=n>rsi</span><span class=p>,</span>
</span><span id=__span-16-9><a href=#__codelineno-16-9 id=__codelineno-16-9 name=__codelineno-16-9></a><span class=w>        </span><span class=c1>// 根据x86-64 ABI要求，确保栈指针对齐到16字节</span>
</span><span id=__span-16-10><a href=#__codelineno-16-10 id=__codelineno-16-10 name=__codelineno-16-10></a><span class=w>        </span><span class=n>rsp</span><span class=p>:</span><span class=w> </span><span class=nc>self</span><span class=p>.</span><span class=n>saved_registers</span><span class=p>.</span><span class=n>rsp</span><span class=w> </span><span class=o>&amp;</span><span class=w> </span><span class=o>!</span><span class=mh>0xf</span><span class=p>,</span>
</span><span id=__span-16-11><a href=#__codelineno-16-11 id=__codelineno-16-11 name=__codelineno-16-11></a><span class=w>        </span><span class=o>..</span><span class=bp>self</span><span class=p>.</span><span class=n>saved_registers</span>
</span><span id=__span-16-12><a href=#__codelineno-16-12 id=__codelineno-16-12 name=__codelineno-16-12></a><span class=w>    </span><span class=p>})</span>
</span></code></pre></div> <p>函数<code>fn_address</code>是我们要调用的函数在目标进程中的虚拟地址，<code>rdi</code>和<code>rsi</code>是根据 x86-64 调用约定传递的前两个函数参数，<code>rsp</code>是栈指针，必须对齐到 16 字节以符合 ABI 要求。特别注意，<code>fn_address</code>必须是目标进程地址空间中的有效地址，否则会触发<code>SIGSEGV</code>信号导致进程崩溃。而目标进程的地址是不固定的，我们需要通过函数相对 so 文件的偏移量来计算。首先分别获取<code>libc.so</code>在 tracer 和 tracee 中的地址，可以通过<code>/proc/&lt;pid&gt;/maps</code>文件获取每个 so 映射到内存的地址。再根据函数在 tracer 中的地址计算函数在<code>libc.so</code>中的偏移量。最后在 tracee 中根据<code>libc.so</code>的地址与函数偏移量计算目标函数在 tracee 中的真实地址，即可根据该地址进行调用。</p> <p>获取函数真实地址的代码比较冗长，感兴趣的话可以参考<a href=https://github.com/reiase/probing/blob/master/probing/cli/src/inject/libc_addresses.rs>仓库中的源码</a>。</p> <p>通过上述步骤，我们可以在 tracee 中调用<code>dlopen</code>函数，加载动态链接库，实现动态注入。</p> <h4 id=_4><a class=toclink href=../../2025/03/28/dist_probe_2/#_4>探针实现</a></h4> <p><code>ptrace</code>只是帮助我们实现了探针的动态注入，而真正的探针逻辑还需要我们自己实现。根据前文所述，借助<code>ptrace</code>可以让目标进程调用<code>dlopen</code>来加载动态链接库。而在动态库加载的过程中，会读取 ELF（Executable and Linkable Format） 文件中的<code>.init_array</code>段，该段中存放了一系列初始化函数的地址。C/C++编译器一般支持<code>__attribute__((constructor))</code>属性，可以将函数注册到<code>.init_array</code>段中。</p> <div class="language-c highlight"><pre><span></span><code><span id=__span-17-1><a href=#__codelineno-17-1 id=__codelineno-17-1 name=__codelineno-17-1></a><span class=n>__attribute__</span><span class=p>((</span><span class=n>constructor</span><span class=p>))</span><span class=w> </span><span class=kt>void</span><span class=w> </span><span class=n>my_init</span><span class=p>()</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-17-2><a href=#__codelineno-17-2 id=__codelineno-17-2 name=__codelineno-17-2></a><span class=w>    </span><span class=c1>// 初始化代码</span>
</span><span id=__span-17-3><a href=#__codelineno-17-3 id=__codelineno-17-3 name=__codelineno-17-3></a><span class=p>}</span>
</span></code></pre></div> <p>而 Rust 中可以通过<code>#[ctor]</code>宏实现类似的功能：</p> <div class="language-rust highlight"><pre><span></span><code><span id=__span-18-1><a href=#__codelineno-18-1 id=__codelineno-18-1 name=__codelineno-18-1></a><span class=cp>#[ctor]</span>
</span><span id=__span-18-2><a href=#__codelineno-18-2 id=__codelineno-18-2 name=__codelineno-18-2></a><span class=k>fn</span><span class=w> </span><span class=nf>my_init</span><span class=p>()</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-18-3><a href=#__codelineno-18-3 id=__codelineno-18-3 name=__codelineno-18-3></a><span class=w>    </span><span class=c1>// 初始化代码</span>
</span><span id=__span-18-4><a href=#__codelineno-18-4 id=__codelineno-18-4 name=__codelineno-18-4></a><span class=p>}</span>
</span></code></pre></div> <p>Probing 的注入框架不仅支持其内置探针模块，还支持用户自定义的探针库，提供了极高的扩展性。关于探针的具体设计细节，我们将在后续文章中深入探讨。</p> <h3 id=abi><a class=toclink href=../../2025/03/28/dist_probe_2/#abi>ABI 兼容性</a></h3> <p>传统的 C/C++项目经常受 ABI（Application Binary Interface）兼容性的困扰。常见的 ABI 兼容性问题有两类：</p> <ol> <li>glibc 中函数的版本问题：为了保证 ABI 的兼容性，glibc 中的函数会有多个版本，比如<code>malloc</code>函数就有<code>malloc@GLIBC_2.2.5</code>、<code>malloc@GLIBC_2.3</code>等多个版本。而动态链接库在链接时会在当前 glibc 中选取一个最新的版本，这就导致了在较新的系统下编译的 so 文件在较旧的系统上无法运行；</li> <li>C++的 ABI 问题：C++的 ABI 问题主要由于最近几年 C++标准的更新较快，导致 libstdc++库的 ABI 不断变化。其中最为常见的一种错误是<code>std::string</code>类型在 C++11 标准中引入了短字符串优化（SSO）机制，导致<code>std::string</code>的内存布局发生了变化。而在 C++11 之前编译的 so 文件在 C++11 标准下运行时，会出现内存布局不一致的问题；</li> </ol> <p>Probing 主要通过两种方式解决 ABI 兼容性问题：纯静态链接与 zigbuild。</p> <h4 id=_5><a class=toclink href=../../2025/03/28/dist_probe_2/#_5>纯静态链接</a></h4> <p>静态链接是解决 ABI 兼容性的一种经典方法，通过将所有依赖库代码打包到一个 so 文件中，并在链接阶段完成所有符号的解析，从而避免了运行时出现 ABI 问题。Rust 在构建 so 文件的时候默认使用纯静态链接，能够很大程度上避免 C/C++项目中的 ABI 兼容性问题。</p> <h4 id=zigbuild><a class=toclink href=../../2025/03/28/dist_probe_2/#zigbuild>zigbuild</a></h4> <p>Zig 是一种新兴的系统级编程语言，内置完整的交叉编译工具链，可针对不同 glibc 版本生成二进制文件：</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-19-1><a href=#__codelineno-19-1 id=__codelineno-19-1 name=__codelineno-19-1></a>zig<span class=w> </span>cc<span class=w> </span>main.c<span class=w> </span>-o<span class=w> </span>main<span class=w> </span>-Dtarget<span class=o>=</span>arch64-linux-gnu.2.31
</span></code></pre></div> <p>这使得使用 Zig 工具链构建的 so 文件可以通过指定低版本的 glibc 来增加 so 文件的兼容性。</p> <p><code>cargo-zigbuild</code>是 Rust 构建工具<code>cargo</code>的一个扩展，可以在编译时指定 glibc 的版本，并借助 Zig 的工具链完成 so 文件的链接。</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-20-1><a href=#__codelineno-20-1 id=__codelineno-20-1 name=__codelineno-20-1></a>cargo<span class=w> </span>zigbuild<span class=w> </span>--target<span class=w> </span>x86_64-unknown-linux-gnu.2.17
</span></code></pre></div> <h3 id=_6><a class=toclink href=../../2025/03/28/dist_probe_2/#_6>打包发布</a></h3> <p>前文已经讨论了探针的动态注入与 ABI 兼容性问题，两者都尽最大的可能让 Probing 可以在任意环境下直接运行，而无须额外的配置。接下来我们将讨论 Probing 的打包发布问题，这是让 Probing 真正成为一个通用的工具的关键。</p> <p>二进制工具发布通常有三种渠道：</p> <ol> <li>发布源码：将源码发布到 github 等代码托管平台，用户可以自行编译；但往往构建一个复杂项目的环境是非常困难的，尤其是在分布式环境下；</li> <li>发行版包管理器：将二进制工具打包成 rpm、deb 等包，发布到发行版的包管理器中，用户可以通过包管理器安装；但是不同发行版的包管理器不同，维护成本较高；并且同一个发行版的不同版本需要维护不同的包；</li> <li>pip/conda 等第三方发布平台：将二进制工具打包成 pip/conda 包，发布到第三方平台，用户可以通过 pip/conda 安装；但是这种方式往往需要用户安装额外的包管理器，不够方便；</li> </ol> <p>不过对于 AI 领域的工具来说，Python 是必不可免的，因此基于 Python 包管理工具 pip 或者 conda 来发布 Probing 是一个不错的选择。</p> <p>不同于一般的 python 包，Probing 是一个以 Rust 为主要开发语言的工具，因此并不适合使用 setup.py 等传统方式来构建 python 包。这里我们选择直接使用脚本来打包<code>whl</code>:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a href=#__codelineno-21-1 id=__codelineno-21-1 name=__codelineno-21-1></a><span class=k>def</span><span class=w> </span><span class=nf>write_wheel_file</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=n>contents</span><span class=p>):</span>
</span><span id=__span-21-2><a href=#__codelineno-21-2 id=__codelineno-21-2 name=__codelineno-21-2></a>    <span class=k>with</span> <span class=n>WheelFile</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s2>"w"</span><span class=p>)</span> <span class=k>as</span> <span class=n>wheel</span><span class=p>:</span>
</span><span id=__span-21-3><a href=#__codelineno-21-3 id=__codelineno-21-3 name=__codelineno-21-3></a>        <span class=k>for</span> <span class=n>member_info</span><span class=p>,</span> <span class=n>member_source</span> <span class=ow>in</span> <span class=n>contents</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-21-4><a href=#__codelineno-21-4 id=__codelineno-21-4 name=__codelineno-21-4></a>            <span class=o>...</span>
</span><span id=__span-21-5><a href=#__codelineno-21-5 id=__codelineno-21-5 name=__codelineno-21-5></a>    <span class=k>return</span> <span class=n>filename</span>
</span><span id=__span-21-6><a href=#__codelineno-21-6 id=__codelineno-21-6 name=__codelineno-21-6></a>
</span><span id=__span-21-7><a href=#__codelineno-21-7 id=__codelineno-21-7 name=__codelineno-21-7></a>
</span><span id=__span-21-8><a href=#__codelineno-21-8 id=__codelineno-21-8 name=__codelineno-21-8></a><span class=k>def</span><span class=w> </span><span class=nf>write_wheel</span><span class=p>(</span><span class=n>out_dir</span><span class=p>,</span> <span class=o>*</span><span class=p>,</span> <span class=n>name</span><span class=p>,</span> <span class=n>version</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>metadata</span><span class=p>,</span> <span class=n>description</span><span class=p>,</span> <span class=n>contents</span><span class=p>):</span>
</span><span id=__span-21-9><a href=#__codelineno-21-9 id=__codelineno-21-9 name=__codelineno-21-9></a>    <span class=n>name_snake</span> <span class=o>=</span> <span class=n>name</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>"-"</span><span class=p>,</span> <span class=s2>"_"</span><span class=p>)</span>
</span><span id=__span-21-10><a href=#__codelineno-21-10 id=__codelineno-21-10 name=__codelineno-21-10></a>    <span class=n>wheel_name</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>name_snake</span><span class=si>}</span><span class=s2>-</span><span class=si>{</span><span class=n>version</span><span class=si>}</span><span class=s2>-</span><span class=si>{</span><span class=n>tag</span><span class=si>}</span><span class=s2>.whl"</span>
</span><span id=__span-21-11><a href=#__codelineno-21-11 id=__codelineno-21-11 name=__codelineno-21-11></a>    <span class=n>dist_info</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>name_snake</span><span class=si>}</span><span class=s2>-</span><span class=si>{</span><span class=n>version</span><span class=si>}</span><span class=s2>.dist-info"</span>
</span><span id=__span-21-12><a href=#__codelineno-21-12 id=__codelineno-21-12 name=__codelineno-21-12></a>    <span class=k>return</span> <span class=n>write_wheel_file</span><span class=p>(</span>
</span><span id=__span-21-13><a href=#__codelineno-21-13 id=__codelineno-21-13 name=__codelineno-21-13></a>        <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>out_dir</span><span class=p>,</span> <span class=n>wheel_name</span><span class=p>),</span>
</span><span id=__span-21-14><a href=#__codelineno-21-14 id=__codelineno-21-14 name=__codelineno-21-14></a>        <span class=p>{</span>
</span><span id=__span-21-15><a href=#__codelineno-21-15 id=__codelineno-21-15 name=__codelineno-21-15></a>            <span class=o>**</span><span class=n>contents</span><span class=p>,</span>
</span><span id=__span-21-16><a href=#__codelineno-21-16 id=__codelineno-21-16 name=__codelineno-21-16></a>            <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>dist_info</span><span class=si>}</span><span class=s2>/METADATA"</span><span class=p>:</span> <span class=n>make_message</span><span class=p>(</span><span class=o>...</span><span class=p>),</span>
</span><span id=__span-21-17><a href=#__codelineno-21-17 id=__codelineno-21-17 name=__codelineno-21-17></a>            <span class=sa>f</span><span class=s2>"</span><span class=si>{</span><span class=n>dist_info</span><span class=si>}</span><span class=s2>/WHEEL"</span><span class=p>:</span> <span class=n>make_message</span><span class=p>(</span><span class=o>...</span><span class=p>),</span>
</span><span id=__span-21-18><a href=#__codelineno-21-18 id=__codelineno-21-18 name=__codelineno-21-18></a>        <span class=p>},</span>
</span><span id=__span-21-19><a href=#__codelineno-21-19 id=__codelineno-21-19 name=__codelineno-21-19></a>    <span class=p>)</span>
</span><span id=__span-21-20><a href=#__codelineno-21-20 id=__codelineno-21-20 name=__codelineno-21-20></a>
</span><span id=__span-21-21><a href=#__codelineno-21-21 id=__codelineno-21-21 name=__codelineno-21-21></a>
</span><span id=__span-21-22><a href=#__codelineno-21-22 id=__codelineno-21-22 name=__codelineno-21-22></a><span class=k>def</span><span class=w> </span><span class=nf>write_probing_wheel</span><span class=p>(</span>
</span><span id=__span-21-23><a href=#__codelineno-21-23 id=__codelineno-21-23 name=__codelineno-21-23></a>    <span class=n>out_dir</span><span class=p>,</span> <span class=o>*</span><span class=p>,</span> <span class=n>platform</span><span class=o>=</span><span class=s2>"manylinux_2_12_x86_64.manylinux2010_x86_64"</span>
</span><span id=__span-21-24><a href=#__codelineno-21-24 id=__codelineno-21-24 name=__codelineno-21-24></a><span class=p>):</span>
</span><span id=__span-21-25><a href=#__codelineno-21-25 id=__codelineno-21-25 name=__codelineno-21-25></a>    <span class=o>...</span>
</span><span id=__span-21-26><a href=#__codelineno-21-26 id=__codelineno-21-26 name=__codelineno-21-26></a>
</span><span id=__span-21-27><a href=#__codelineno-21-27 id=__codelineno-21-27 name=__codelineno-21-27></a>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>path</span> <span class=ow>in</span> <span class=p>{</span>
</span><span id=__span-21-28><a href=#__codelineno-21-28 id=__codelineno-21-28 name=__codelineno-21-28></a>        <span class=s2>"probing"</span><span class=p>:</span> <span class=s2>"target/x86_64-unknown-linux-gnu/release/probing"</span><span class=p>,</span>
</span><span id=__span-21-29><a href=#__codelineno-21-29 id=__codelineno-21-29 name=__codelineno-21-29></a>        <span class=s2>"libprobing.so"</span><span class=p>:</span> <span class=s2>"target/x86_64-unknown-linux-gnu/release/libprobing.so"</span><span class=p>,</span>
</span><span id=__span-21-30><a href=#__codelineno-21-30 id=__codelineno-21-30 name=__codelineno-21-30></a>    <span class=p>}</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span><span id=__span-21-31><a href=#__codelineno-21-31 id=__codelineno-21-31 name=__codelineno-21-31></a>        <span class=n>zip_info</span> <span class=o>=</span> <span class=n>ZipInfo</span><span class=p>(</span><span class=sa>f</span><span class=s2>"probing-</span><span class=si>{</span><span class=n>metadata</span><span class=p>[</span><span class=s2>"version"</span><span class=p>]</span><span class=si>}</span><span class=s2>.data/scripts/</span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-21-32><a href=#__codelineno-21-32 id=__codelineno-21-32 name=__codelineno-21-32></a>        <span class=n>zip_info</span><span class=o>.</span><span class=n>external_attr</span> <span class=o>=</span> <span class=p>(</span><span class=n>stat</span><span class=o>.</span><span class=n>S_IFREG</span> <span class=o>|</span> <span class=mo>0o755</span><span class=p>)</span> <span class=o>&lt;&lt;</span> <span class=mi>16</span>
</span><span id=__span-21-33><a href=#__codelineno-21-33 id=__codelineno-21-33 name=__codelineno-21-33></a>        <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>path</span><span class=p>,</span> <span class=s2>"rb"</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span><span id=__span-21-34><a href=#__codelineno-21-34 id=__codelineno-21-34 name=__codelineno-21-34></a>            <span class=n>contents</span><span class=p>[</span><span class=n>zip_info</span><span class=p>]</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span><span id=__span-21-35><a href=#__codelineno-21-35 id=__codelineno-21-35 name=__codelineno-21-35></a>    <span class=o>...</span>
</span><span id=__span-21-36><a href=#__codelineno-21-36 id=__codelineno-21-36 name=__codelineno-21-36></a>    <span class=k>return</span> <span class=n>write_wheel</span><span class=p>(</span>
</span><span id=__span-21-37><a href=#__codelineno-21-37 id=__codelineno-21-37 name=__codelineno-21-37></a>        <span class=n>out_dir</span><span class=p>,</span>
</span><span id=__span-21-38><a href=#__codelineno-21-38 id=__codelineno-21-38 name=__codelineno-21-38></a>        <span class=n>name</span><span class=o>=</span><span class=s2>"probing"</span><span class=p>,</span>
</span><span id=__span-21-39><a href=#__codelineno-21-39 id=__codelineno-21-39 name=__codelineno-21-39></a>        <span class=n>version</span><span class=o>=</span><span class=n>metadata</span><span class=p>[</span><span class=s2>"version"</span><span class=p>],</span>
</span><span id=__span-21-40><a href=#__codelineno-21-40 id=__codelineno-21-40 name=__codelineno-21-40></a>        <span class=n>tag</span><span class=o>=</span><span class=sa>f</span><span class=s2>"py3-none-</span><span class=si>{</span><span class=n>platform</span><span class=si>}</span><span class=s2>"</span><span class=p>,</span>
</span><span id=__span-21-41><a href=#__codelineno-21-41 id=__codelineno-21-41 name=__codelineno-21-41></a>        <span class=n>metadata</span><span class=o>=</span><span class=p>{</span><span class=o>...</span><span class=p>},</span>
</span><span id=__span-21-42><a href=#__codelineno-21-42 id=__codelineno-21-42 name=__codelineno-21-42></a>        <span class=n>description</span><span class=o>=</span><span class=n>description</span><span class=p>,</span>
</span><span id=__span-21-43><a href=#__codelineno-21-43 id=__codelineno-21-43 name=__codelineno-21-43></a>        <span class=n>contents</span><span class=o>=</span><span class=n>contents</span><span class=p>,</span>
</span><span id=__span-21-44><a href=#__codelineno-21-44 id=__codelineno-21-44 name=__codelineno-21-44></a>    <span class=p>)</span>
</span><span id=__span-21-45><a href=#__codelineno-21-45 id=__codelineno-21-45 name=__codelineno-21-45></a>
</span><span id=__span-21-46><a href=#__codelineno-21-46 id=__codelineno-21-46 name=__codelineno-21-46></a>
</span><span id=__span-21-47><a href=#__codelineno-21-47 id=__codelineno-21-47 name=__codelineno-21-47></a><span class=k>def</span><span class=w> </span><span class=nf>main</span><span class=p>():</span>
</span><span id=__span-21-48><a href=#__codelineno-21-48 id=__codelineno-21-48 name=__codelineno-21-48></a>    <span class=n>wheel_path</span> <span class=o>=</span> <span class=n>write_probing_wheel</span><span class=p>(</span><span class=s2>"dist/"</span><span class=p>)</span>
</span><span id=__span-21-49><a href=#__codelineno-21-49 id=__codelineno-21-49 name=__codelineno-21-49></a>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>wheel_path</span><span class=p>,</span> <span class=s2>"rb"</span><span class=p>)</span> <span class=k>as</span> <span class=n>wheel</span><span class=p>:</span>
</span><span id=__span-21-50><a href=#__codelineno-21-50 id=__codelineno-21-50 name=__codelineno-21-50></a>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"  </span><span class=si>{</span><span class=n>wheel_path</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-21-51><a href=#__codelineno-21-51 id=__codelineno-21-51 name=__codelineno-21-51></a>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>"    </span><span class=si>{</span><span class=n>hashlib</span><span class=o>.</span><span class=n>sha256</span><span class=p>(</span><span class=n>wheel</span><span class=o>.</span><span class=n>read</span><span class=p>())</span><span class=o>.</span><span class=n>hexdigest</span><span class=p>()</span><span class=si>}</span><span class=s2>"</span><span class=p>)</span>
</span><span id=__span-21-52><a href=#__codelineno-21-52 id=__codelineno-21-52 name=__codelineno-21-52></a>
</span><span id=__span-21-53><a href=#__codelineno-21-53 id=__codelineno-21-53 name=__codelineno-21-53></a>
</span><span id=__span-21-54><a href=#__codelineno-21-54 id=__codelineno-21-54 name=__codelineno-21-54></a><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>"__main__"</span><span class=p>:</span>
</span><span id=__span-21-55><a href=#__codelineno-21-55 id=__codelineno-21-55 name=__codelineno-21-55></a>    <span class=n>main</span><span class=p>()</span>
</span></code></pre></div> <p>该脚本主要使用<code>wheel</code>包中的<code>WheelFile</code>类来构建<code>whl</code>文件，并将构建出来的二进制写入到<code>probing-{version}.data/scripts</code>目录下。此外需要提供<code>METADATA</code>和<code>WHEEL</code>文件，分别用于描述包的元信息和 wheel 的版本信息。</p> <h3 id=_7><a class=toclink href=../../2025/03/28/dist_probe_2/#_7>总结</a></h3> <p>本文主要讨论了 Probing 的核心机制——探针注入，并讨论了如何将这一机制变成一个通用工具，让其能使用到复杂多样的生产环境中，能够快速发布给尽可能多的用户。所有这些设计都是为了 Probing 的一个核心设计理念：解决问题时，应直接面对根本问题，避免陷入工具配置、环境搭建等元问题的循环中。或者可以认为这一设计理念是马斯克第一性原则的一种体现，缩短解决问题的路径，提高解决问题的效率。</p> <p>在下一篇文章中将会介绍探针 so 的设计与实现。</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-03-26 00:00:00+00:00">2025年3月26日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/llm/ class=md-meta__link>LLM</a>, <a href=../../category/training/ class=md-meta__link>Training</a></li> <li class=md-meta__item> 需要 1 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=probing><a href=../../2025/03/26/dist_probe_1/ class=toclink>Probing分布式探针开发随笔（一）：背景与设计理念</a></h2> <h3 id=_1><a class=toclink href=../../2025/03/26/dist_probe_1/#_1>分布式训练系统的泥潭</a></h3> <p>在过去半年多的时间里，我一直在支持千卡规模的LLM分布式训练。坦白讲，千卡训练的过程并不愉快，尤其是在性能调优和故障排查方面。在这个过程中，我们遇到了一系列棘手的问题：训练无法正常启动、通信库突然hang住、节点性能不及预期、训练速度不稳定等等。这些问题不仅严重影响了训练效率，还大幅增加了调试的复杂度，导致我们不得不花费大量时间和精力在性能调优和故障排查上。</p> <p>有人可能会说，千卡（乃至万卡）规模的稳定性问题在大厂内部已经解决得相当好了。然而，那些耗费无数人力堆砌出来的系统，往往只是在这些大厂已有的复杂基础设施上打补丁，解决眼前可见的问题，而且很多时候仅仅是在处理问题的表象。大规模分布式异构训练真正需要的是类似Hadoop、Spark、Kubernetes或TensorFlow这样具有前瞻性的系统设计，能够解决问题的本质，并提供解决问题的框架，而不仅仅是一些堆砌在特定基础设施上、不具备任何迁移性的”补丁”。我们需要一种更加系统化、可扩展的方法来应对这些挑战。</p> <h3 id=probing_1><a class=toclink href=../../2025/03/26/dist_probe_1/#probing_1>Probing——分布式探针系统的原型探索</a></h3> <p>在解决问题的过程中，我一直思索自己到底需要什么。我需要一种能够在任何时刻动态启用，无需预先部署或插桩，在生产任务中以极低性能开销持续运行，实现实时监控与故障追溯的诊断工具。我需要一种不仅支持单机诊断，还能无缝覆盖分布式训练环境，无论集群规模如何，都能确保数据采集与故障分析的一致性的诊断工具。我需要一种能够从硬件层面的诊断数据、芯片互联状态，到框架、系统和模型各层数据的全面采集，构建完整的闭环监控系统的诊断工具。而现有的种种工具，要么需要侵入式的代码修改和预先部署，要么会严重影响性能，要么只能关注单机，无法覆盖分布式环境，要么只能关注单一维度，无法实现综合分析。</p> <p>基于自己的需求，我开始尝试设计一种“探针”系统：</p> <ul> <li>可以在任意时刻通过动态注入的方式启用，无需预先部署或插桩；</li> <li>运行开销极低或者无开销，可以在生产任务中持续收集性能数据和故障数据；</li> <li>“寄生”在目标进程中，具有相同的内存地址空间与权限，进而实现观测和调试；</li> <li>支持分布式，更好地覆盖大规模分布式训练环境；</li> </ul> <p>这套探针系统大致用法如下：</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-1-1><a href=#__codelineno-1-1 id=__codelineno-1-1 name=__codelineno-1-1></a>$<span class=w> </span>probing<span class=w> </span>&lt;pid&gt;<span class=w> </span>inject<span class=w> </span><span class=c1># 注入探针</span>
</span><span id=__span-1-2><a href=#__codelineno-1-2 id=__codelineno-1-2 name=__codelineno-1-2></a>$<span class=w> </span>probing<span class=w> </span>&lt;pid&gt;<span class=w> </span><span class=nb>eval</span><span class=w> </span><span class=s2>"print('Hello, Probing!')"</span><span class=w> </span><span class=c1># 在目标进程中执行代码</span>
</span><span id=__span-1-3><a href=#__codelineno-1-3 id=__codelineno-1-3 name=__codelineno-1-3></a>$<span class=w> </span>probing<span class=w> </span>&lt;pid&gt;<span class=w> </span>query<span class=w> </span><span class=s2>"SHOW tables"</span><span class=w> </span><span class=c1># 查看可用数据</span>
</span><span id=__span-1-4><a href=#__codelineno-1-4 id=__codelineno-1-4 name=__codelineno-1-4></a>$<span class=w> </span>probing<span class=w> </span>&lt;pid&gt;<span class=w> </span>query<span class=w> </span><span class=s2>"SELECT * FROM process.envs"</span><span class=w> </span><span class=c1># 查询进程环境变量</span>
</span></code></pre></div> <p><code>probing</code>通过<code>query</code>命令提供SQL查询接口，并在这一接口下标准化了不同类型的数据，包括进程状态、硬件性能指标、网络状态、文件系统状态等，使用户无须单独学习每种数据的获取和分析方式。另一方面，SQL查询也提供和AI接入能力，用户可以借助AI生成查询与分析语句，实现自动化的性能分析与故障诊断。后续也会直接扩展SQL支持分布是查询，实现对整个集群的性能分析与故障诊断。</p> <p>在接下来的一系列文章里，我将详细介绍Probing的设计与实现，包括探针机制、数据采集、分析方法等方面。希望这个探索能够为大规模分布式训练的性能分析与故障诊断提供一些启发。以下是接下来需要进行讨论的内容：</p> <ol> <li>如何实现探针的动态注入与运行时加载，如何规避C/C++常见的ABI兼容性问题；</li> <li>如何实现高频数据的采集和存储，如何实现数据的压缩和优化；</li> <li>如何避免跨节点时钟漂移带来的事件时间不一致问题；</li> </ol> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-02-20 00:00:00+00:00">2025年2月20日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/llm/ class=md-meta__link>LLM</a>, <a href=../../category/training/ class=md-meta__link>Training</a>, <a href=../../category/training-dynamics/ class=md-meta__link>Training Dynamics</a></li> <li class=md-meta__item> 需要 5 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=training-dynamicsoutlierllm><a href=../../2025/02/20/training_dynamics/ class=toclink>从Training Dynamics到Outlier——LLM模型训练过程中的数值特性分析</a></h2> <p>Training Dynamics是一个未被严格定义的词，泛指模型训练过程中观测到的各种现象、变化和行为规律。我们可以从loss、泛化loss、梯度大小以及等等表现来观察模型内部的演变机制，并总结出类似涌现现象（Emergency）、Scaling Law、Double Decent和Gradient Pathologies等现象。</p> <p>特别地，权重矩阵与激活值的动态演变(Dynamics)会直接影响数值表达范围，进而决定硬件计算精度选择与量化误差控制策略。本文聚焦Transformer架构中关键组件的数值动态特性，重点分析其对低精度训练与推理的工程影响。</p> <h3 id=_1><a class=toclink href=../../2025/02/20/training_dynamics/#_1>权重与激活的数值演变特征</a></h3> <p>这里先给出权重与梯度的直观数值变化，帮助直观理解训练过程。下图取自某开源仓库<sup id=fnref2:llm_facts><a class=footnote-ref href=../../2025/02/20/training_dynamics/#fn:llm_facts>1</a></sup>，展示了权重数值的直方分布随训练进行的变化情况：</p> <figure><br> <img alt src=../../imgs/tdynamics/weight_histograms_all_together_animation_mlp_h24h_800m.gif width=500><br> </figure> <p>可以发现，各个block的FFN部分权重从随机初始化的高斯分布，开始时较为稳定；在2000 step左右开始剧烈变化；随后整体分布再次稳定下来。权重整体保留了高斯分布，但是存在一些不是非常大的outlier。</p> <p>接下来再看一下激活值的分布变化，在训练开始后，残差激活值迅速从高斯分布转变为逻辑分布（Logistic Distribution），并且出现较大的outlier：</p> <figure><br> <img alt src=../../imgs/tdynamics/acc.gif width=500><br> </figure> <p>这种激活上的outlier会对模型量化过程产生极大的影响，因此诸如AWQ等量化方法会重点关注激活中的outlier情况，以保证模型推理时的精度。</p> <p>梯度分布的变化趋势与权重类似，训练过程也未出现较大的outlier，说明梯度本身也具备较好的稳定性，存在低精度计算和存储的可能性。</p> <figure><br> <img alt src=../../imgs/tdynamics/grad.gif width=500><br> </figure> <nav class=md-post__action> <a href=../../2025/02/20/training_dynamics/ > 继续阅读 </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-02-15 00:00:00+00:00">2025年2月15日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/llm/ class=md-meta__link>LLM</a>, <a href=../../category/training/ class=md-meta__link>Training</a>, <a href=../../category/fp8/ class=md-meta__link>FP8</a></li> <li class=md-meta__item> 需要 2 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=int8><a href=../../2025/02/15/int8_training/ class=toclink>INT8也能训练</a></h2> <p>在 <a href=../../../../../../../2025/02/12/fp8_training>前一篇博客</a> 中，我们深入探讨了DeepSeek V3如何通过FP8实现高效训练，并成功克服了精度挑战。本文探讨另一个问题：如果用INT8代替FP8做训练，会发生什么？</p> <h3 id=int8_1><a class=toclink href=../../2025/02/15/int8_training/#int8_1>INT8 量化</a></h3> <p>给定一个浮点数向量 <span class=arithmatex>\(x \in \mathbb{R}^n\)</span>，INT8量化的目标是将其映射到 [-128, 127] 的整数空间。这一过程需要确定缩放因子 <span class=arithmatex>\(\alpha\)</span> 和零点偏移 <span class=arithmatex>\(\beta\)</span>，使得：</p> <div class=arithmatex>\[ x_q = round(\frac{x}{\alpha}) + \beta \]</div> <p>其中 <span class=arithmatex>\(x_q\)</span> 表示量化后的INT8值。缩放因子 <span class=arithmatex>\(\alpha\)</span> 通常通过以下方式计算：</p> <div class=arithmatex>\[ \alpha = \frac{max(|x|)}{127} \]</div> <p>这确保了量化后的值不会超出INT8的表示范围。而零点偏移 <span class=arithmatex>\(\beta\)</span> 在对称量化场景下通常设置为0，在非对称量化时则需要根据数据分布来确定。</p> <p>对于LLM训练场景，由于权重和激活值通常呈现对称分布，我们可以使用对称量化方案：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a href=#__codelineno-2-1 id=__codelineno-2-1 name=__codelineno-2-1></a><span class=k>def</span><span class=w> </span><span class=nf>symmetric_quantize</span><span class=p>(</span><span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>float</span><span class=p>]:</span>
</span><span id=__span-2-2><a href=#__codelineno-2-2 id=__codelineno-2-2 name=__codelineno-2-2></a>    <span class=n>alpha</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>/</span> <span class=mf>127.0</span>  <span class=c1># 计算缩放因子</span>
</span><span id=__span-2-3><a href=#__codelineno-2-3 id=__codelineno-2-3 name=__codelineno-2-3></a>    <span class=n>x_q</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>x</span> <span class=o>/</span> <span class=n>alpha</span><span class=p>)</span>   <span class=c1># 量化</span>
</span><span id=__span-2-4><a href=#__codelineno-2-4 id=__codelineno-2-4 name=__codelineno-2-4></a>    <span class=n>x_q</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>x_q</span><span class=p>,</span> <span class=o>-</span><span class=mi>128</span><span class=p>,</span> <span class=mi>127</span><span class=p>)</span>  <span class=c1># 截断</span>
</span><span id=__span-2-5><a href=#__codelineno-2-5 id=__codelineno-2-5 name=__codelineno-2-5></a>    <span class=k>return</span> <span class=n>x_q</span><span class=p>,</span> <span class=n>alpha</span>
</span></code></pre></div> <p>反量化操作则是将INT8值映射回浮点数空间：</p> <div class=arithmatex>\[ x_r = (x_q - \beta) \times \alpha \]</div> <p>其中 <span class=arithmatex>\(x_r\)</span> 是反量化后的浮点数值。在对称量化场景下，由于 <span class=arithmatex>\(\beta = 0\)</span>，反量化简化为：</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a href=#__codelineno-3-1 id=__codelineno-3-1 name=__codelineno-3-1></a><span class=k>def</span><span class=w> </span><span class=nf>symmetric_dequantize</span><span class=p>(</span><span class=n>x_q</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>alpha</span><span class=p>:</span> <span class=nb>float</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-3-2><a href=#__codelineno-3-2 id=__codelineno-3-2 name=__codelineno-3-2></a>    <span class=k>return</span> <span class=n>x_q</span> <span class=o>*</span> <span class=n>alpha</span>
</span></code></pre></div> <p>与FP8的浮点量化不同，INT8采用均匀量化方案：</p> <ul> <li>优势区间：大值区域精度更高（固定量化步长）</li> <li>劣势区间：小值区域精度较低（相对误差更大）</li> </ul> <p>这种特性使得INT8对数据分布形态更为敏感，需要针对性优化策略。</p> <nav class=md-post__action> <a href=../../2025/02/15/int8_training/ > 继续阅读 </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-02-12 00:00:00+00:00">2025年2月12日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/llm/ class=md-meta__link>LLM</a>, <a href=../../category/training/ class=md-meta__link>Training</a>, <a href=../../category/fp8/ class=md-meta__link>FP8</a></li> <li class=md-meta__item> 需要 9 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=deepseek-v3fp8><a href=../../2025/02/12/fp8_training/ class=toclink>从DeepSeek V3看FP8训练的挑战</a></h2> <p>DeepSeek V3 的发布引起了对 FP8 训练的广泛关注，业界也出现了大量文章解析 How 的问题——DeepSeek 是怎么进行 FP8 训练的，与传统方案有哪些不同。但是目前鲜有文章对 Why 问题进行深入探讨，为何 DeepSeek 的方案能够取得成功。本文尝试对 FP8 训练所面临的挑战进行深入解析，并尝试猜测 DeepSeek 团队设计其 FP 方案的背后原理。（如果你对 INT8 训练感兴趣，可以参考本文的姊妹篇：<a href=../../../../../../../2025/02/15/int8_training>INT8 训练</a>）</p> <h3 id=1-fp8><a class=toclink href=../../2025/02/12/fp8_training/#1-fp8>1. FP8 浮点格式</a></h3> <h4 id=11-fp8><a class=toclink href=../../2025/02/12/fp8_training/#11-fp8>1.1 FP8 格式的历史</a></h4> <p>FP8 是一种遵循 IEEE 754 规范<sup id=fnref2:ieee754><a class=footnote-ref href=../../2025/02/12/fp8_training/#fn:ieee754>1</a></sup>的 8 位浮点数格式，由 Nvidia 在 2022 年发布的 H100 GPU 中首次引入。在此之前，Nvidia 硬件上浮点数格式的发展历程如下<sup id=fnref2:huang_law_1><a class=footnote-ref href=../../2025/02/12/fp8_training/#fn:huang_law_1>2</a></sup>：</p> <ul> <li>2016 年 P100 GPU 首次引入 FP16 数据格式，直接开启了深度学习混合精度训练的技术路线；</li> <li>2017 年 V100 GPU 首次引入 Tensor Core, 用于加速 FP16 矩阵乘法运算；</li> <li>2020 年 A100 GPU 首次引入 TF32 数据格式，可通过 Tensor Core 加速；引入 bfloat16 数据格式，提供比 FP16 更宽的动态范围（当下 BF16 已经成为 LLM 训练的主流方案）；</li> <li>2022 年 H100 GPU 首次引入 FP8 数据格式；</li> </ul> <p>FP8 被 Nvidia 给予厚望，认为其成功的延续了 CEO 提出的 Huang’s Law<sup id=fnref2:huang_law_2><a class=footnote-ref href=../../2025/02/12/fp8_training/#fn:huang_law_2>3</a></sup>，即 10 年间 GPU 硬件算力提升 1000 倍。在过去的 10 年间，新型数值表达的引入了 16 倍算力提升，是诸多技术中贡献最大者，GPU 架构与复杂指令集紧随其后带来了 12.5 倍提升，而制程进步带来的收益非常有限，仅 2.5 倍<sup id=fnref2:nvidia_gpu><a class=footnote-ref href=../../2025/02/12/fp8_training/#fn:nvidia_gpu>4</a></sup>。</p> <h4 id=12-ieee-754><a class=toclink href=../../2025/02/12/fp8_training/#12-ieee-754>1.2. 常见浮点数与 IEEE 754</a></h4> <p>IEEE 754 是目前广为使用的浮点数规范，定义了浮点数的 bitwise 表达与量化方式。浮点数的二进制表达分为三部分：</p> <ul> <li>符号位（sign）</li> <li>指数位（exponent）</li> <li>尾数位（mantissa）</li> </ul> <p>常见的浮点数格式的二进制表达如下图所示：</p> <pre class=mermaid><code>block-beta
    columns 33
    FP32["fp32"]
    S1["S"]
    E1["E"]
    E2["E"]
    E3["E"]
    E4["E"]
    E5["E"]
    E6["E"]
    E7["E"]
    E8["E"]
    M1["M"]
    M2["M"]
    M3["M"]
    M4["M"]
    M5["M"]
    M6["M"]
    M7["M"]
    M8["M"]
    M9["M"]
    M10["M"]
    M11["M"]
    M12["M"]
    M13["M"]
    M14["M"]
    M15["M"]
    M16["M"]
    M17["M"]
    M18["M"]
    M19["M"]
    M20["M"]
    M21["M"]
    M22["M"]
    M23["M"]

    BF16["bf16"]
    SS1["S"]
    EE1["E"]
    EE2["E"]
    EE3["E"]
    EE4["E"]
    EE5["E"]
    EE6["E"]
    EE7["E"]
    EE8["E"]
    MM1["M"]
    MM2["M"]
    MM3["M"]
    MM4["M"]
    MM5["M"]
    MM6["M"]
    MM7["M"]
    space:16

    FP16["fp16"]
    space:3
    ss1["S"]
    ee1["E"]
    ee2["E"]
    ee3["E"]
    ee4["E"]
    ee5["E"]
    mm1["M"]
    mm2["M"]
    mm3["M"]
    mm4["M"]
    mm5["M"]
    mm6["M"]
    mm7["M"]
    mm8["M"]
    mm9["M"]
    mm10["M"]
    space:13

    E5M2["fp8"]
    space:3
    s1["S"]
    e1["E"]
    e2["E"]
    e3["E"]
    e4["E"]
    e5["E"]
    m1["M"]
    m2["M"]
    space:21

    E4M3["fp8"]
    space:4
    sss1["S"]
    eee1["E"]
    eee2["E"]
    eee3["E"]
    eee4["E"]
    mmm1["M"]
    mmm2["M"]
    mmm3["M"]
    space:21

    classDef name fill:#00000000, stroke:#00000000
    class FP32,BF16,FP16,E4M3,E5M2 name

    classDef sign fill:#EE0000, stroke:#00000000
    class S1,SS1,s1,ss1,sss1 sign

    classDef exp fill:#00EE00, stroke:#00000000
    class E1,E2,E3,E4,E5,E6,E7,E8 exp
    class EE1,EE2,EE3,EE4,EE5,EE6,EE7,EE8 exp
    class e1,e2,e3,e4,e5,e6,e7,e8 exp
    class ee1,ee2,ee3,ee4,ee5,ee6,ee7,ee8 exp
    class eee1,eee2,eee3,eee4,eee5,eee6,eee7,eee8 exp</code></pre> <h4 id=13-fp8><a class=toclink href=../../2025/02/12/fp8_training/#13-fp8>1.3. FP8 有两种格式</a></h4> <p>随着浮点数位数从 16 位进一步降低到 8 位，动态范围不足的问题逐渐显现。因此 Nvidia、Arm 和 Intel 在 FP8 规范中设计了两种浮点数类型<sup id=fnref2:fp8><a class=footnote-ref href=../../2025/02/12/fp8_training/#fn:fp8>5</a></sup>：E4M3 和 E5M2</p> <table> <thead> <tr> <th></th> <th>E4M3</th> <th>E5M2</th> </tr> </thead> <tbody> <tr> <td>format(s/e/m)</td> <td>1:4:3</td> <td>1:5:2</td> </tr> <tr> <td>Exponent bias</td> <td>7</td> <td>15</td> </tr> <tr> <td>Infinities</td> <td>N/A</td> <td>S.11111.00</td> </tr> <tr> <td>NaN</td> <td>S.1111.111</td> <td>S.11111.{01,10,11}</td> </tr> <tr> <td>Zeros</td> <td>S.0000.000</td> <td>S.00000.00</td> </tr> <tr> <td>Max normal</td> <td>S.1111.110 = <span class=arithmatex>\(1.75 \times 2^8\)</span> = 448</td> <td>S.11110.11 = <span class=arithmatex>\(1.75 \times 2^15\)</span> = 57.344</td> </tr> <tr> <td>Min normal</td> <td>S.0001.0000 = <span class=arithmatex>\(2^{-6}\)</span></td> <td>S.00001.00 = <span class=arithmatex>\(2^{-14}\)</span></td> </tr> <tr> <td>Max subnorm</td> <td>S.0000.111 = <span class=arithmatex>\(0.875 \times 2^{-6}\)</span></td> <td>S.00000.11 = <span class=arithmatex>\(0.75\times 2^{-14}\)</span></td> </tr> <tr> <td>Min subnorm</td> <td>S.0000.001 = <span class=arithmatex>\(2^{-9}\)</span></td> <td>S.00000.01 = $ 2^{-16}$</td> </tr> </tbody> </table> <p>浮点数都会分配一些二进制表达来表示特殊值**NaN**和 <strong><span class=arithmatex>\(\mathbb{\pm}\)</span>Inf</strong>，IEEE 754 规范约定使用指数位全**1**的二进制表达来表示这些特殊值。对于 E4M3 格式来说，若严格遵循 IEEE 754 规范，会 8 个二进制表达。因此在定义 E4M3 规范时对这些二进制表达进行了额外开发，仅在指数位尾数位同时全为 <strong>1</strong> 时才表示 <strong>NaN</strong>，全为 <strong>0</strong> 的时候表示 <strong><span class=arithmatex>\(\pm\)</span>Inf</strong>。</p> <p>H100 的 Tensor Core 提供 3 倍 A100 FP16 性能，若启用 FP8 算力能够再次翻倍。</p> <p><img alt src=https://developer-blogs.nvidia.com/zh-cn-blog/wp-content/uploads/sites/2/2024/04/%E8%A1%A81-1536x819.png></p> <nav class=md-post__action> <a href=../../2025/02/12/fp8_training/ > 继续阅读 </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-02-09 00:00:00+00:00">2025年2月9日</time></li> <li class=md-meta__item> 分类于 <a href=../../category/llm/ class=md-meta__link>LLM</a>, <a href=../../category/training/ class=md-meta__link>Training</a>, <a href=../../category/rl/ class=md-meta__link>RL</a></li> <li class=md-meta__item> 需要 6 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=deepseek-r1><a href=../../2025/02/09/rl_ds_r1/ class=toclink>从强化学习到DeepSeek R1</a></h2> <h3 id=1-rl-reinforcement-learning><a class=toclink href=../../2025/02/09/rl_ds_r1/#1-rl-reinforcement-learning>1. 什么是强化学习(RL, Reinforcement Learning)</a></h3> <p>传统的机器学习，包括深度学习，其本质是数学性的，严格遵守函数的数学定义：对于给定输入，产生确定的输出</p> <div class=arithmatex>\[F(x) = y\]</div> <p>随着输入<span class=arithmatex>\(x\)</span>和输出<span class=arithmatex>\(y\)</span>的不同，这一范式可以适配各种不同的任务，比如：</p> <ul> <li><span class=arithmatex>\(x\)</span> 是图像，<span class=arithmatex>\(y\)</span>是类别，那么<span class=arithmatex>\(F\)</span>就是Resnet这种图像模型；</li> <li><span class=arithmatex>\(x\)</span> 是语音信号，<span class=arithmatex>\(y\)</span>是文字，那么<span class=arithmatex>\(F\)</span>就是一个语音识别模型；</li> <li><span class=arithmatex>\(x\)</span> 是文本输入，<span class=arithmatex>\(y\)</span>是文本输出，那么<span class=arithmatex>\(F\)</span>就是时下火热的大语言模型；<br> …</li> </ul> <p>强化学习（Reinforcement Learning）的本质上则是哲学性的，它探讨三个核心问题：</p> <ul> <li>我是谁？一个Agent</li> <li>我在哪？处于某个State</li> <li>到哪里去？采取一个Action</li> </ul> <p>如果站在上帝视角去观测这个Agent，我们还会发现：</p> <ul> <li>Agent处在一个环境中（Environment）</li> <li>Agent有一个用来策略（Policy）告诉我该采取什么动作（Action）</li> <li>每执行一个动作（Action），环境都会给我反馈 (Reward)</li> </ul> <p>以上就是强化学习中的主要概念。</p> <p><img alt="alt text" src=../../imgs/rl.png></p> <h3 id=2><a class=toclink href=../../2025/02/09/rl_ds_r1/#2>2. 如何进行强化学习</a></h3> <p>这里以一个迷宫问题为例，介绍如何进行强化学习：</p> <p>迷宫：(S: Start, E: End, W: Wall)</p> <pre class=mermaid><code>block-beta
  columns 3
  S1["S1(S)"] S2 S3["S3(W)"]
  S4 S5 S6
  S7["S7(W)"] S8 S9["S9(E)"]
</code></pre> <p>这个迷宫就是一个Environment。我们放置一个机器人在开始处（Start），让机器人自动学习如何走迷宫的策略（Policy）。这个策略可以记成<span class=arithmatex>\(\pi(s)\rightarrow a, s \in [1-9], a \in [上, 下, 左, 右]\)</span>。开始时机器人对于迷宫一无所知，所以<span class=arithmatex>\(\pi(s)会随机输出一个方向\)</span>。</p> <nav class=md-post__action> <a href=../../2025/02/09/rl_ds_r1/ > 继续阅读 </a> </nav> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <nav aria-label=页脚 class="md-footer__inner md-grid"> <a aria-label="上一页: 分布式训练" href=../../category/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/ class="md-footer__link md-footer__link--prev"> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> 分布式训练 </div> </div> </a> <a aria-label="下一页: 2024" href=../2024/ class="md-footer__link md-footer__link--next"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> 2024 </div> </div> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2023 - 2025 Reiase </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material for MkDocs </a> </div> <div class=md-social> <a class=md-social__link href=https://github.com/reiase rel=noopener target=_blank title=github.com> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../assets/_markdown_exec_pyodide.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>